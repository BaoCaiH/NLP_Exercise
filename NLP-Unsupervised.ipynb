{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"first-part\"></a>\n",
    "\n",
    "#!/usr/bin/env python<br># coding: utf-8\n",
    "\n",
    "Author: Bao Cai\n",
    "\n",
    "Course: Machine Learning for Descriptive Problems\n",
    "\n",
    "Topic: NLP-Unsupervised\n",
    "\n",
    "Start Date: 2020-03-11\n",
    "\n",
    "Last Save: 2020-03-11"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[1. Topic analysis](#topic-part)\n",
    "\n",
    "There are quantitative measures for evaluating various aspects of clustering results and topic models intrinsically, and they can also be evaluated extrinsically by how well the clusters/topics serve some supervised task as features. In this exervise, however, we well fovus on qualitative evaluation of the results in terms of their descriptiveness. As in the example code, you may limit yourself to the 1000 first documents of ther corpus when performing clustering, in order to simplify the task and speed up experimentation, but use the whole corpus to calculate tf-idf features.\n",
    "\n",
    "a. Experiment with different setups of the tf-idf feature extraction and clustering (k-means or hierarchical). In order to obtain meaningful results. When you arrive at a good configuration, describe it and motivate your chosen setup/parameters.\n",
    "\n",
    "b. Inspect the keywords of the clusters. List the 10 first clusters out of all (o.e. not cherry picked examples) and privde an as descriptive label as possible for each of them.\n",
    "\n",
    "c. Select one or two good clusters (that can be clearly interpreted) and one or two bad clusters (that might be difficult to interpret or distinguish). Motivate your choise (clusters may for instance, be overlapping, to broad/narrow or incoherent).\n",
    "\n",
    "d. Repeat the experiment in (a) with LDA topic modelling instead (on the whole corpus), and explain briefly how the results compare to your previously chosen clustering setup. A few concrete examples may be helpful. Do your best to make sure the list of topic keywords are informative through appropriate post-processing.\n",
    "\n",
    "[2. Word vectors](#word-vector)\n",
    "\n",
    "a. Choose about 5 words (arbitrary) to use as seed words in the following experiment. Train word2vec vectors on the corpus while trying out variations on the parameters. Evaluate the vector models by inspecting the most similar words for each of the seed words, and try to identify qualitative differences between different parameter choices. Which parameters seem to have the most interesting effect? At what values? Motivate. Finally, study the qualitative effect of increasing the training data, by similarly comparing vectors trained with the best setup on the texts from the awards_2020 directory against vectors trained on the whole set of abstracts (1990-2002).\n",
    "\n",
    "b. Repeat the experiment with ELMo from the lecture, with a different target word and diferent sentences. Choose a word that can have multiple senses, and construct 10 sentences that express 2-3 different senses of the word. Produce ELMo embeddings for the target word in each sentence and measure the similarity between the vectors. Evaluate in how many cases the measured similarities can be used to successfully distinguish between the different senses. Comment on the results, e.g. are you able to identify a particular way in which the model fails?\n",
    "\n",
    "[top](#first-part)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "import os\n",
    "import re\n",
    "import binascii\n",
    "import itertools\n",
    "import heapq\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from time import time\n",
    "from collections import Counter\n",
    "from gensim import corpora, models\n",
    "from scipy.spatial.distance import cosine\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage, fcluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions\n",
    "def get_fnames(path='./Data'):\n",
    "    \"\"\"Read all text files in a folder.\n",
    "    \"\"\"\n",
    "    fnames = []\n",
    "    for root, _, files in os.walk(path):\n",
    "        for fname in files:\n",
    "            if fname[-4:] == '.txt':\n",
    "                fnames.append(os.path.join(root, fname))\n",
    "    return fnames\n",
    "\n",
    "\n",
    "def read_file(fname):\n",
    "    with open(fname, 'rt', encoding='latin-1') as f:\n",
    "        # skip all lines until abstract\n",
    "        for line in f:\n",
    "            if \"Abstract    :\" in line:\n",
    "                break\n",
    "\n",
    "        # get abstract as a single string\n",
    "        abstract = ' '.join([line[:-1].strip() for line in f])\n",
    "        abstract = re.sub(' +', ' ', abstract)\n",
    "        return abstract\n",
    "\n",
    "\n",
    "def print_clusters(matrix, clusters, features, n_keywords=10):\n",
    "    for cluster in range(min(clusters), max(clusters)+1):\n",
    "        cluster_docs = [i for i, c in enumerate(clusters) if c == cluster]\n",
    "        print(\"Cluster: %d (%d docs)\" % (cluster, len(cluster_docs)))\n",
    "        \n",
    "        # Keep scores for top n terms\n",
    "        new_matrix = np.zeros((len(cluster_docs), matrix.shape[1]))\n",
    "        for cluster_i, doc_vec in enumerate(matrix[cluster_docs].toarray()):\n",
    "            for idx, score in heapq.nlargest(n_keywords, enumerate(doc_vec), key=lambda x:x[1]):\n",
    "                new_matrix[cluster_i][idx] = score\n",
    "\n",
    "        # Aggregate scores for kept top terms\n",
    "        keywords = heapq.nlargest(n_keywords, zip(new_matrix.sum(axis=0), features))\n",
    "        print(', '.join([w for s,w in keywords]))\n",
    "        print()\n",
    "\n",
    "\n",
    "def vectorize_cluster(\n",
    "    documents,\n",
    "    sample_size=None,\n",
    "    ngram_range=(1, 1),\n",
    "    analyzer='word',\n",
    "    max_df=1.0,\n",
    "    min_df=0.0,\n",
    "    max_features=None,\n",
    "    use_idf=True,\n",
    "    sublinear_tf=True,\n",
    "    n_clusters=10,\n",
    "    tol=0.0001\n",
    "):\n",
    "    vectorizer = TfidfVectorizer(\n",
    "        ngram_range=ngram_range,\n",
    "        analyzer=analyzer,\n",
    "        max_df=max_df,\n",
    "        min_df=min_df,\n",
    "        max_features=max_features,\n",
    "        use_idf=use_idf,\n",
    "        sublinear_tf=sublinear_tf\n",
    "    )\n",
    "    token_matrix = vectorizer.fit_transform(documents)\n",
    "    print(\n",
    "        'The shape of the token matrix is:',\n",
    "        token_matrix.toarray().shape\n",
    "    )\n",
    "    print()\n",
    "    features = vectorizer.get_feature_names()\n",
    "    print('The top 20 tokens are:')\n",
    "    for feature, idf in sorted(\n",
    "        zip(features, vectorizer._tfidf.idf_),\n",
    "        key=lambda x:x[1]\n",
    "    )[:20]:\n",
    "        print('{:.2f}\\t{}'.format(idf, feature))\n",
    "    print()\n",
    "    print('For the first 5 docs, the top tokens are:')\n",
    "    for i in range(5):\n",
    "        print('\\nDocument {}, top terms by TF-IDF'.format(i))\n",
    "        for feature, score in sorted(\n",
    "            zip(features, token_matrix.toarray()[i]),\n",
    "            key=lambda x: -x[1]\n",
    "        )[:5]:\n",
    "            print('{:.2f}\\t{}'.format(score, feature))\n",
    "    print()\n",
    "    if not sample_size:\n",
    "        matrix_to_cluster = token_matrix\n",
    "    else:\n",
    "        matrix_to_cluster = token_matrix[:sample_size]\n",
    "    km = KMeans(\n",
    "        n_clusters=n_clusters,\n",
    "        tol=tol,\n",
    "        random_state=44,\n",
    "        verbose=False\n",
    "    )\n",
    "    km.fit(matrix_to_cluster)\n",
    "    print_clusters(matrix_to_cluster, km.labels_, features)\n",
    "    return (\n",
    "        vectorizer,\n",
    "        token_matrix,\n",
    "        features,\n",
    "        matrix_to_cluster,\n",
    "        km\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 17 µs, sys: 9 µs, total: 26 µs\n",
      "Wall time: 25.7 µs\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "documents = [read_file(file) for file in get_fnames('./Data/Arcada_BigDataAnalytic/')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"topic-part\"></a>\n",
    "### Topic analysis\n",
    "\n",
    "[top](#first-part)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of the token matrix is: (9923, 53816)\n",
      "\n",
      "The top 20 tokens are:\n",
      "1.03\tthe\n",
      "1.03\tof\n",
      "1.03\tand\n",
      "1.04\tto\n",
      "1.05\tin\n",
      "1.13\tthis\n",
      "1.14\tfor\n",
      "1.18\tis\n",
      "1.19\twill\n",
      "1.26\tbe\n",
      "1.29\ton\n",
      "1.31\twith\n",
      "1.33\tthat\n",
      "1.40\tare\n",
      "1.40\tresearch\n",
      "1.41\tby\n",
      "1.43\tas\n",
      "1.51\tfrom\n",
      "1.55\tan\n",
      "1.61\tthese\n",
      "\n",
      "For the first 5 docs, the top tokens are:\n",
      "\n",
      "Document 0, top terms by TF-IDF\n",
      "0.34\ttrafficking\n",
      "0.22\tdrug\n",
      "0.20\tdatabase\n",
      "0.19\tdiscovery\n",
      "0.18\tmislocalization\n",
      "\n",
      "Document 1, top terms by TF-IDF\n",
      "0.25\tnmr\n",
      "0.23\toptically\n",
      "0.20\tingap\n",
      "0.18\thayes\n",
      "0.17\tinp\n",
      "\n",
      "Document 2, top terms by TF-IDF\n",
      "0.25\tfabric\n",
      "0.19\ttextiles\n",
      "0.18\tpesticides\n",
      "0.18\tweapons\n",
      "0.18\tdrapes\n",
      "\n",
      "Document 3, top terms by TF-IDF\n",
      "0.33\tthundersnow\n",
      "0.19\trawinsonde\n",
      "0.18\tlightning\n",
      "0.16\tsnow\n",
      "0.15\tforecasting\n",
      "\n",
      "Document 4, top terms by TF-IDF\n",
      "0.20\tdots\n",
      "0.19\ttutorials\n",
      "0.18\thelium\n",
      "0.17\tcondensates\n",
      "0.17\tquantum\n",
      "Cluster: 0 (1505 docs)\n",
      "polymer, magnetic, spin, nano, laser, films, quantum, optical, nanoscale, nanoparticles\n",
      "\n",
      "Cluster: 1 (1494 docs)\n",
      "species, birds, political, social, genetic, populations, evolutionary, language, decision, firms\n",
      "\n",
      "Cluster: 2 (834 docs)\n",
      "protein, proteins, genes, nmr, arabidopsis, gene, complexes, genome, dna, cell\n",
      "\n",
      "Cluster: 3 (1791 docs)\n",
      "ice, ocean, mantle, solar, climate, galaxies, arctic, seismic, stars, fault\n",
      "\n",
      "Cluster: 4 (682 docs)\n",
      "equations, manifolds, spaces, algebraic, quantum, algebras, hyperbolic, geometry, topology, random\n",
      "\n",
      "Cluster: 5 (1064 docs)\n",
      "teachers, reu, stem, engineering, mathematics, curriculum, learning, scholarship, college, fellows\n",
      "\n",
      "Cluster: 6 (62 docs)\n",
      "fellowship, mathematical, sciences, postdoctoral, informatics, training, fellowships, minority, biological, genetic\n",
      "\n",
      "Cluster: 7 (1608 docs)\n",
      "wireless, software, grid, sensor, mobile, networks, power, security, computing, visualization\n",
      "\n",
      "Cluster: 8 (225 docs)\n",
      "available, not, zyss, zylstra, zygotically, zygotic, zygote, zygomycota, zygomycetes, zygomorphy\n",
      "\n",
      "Cluster: 9 (658 docs)\n",
      "workshop, conference, symposium, meeting, gordon, 2002, sessions, 2003, session, eu\n",
      "\n",
      "CPU times: user 11min 27s, sys: 3min 11s, total: 14min 39s\n",
      "Wall time: 10min 37s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "default_setup = vectorize_cluster(\n",
    "    documents,\n",
    "    sample_size=None,\n",
    "    ngram_range=(1, 1),\n",
    "    analyzer='word',\n",
    "    max_df=1.0,\n",
    "    min_df=0.0,\n",
    "    max_features=None,\n",
    "    use_idf=True,\n",
    "    sublinear_tf=True,\n",
    "    n_clusters=10,\n",
    "    tol=0.0001\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of the token matrix is: (9923, 10000)\n",
      "\n",
      "The top 20 tokens are:\n",
      "1.13\tthis\n",
      "1.14\tfor\n",
      "1.18\tis\n",
      "1.19\twill\n",
      "1.26\tbe\n",
      "1.29\ton\n",
      "1.31\twith\n",
      "1.33\tthat\n",
      "1.40\tare\n",
      "1.40\tresearch\n",
      "1.41\tby\n",
      "1.43\tas\n",
      "1.51\tfrom\n",
      "1.55\tan\n",
      "1.61\tthese\n",
      "1.61\tproject\n",
      "1.61\tat\n",
      "1.84\thave\n",
      "1.88\twhich\n",
      "1.92\tnew\n",
      "\n",
      "For the first 5 docs, the top tokens are:\n",
      "\n",
      "Document 0, top terms by TF-IDF\n",
      "0.36\ttrafficking\n",
      "0.24\tdrug\n",
      "0.22\tdatabase\n",
      "0.20\tdiscovery\n",
      "0.19\tdiseases\n",
      "\n",
      "Document 1, top terms by TF-IDF\n",
      "0.27\tnmr\n",
      "0.24\toptically\n",
      "0.15\twells\n",
      "0.15\tgaas\n",
      "0.15\theterostructures\n",
      "\n",
      "Document 2, top terms by TF-IDF\n",
      "0.29\tfabric\n",
      "0.21\ttextiles\n",
      "0.20\tpesticides\n",
      "0.20\tweapons\n",
      "0.16\tprotect\n",
      "\n",
      "Document 3, top terms by TF-IDF\n",
      "0.21\tlightning\n",
      "0.18\tsnow\n",
      "0.17\tforecasting\n",
      "0.17\tsynoptic\n",
      "0.17\tdangerous\n",
      "\n",
      "Document 4, top terms by TF-IDF\n",
      "0.21\tdots\n",
      "0.20\ttutorials\n",
      "0.19\thelium\n",
      "0.18\tquantum\n",
      "0.17\tpath\n",
      "\n",
      "Cluster: 0 (427 docs)\n",
      "phase, fuel, sensor, market, polymer, optical, cell, devices, manufacturing, drug\n",
      "\n",
      "Cluster: 1 (225 docs)\n",
      "available, not, zooplankton, zoology, zones, zone, zonal, zno, zn, zircon\n",
      "\n",
      "Cluster: 2 (378 docs)\n",
      "chemistry, reactions, molecules, nmr, metal, complexes, organic, reaction, professor, catalysts\n",
      "\n",
      "Cluster: 3 (768 docs)\n",
      "ice, ocean, climate, arctic, carbon, soil, sea, variability, forest, atmospheric\n",
      "\n",
      "Cluster: 4 (359 docs)\n",
      "solar, galaxies, stars, wind, galaxy, star, waves, magnetosphere, magnetic, ionosphere\n",
      "\n",
      "Cluster: 5 (84 docs)\n",
      "fellowship, mathematical, sciences, postdoctoral, informatics, training, microbial, biology, minority, fellowships\n",
      "\n",
      "Cluster: 6 (379 docs)\n",
      "mantle, seismic, fault, deformation, magma, crust, subduction, earthquake, arc, lithosphere\n",
      "\n",
      "Cluster: 7 (157 docs)\n",
      "teachers, school, fellows, mathematics, districts, middle, teacher, schools, district, inquiry\n",
      "\n",
      "Cluster: 8 (580 docs)\n",
      "workshop, conference, meeting, symposium, 2002, 2003, participants, gordon, held, speakers\n",
      "\n",
      "Cluster: 9 (1070 docs)\n",
      "algorithms, estimation, visual, brain, optimization, recognition, stochastic, models, 3d, inference\n",
      "\n",
      "Cluster: 10 (320 docs)\n",
      "stem, college, scholarship, scholarships, csems, igert, colleges, minority, csem, scholars\n",
      "\n",
      "Cluster: 11 (154 docs)\n",
      "reu, summer, site, experience, week, chemistry, ten, 2003, undergraduates, colleges\n",
      "\n",
      "Cluster: 12 (477 docs)\n",
      "learning, curriculum, engineering, nsdl, digital, library, teachers, assessment, mathematics, stem\n",
      "\n",
      "Cluster: 13 (765 docs)\n",
      "wireless, software, power, sensor, distributed, network, networks, grid, computing, mobile\n",
      "\n",
      "Cluster: 14 (638 docs)\n",
      "equipment, french, center, contract, cnrs, dr, france, facility, abroad, us\n",
      "\n",
      "Cluster: 15 (498 docs)\n",
      "proteins, protein, genes, gene, cell, arabidopsis, expression, cells, plant, signaling\n",
      "\n",
      "Cluster: 16 (597 docs)\n",
      "species, genetic, evolutionary, populations, birds, plant, plants, reproductive, diversity, females\n",
      "\n",
      "Cluster: 17 (507 docs)\n",
      "equations, manifolds, algebraic, spaces, theory, geometry, geometric, algebras, hyperbolic, topology\n",
      "\n",
      "Cluster: 18 (825 docs)\n",
      "magnetic, spin, nanoscale, polymer, quantum, films, nano, optical, polymers, laser\n",
      "\n",
      "Cluster: 19 (715 docs)\n",
      "social, political, archaeological, cultural, language, women, policy, organizational, decision, firms\n",
      "\n",
      "CPU times: user 10min 38s, sys: 3min 23s, total: 14min 2s\n",
      "Wall time: 9min 3s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# So for this, I'll take out the top and bottom words\n",
    "# Essentially stop words and\n",
    "# words that are way too niche to be classified as descriptive\n",
    "# By enforcing max_features=10000\n",
    "# min_df is there to make sure of those 10000, there's no leftover\n",
    "# Also looser clusters because too many things got put together\n",
    "picky_setup = vectorize_cluster(\n",
    "    documents,\n",
    "    sample_size=None,\n",
    "    ngram_range=(1, 1),\n",
    "    analyzer='word',\n",
    "    max_df=0.95,\n",
    "    min_df=0.001,\n",
    "    max_features=10000,\n",
    "    use_idf=True,\n",
    "    sublinear_tf=True,\n",
    "    n_clusters=20,\n",
    "    tol=0.0001\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of the token matrix is: (9923, 2378)\n",
      "\n",
      "The top 20 tokens are:\n",
      "1.13\tthis\n",
      "1.14\tfor\n",
      "1.18\tis\n",
      "1.19\twill\n",
      "1.26\tbe\n",
      "1.29\ton\n",
      "1.31\twith\n",
      "1.33\tthat\n",
      "1.40\tare\n",
      "1.40\tresearch\n",
      "1.41\tby\n",
      "1.43\tas\n",
      "1.51\tfrom\n",
      "1.55\tan\n",
      "1.61\tthese\n",
      "1.61\tproject\n",
      "1.61\tat\n",
      "1.84\thave\n",
      "1.88\twhich\n",
      "1.92\tnew\n",
      "\n",
      "For the first 5 docs, the top tokens are:\n",
      "\n",
      "Document 0, top terms by TF-IDF\n",
      "0.28\tdrug\n",
      "0.26\tdatabase\n",
      "0.24\tdiscovery\n",
      "0.23\tdiseases\n",
      "0.21\tprotein\n",
      "\n",
      "Document 1, top terms by TF-IDF\n",
      "0.15\tchemistry\n",
      "0.15\tnanostructures\n",
      "0.15\tassociates\n",
      "0.15\tdetected\n",
      "0.15\tacademia\n",
      "\n",
      "Document 2, top terms by TF-IDF\n",
      "0.23\tmedical\n",
      "0.18\ttesting\n",
      "0.17\tworkers\n",
      "0.17\tcould\n",
      "0.17\tpersonnel\n",
      "\n",
      "Document 3, top terms by TF-IDF\n",
      "0.18\tobservations\n",
      "0.17\tcollection\n",
      "0.17\tvertical\n",
      "0.16\tweather\n",
      "0.16\tsummer\n",
      "\n",
      "Document 4, top terms by TF-IDF\n",
      "0.22\tquantum\n",
      "0.22\tpath\n",
      "0.21\tnanostructures\n",
      "0.18\tsemiconductor\n",
      "0.17\tintegral\n",
      "\n",
      "Cluster: 0 (398 docs)\n",
      "chemistry, molecules, reactions, organic, metal, complexes, professor, compounds, reaction, chemical\n",
      "\n",
      "Cluster: 1 (631 docs)\n",
      "solar, stars, wind, magnetic, formation, waves, observations, plasma, radar, flow\n",
      "\n",
      "Cluster: 2 (470 docs)\n",
      "protein, proteins, genes, gene, cell, expression, plant, cells, plants, dna\n",
      "\n",
      "Cluster: 3 (426 docs)\n",
      "learning, engineering, curriculum, courses, course, faculty, assessment, education, modules, design\n",
      "\n",
      "Cluster: 4 (640 docs)\n",
      "software, wireless, network, distributed, power, computing, performance, networks, sensor, grid\n",
      "\n",
      "Cluster: 5 (428 docs)\n",
      "phase, commercial, ii, cost, optical, market, devices, fuel, sensor, power\n",
      "\n",
      "Cluster: 6 (620 docs)\n",
      "species, evolutionary, populations, genetic, plant, plants, diversity, population, tree, ecological\n",
      "\n",
      "Cluster: 7 (485 docs)\n",
      "theory, equations, geometry, manifolds, spaces, algebraic, geometric, topology, differential, dimensional\n",
      "\n",
      "Cluster: 8 (335 docs)\n",
      "mantle, seismic, fault, deformation, crust, rocks, tectonic, plate, continental, zone\n",
      "\n",
      "Cluster: 9 (263 docs)\n",
      "college, stem, scholarship, mathematics, faculty, academic, programs, engineering, students, minority\n",
      "\n",
      "Cluster: 10 (627 docs)\n",
      "climate, ice, ocean, carbon, arctic, sea, water, circulation, atmospheric, soil\n",
      "\n",
      "Cluster: 11 (780 docs)\n",
      "materials, magnetic, quantum, nanoscale, polymer, spin, films, nano, optical, electron\n",
      "\n",
      "Cluster: 12 (225 docs)\n",
      "available, not, zones, zone, young, york, yield, yet, years, year\n",
      "\n",
      "Cluster: 13 (545 docs)\n",
      "workshop, conference, meeting, symposium, 2002, participants, held, speakers, 2003, travel\n",
      "\n",
      "Cluster: 14 (788 docs)\n",
      "center, equipment, dr, us, facility, digital, marine, industry, university, international\n",
      "\n",
      "Cluster: 15 (1007 docs)\n",
      "algorithms, models, optimization, problems, estimation, computational, control, we, visual, statistical\n",
      "\n",
      "Cluster: 16 (84 docs)\n",
      "fellowship, mathematical, sciences, postdoctoral, training, microbial, biology, biological, minority, further\n",
      "\n",
      "Cluster: 17 (751 docs)\n",
      "social, political, policy, archaeological, decision, cultural, economic, market, labor, children\n",
      "\n",
      "Cluster: 18 (267 docs)\n",
      "teachers, school, mathematics, teacher, fellows, schools, middle, districts, 12, inquiry\n",
      "\n",
      "Cluster: 19 (153 docs)\n",
      "reu, site, summer, students, experience, recruited, week, projects, chemistry, undergraduates\n",
      "\n",
      "CPU times: user 8min 2s, sys: 3min 14s, total: 11min 16s\n",
      "Wall time: 6min 33s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# So for this, I'll take out the top and bottom words\n",
    "# Essentially stop words and\n",
    "# words that are way too niche to be classified as descriptive\n",
    "# By enforcing max_features=10000\n",
    "# min_df is there to make sure of those 10000, there's no leftover\n",
    "# Also looser clusters because too many things got put together\n",
    "pickier_setup = vectorize_cluster(\n",
    "    documents,\n",
    "    sample_size=None,\n",
    "    ngram_range=(1, 1),\n",
    "    analyzer='word',\n",
    "    max_df=0.95,\n",
    "    min_df=0.01,\n",
    "    max_features=10000,\n",
    "    use_idf=True,\n",
    "    sublinear_tf=True,\n",
    "    n_clusters=20,\n",
    "    tol=0.0001\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I tried with even stricter setup and it's a bit better, not too clear but I can identify topic much quicker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of the token matrix is: (9923, 10000)\n",
      "\n",
      "The top 20 tokens are:\n",
      "1.13\tthis\n",
      "1.14\tfor\n",
      "1.18\tis\n",
      "1.19\twill\n",
      "1.22\tof the\n",
      "1.26\tbe\n",
      "1.29\ton\n",
      "1.31\twith\n",
      "1.33\tthat\n",
      "1.40\tare\n",
      "1.40\tresearch\n",
      "1.40\tin the\n",
      "1.41\tby\n",
      "1.43\tas\n",
      "1.51\tfrom\n",
      "1.55\tan\n",
      "1.57\twill be\n",
      "1.61\tthese\n",
      "1.61\tproject\n",
      "1.61\tat\n",
      "\n",
      "For the first 5 docs, the top tokens are:\n",
      "\n",
      "Document 0, top terms by TF-IDF\n",
      "0.19\tdrug\n",
      "0.18\tthe database\n",
      "0.17\tdatabase\n",
      "0.16\tdiscovery\n",
      "0.15\tdiseases\n",
      "\n",
      "Document 1, top terms by TF-IDF\n",
      "0.20\tnmr\n",
      "0.13\tand undergraduate\n",
      "0.12\tgraduate and\n",
      "0.11\tgaas\n",
      "0.11\theterostructures\n",
      "\n",
      "Document 2, top terms by TF-IDF\n",
      "0.28\tfabric\n",
      "0.16\tprotect\n",
      "0.15\tmedical\n",
      "0.15\tmilitary\n",
      "0.13\tpolymerization\n",
      "\n",
      "Document 3, top terms by TF-IDF\n",
      "0.19\tlightning\n",
      "0.16\tsnow\n",
      "0.15\tforecasting\n",
      "0.12\twinter\n",
      "0.12\tcloud\n",
      "\n",
      "Document 4, top terms by TF-IDF\n",
      "0.17\tdots\n",
      "0.16\thelium\n",
      "0.14\tquantum\n",
      "0.14\tpath\n",
      "0.14\tstatistical mechanics\n",
      "\n",
      "Cluster: 0 (351 docs)\n",
      "mantle, seismic, fault, deformation, magma, subduction, lithosphere, crust, the mantle, rocks\n",
      "\n",
      "Cluster: 1 (774 docs)\n",
      "galaxies, stars, star, quantum, galaxy, stellar, star formation, laser, particles, dark\n",
      "\n",
      "Cluster: 2 (791 docs)\n",
      "social, political, archaeological, decision, firms, policy, cultural, children, language, organizational\n",
      "\n",
      "Cluster: 3 (876 docs)\n",
      "software, wireless, distributed, power, sensor, grid, computing, network, networks, mobile\n",
      "\n",
      "Cluster: 4 (568 docs)\n",
      "workshop, conference, the workshop, the conference, meeting, symposium, workshop will, conference will, 2002, this workshop\n",
      "\n",
      "Cluster: 5 (545 docs)\n",
      "species, evolutionary, genetic, populations, birds, plants, reproductive, plant, females, diversity\n",
      "\n",
      "Cluster: 6 (538 docs)\n",
      "genes, proteins, protein, gene, fellowship, cell, plant, signaling, cells, expression\n",
      "\n",
      "Cluster: 7 (667 docs)\n",
      "algorithms, estimation, optimization, nonlinear, stochastic, inference, regression, linear, image, equations\n",
      "\n",
      "Cluster: 8 (225 docs)\n",
      "not available, available, not, zooplankton, zones, zone, zero, zealand, yr, younger\n",
      "\n",
      "Cluster: 9 (295 docs)\n",
      "stem, scholarship, scholarships, csems, college, community college, igert, computer science, the program, the college\n",
      "\n",
      "Cluster: 10 (32 docs)\n",
      "fellowship, mathematical sciences, mathematical, sciences, postdoctoral, fellowships, zooplankton, zones, zone, zero\n",
      "\n",
      "Cluster: 11 (439 docs)\n",
      "phase ii, phase, cost, optical, sensor, fuel, ii project, commercial, market, ii\n",
      "\n",
      "Cluster: 12 (560 docs)\n",
      "spin, magnetic, nanoscale, nano, films, polymer, nanotubes, thin, optical, quantum\n",
      "\n",
      "Cluster: 13 (434 docs)\n",
      "manifolds, equations, spaces, algebraic, geometry, theory, geometric, the investigator, algebras, hyperbolic\n",
      "\n",
      "Cluster: 14 (279 docs)\n",
      "chemistry, molecules, reactions, complexes, metal, organic, nmr, catalysts, compounds, professor\n",
      "\n",
      "Cluster: 15 (152 docs)\n",
      "reu, reu site, students will, research experience, the program, summer, the reu, chemistry, 2003, the chemistry\n",
      "\n",
      "Cluster: 16 (493 docs)\n",
      "ice, climate, carbon, soil, forest, arctic, ecosystem, co2, ocean, lake\n",
      "\n",
      "Cluster: 17 (502 docs)\n",
      "teachers, learning, mathematics, curriculum, teacher, fellows, engineering, middle, school, districts\n",
      "\n",
      "Cluster: 18 (613 docs)\n",
      "solar, ocean, wind, ice, this is, ionosphere, waves, solar wind, radar, variability\n",
      "\n",
      "Cluster: 19 (789 docs)\n",
      "equipment, french, center, the center, cnrs, dr, the us, us, france, facility\n",
      "\n",
      "CPU times: user 12min 20s, sys: 2min 46s, total: 15min 7s\n",
      "Wall time: 11min 3s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# So for this, it will be the same as above\n",
    "# but with 2 words as well\n",
    "two_words_setup = vectorize_cluster(\n",
    "    documents,\n",
    "    sample_size=None,\n",
    "    ngram_range=(1, 2),\n",
    "    analyzer='word',\n",
    "    max_df=0.95,\n",
    "    min_df=0.001,\n",
    "    max_features=10000,\n",
    "    use_idf=True,\n",
    "    sublinear_tf=True,\n",
    "    n_clusters=20,\n",
    "    tol=0.0001\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This one is good too but at some point it's quite messed up so I'll go with the `pickier_setup`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0\n",
      "0.0058\treef\n",
      "0.0044\tcoral\n",
      "0.0044\tcorals\n",
      "0.0036\trig\n",
      "0.0035\tpreK\n",
      "0.0030\tWGBH\n",
      "0.0028\tPM\n",
      "0.0025\treefs\n",
      "0.0020\tjob\n",
      "0.0012\tjudicial\n",
      "\n",
      "Topic 1\n",
      "0.0374\tAvailable\n",
      "0.0111\trouting\n",
      "0.0052\tprotocol\n",
      "0.0050\tquery\n",
      "0.0028\tterminals\n",
      "0.0025\tadaptivity\n",
      "0.0023\tbox\n",
      "0.0022\tRPI\n",
      "0.0020\tvolatility\n",
      "0.0019\tInternet\n",
      "\n",
      "Topic 2\n",
      "0.0088\tresearch\n",
      "0.0035\tproject\n",
      "0.0034\ttheir\n",
      "0.0027\tworkshop\n",
      "0.0025\twhich\n",
      "0.0024\tstudy\n",
      "0.0022\tinformation\n",
      "0.0022\tdevelopment\n",
      "0.0021\thow\n",
      "0.0021\tdata\n",
      "\n",
      "Topic 3\n",
      "0.0132\tgirls\n",
      "0.0093\tchildren\n",
      "0.0083\tscience\n",
      "0.0036\ttheir\n",
      "0.0034\tproject\n",
      "0.0033\tresearch\n",
      "0.0033\tstudents\n",
      "0.0023\tgeoscience\n",
      "0.0020\tabout\n",
      "0.0019\tfMRI\n",
      "\n",
      "Topic 4\n",
      "0.0090\tdata\n",
      "0.0042\tproject\n",
      "0.0028\tsuch\n",
      "0.0028\tmodel\n",
      "0.0026\twhich\n",
      "0.0026\tbetween\n",
      "0.0026\tresearch\n",
      "0.0025\tmodels\n",
      "0.0025\thow\n",
      "0.0024\tinformation\n",
      "\n",
      "Topic 5\n",
      "0.0088\ttheory\n",
      "0.0058\tproblems\n",
      "0.0053\tstudy\n",
      "0.0053\twhich\n",
      "0.0049\tsystems\n",
      "0.0048\tequations\n",
      "0.0043\tproject\n",
      "0.0040\tsuch\n",
      "0.0034\tgeometry\n",
      "0.0034\tmathematical\n",
      "\n",
      "Topic 6\n",
      "0.0060\tproject\n",
      "0.0041\tPhase\n",
      "0.0037\thigh\n",
      "0.0034\tDNA\n",
      "0.0033\tresearch\n",
      "0.0030\tdevelopment\n",
      "0.0028\tbased\n",
      "0.0027\tsystem\n",
      "0.0026\tgene\n",
      "0.0024\tdevelop\n",
      "\n",
      "Topic 7\n",
      "0.0036\tproject\n",
      "0.0030\twhich\n",
      "0.0029\tresearch\n",
      "0.0026\tcell\n",
      "0.0026\tgenes\n",
      "0.0025\tspecies\n",
      "0.0025\tunderstanding\n",
      "0.0023\tbetween\n",
      "0.0023\ttheir\n",
      "0.0022\tused\n",
      "\n",
      "Topic 8\n",
      "0.0083\tresearch\n",
      "0.0043\tproject\n",
      "0.0033\tsystems\n",
      "0.0028\tsystem\n",
      "0.0027\tbased\n",
      "0.0027\thigh\n",
      "0.0026\tsuch\n",
      "0.0026\tdesign\n",
      "0.0026\twhich\n",
      "0.0022\tuse\n",
      "\n",
      "Topic 9\n",
      "0.0104\tnoncommutative\n",
      "0.0042\tKaehler\n",
      "0.0042\tadic\n",
      "0.0027\tBaum\n",
      "0.0027\tmanifolds\n",
      "0.0021\tstring\n",
      "0.0020\tlattices\n",
      "0.0015\tDiophantine\n",
      "0.0013\tHopf\n",
      "0.0011\ttheory\n",
      "\n",
      "Topic 10\n",
      "0.0031\tpush\n",
      "0.0013\tdata\n",
      "0.0013\tmobile\n",
      "0.0012\tbroadcast\n",
      "0.0008\twireless\n",
      "0.0007\tscience\n",
      "0.0006\tstudents\n",
      "0.0005\twhich\n",
      "0.0004\tresearch\n",
      "0.0004\tusers\n",
      "\n",
      "Topic 11\n",
      "0.0048\tXML\n",
      "0.0047\tvesicles\n",
      "0.0044\tdata\n",
      "0.0039\tproject\n",
      "0.0033\tcompressed\n",
      "0.0032\tmanifold\n",
      "0.0026\tfusion\n",
      "0.0024\tresearch\n",
      "0.0023\ttext\n",
      "0.0022\twhich\n",
      "\n",
      "Topic 12\n",
      "0.0038\tretreat\n",
      "0.0034\tDFT\n",
      "0.0034\tresearch\n",
      "0.0034\tCLR\n",
      "0.0025\tproject\n",
      "0.0025\tproprietary\n",
      "0.0018\tsystem\n",
      "0.0018\tlegume\n",
      "0.0017\tsubducting\n",
      "0.0015\tsystems\n",
      "\n",
      "Topic 13\n",
      "0.0129\tUniversity\n",
      "0.0101\tresearch\n",
      "0.0098\tchemistry\n",
      "0.0083\tChemistry\n",
      "0.0067\torganic\n",
      "0.0064\treactions\n",
      "0.0059\taward\n",
      "0.0057\tstudents\n",
      "0.0051\tmolecules\n",
      "0.0048\tmaterials\n",
      "\n",
      "Topic 14\n",
      "0.0076\teruptions\n",
      "0.0066\tcrystallization\n",
      "0.0045\teruptive\n",
      "0.0045\tdegassing\n",
      "0.0043\tmagma\n",
      "0.0023\talkaline\n",
      "0.0018\teruption\n",
      "0.0017\tmagmas\n",
      "0.0016\tEAR\n",
      "0.0015\tOs\n",
      "\n",
      "Topic 15\n",
      "0.0192\tstudents\n",
      "0.0109\tresearch\n",
      "0.0098\tprogram\n",
      "0.0080\tscience\n",
      "0.0066\tUniversity\n",
      "0.0055\tfaculty\n",
      "0.0054\tproject\n",
      "0.0053\tengineering\n",
      "0.0051\ttheir\n",
      "0.0050\tprograms\n",
      "\n",
      "Topic 16\n",
      "0.0051\tmodels\n",
      "0.0043\tresearch\n",
      "0.0042\tproject\n",
      "0.0039\tflow\n",
      "0.0038\tdynamics\n",
      "0.0036\tmodeling\n",
      "0.0036\tfluid\n",
      "0.0031\tcomputational\n",
      "0.0029\tscale\n",
      "0.0028\twaves\n",
      "\n",
      "Topic 17\n",
      "0.0048\tresearch\n",
      "0.0048\tdata\n",
      "0.0037\tproject\n",
      "0.0036\tstudy\n",
      "0.0025\tbetween\n",
      "0.0025\twhich\n",
      "0.0023\tprovide\n",
      "0.0023\tclimate\n",
      "0.0022\tspecies\n",
      "0.0022\thow\n",
      "\n",
      "Topic 18\n",
      "0.0112\tmaterials\n",
      "0.0057\tresearch\n",
      "0.0053\tproperties\n",
      "0.0049\toptical\n",
      "0.0043\tproject\n",
      "0.0043\tdevices\n",
      "0.0041\tsurface\n",
      "0.0039\tmagnetic\n",
      "0.0037\thigh\n",
      "0.0034\tused\n",
      "\n",
      "Topic 19\n",
      "0.0035\tRAM\n",
      "0.0015\ttape\n",
      "0.0012\tPipeline\n",
      "0.0012\tbackup\n",
      "0.0011\treflectance\n",
      "0.0009\tdata\n",
      "0.0009\tproject\n",
      "0.0007\tresearch\n",
      "0.0007\tCPU\n",
      "0.0006\tscience\n",
      "\n",
      "CPU times: user 26.3 s, sys: 16.7 ms, total: 26.3 s\n",
      "Wall time: 26.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "new_vectorizer = TfidfVectorizer(\n",
    "    documents,\n",
    "    ngram_range=(1, 1),\n",
    "    analyzer='word',\n",
    "    max_df=0.95,\n",
    "    min_df=0.01,\n",
    "    max_features=10000,\n",
    "    use_idf=True,\n",
    "    sublinear_tf=True\n",
    ")\n",
    "# new_vectorizer = TfidfVectorizer()\n",
    "word_tokenizer = new_vectorizer.build_tokenizer()\n",
    "tokenized_text = [word_tokenizer(doc) for doc in documents]\n",
    "\n",
    "dictionary = corpora.Dictionary(tokenized_text)\n",
    "lda_corpus = [dictionary.doc2bow(text) for text in tokenized_text]\n",
    "lda_model = models.LdaModel(lda_corpus, id2word=dictionary, num_topics=20)\n",
    "\n",
    "for i, topic in lda_model.show_topics(num_topics=20, num_words=50, formatted=False):\n",
    "    print(\"Topic\", i)\n",
    "    printed_terms = 0\n",
    "    for term, score in topic:\n",
    "        if printed_terms >= 10:\n",
    "            break\n",
    "        elif term in \"this This that That these These have will Will\\\n",
    "        the of and to for in or The is be may an a with at are on by as from can \\\n",
    "        In it It has Has also not Not new\".split():\n",
    "            continue\n",
    "        printed_terms += 1\n",
    "        print(\"%.4f\\t%s\" % (score,term))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's rather quick and output the results quite nicely. But with a little help of preprocessing on the tokenizer to filter out the noises, it's more meaningful than on its own"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"word-vector\"></a>\n",
    "### Word vectors\n",
    "\n",
    "[top](#first-part)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
