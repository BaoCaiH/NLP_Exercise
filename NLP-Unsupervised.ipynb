{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"first-part\"></a>\n",
    "\n",
    "#!/usr/bin/env python<br># coding: utf-8\n",
    "\n",
    "Author: Bao Cai\n",
    "\n",
    "Course: Machine Learning for Descriptive Problems\n",
    "\n",
    "Topic: NLP-Unsupervised\n",
    "\n",
    "Start Date: 2020-03-11\n",
    "\n",
    "Last Save: 2020-03-11"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[1. Topic analysis](#topic-part)\n",
    "\n",
    "There are quantitative measures for evaluating various aspects of clustering results and topic models intrinsically, and they can also be evaluated extrinsically by how well the clusters/topics serve some supervised task as features. In this exervise, however, we well fovus on qualitative evaluation of the results in terms of their descriptiveness. As in the example code, you may limit yourself to the 1000 first documents of ther corpus when performing clustering, in order to simplify the task and speed up experimentation, but use the whole corpus to calculate tf-idf features.\n",
    "\n",
    "a. Experiment with different setups of the tf-idf feature extraction and clustering (k-means or hierarchical). In order to obtain meaningful results. When you arrive at a good configuration, describe it and motivate your chosen setup/parameters.\n",
    "\n",
    "b. Inspect the keywords of the clusters. List the 10 first clusters out of all (i.e. not cherry picked examples) and privde an as descriptive label as possible for each of them.\n",
    "\n",
    "c. Select one or two good clusters (that can be clearly interpreted) and one or two bad clusters (that might be difficult to interpret or distinguish). Motivate your choise (clusters may for instance, be overlapping, to broad/narrow or incoherent).\n",
    "\n",
    "d. Repeat the experiment in (a) with LDA topic modelling instead (on the whole corpus), and explain briefly how the results compare to your previously chosen clustering setup. A few concrete examples may be helpful. Do your best to make sure the list of topic keywords are informative through appropriate post-processing.\n",
    "\n",
    "[2. Word vectors](#word-vector)\n",
    "\n",
    "a. Choose about 5 words (arbitrary) to use as seed words in the following experiment. Train word2vec vectors on the corpus while trying out variations on the parameters. Evaluate the vector models by inspecting the most similar words for each of the seed words, and try to identify qualitative differences between different parameter choices. Which parameters seem to have the most interesting effect? At what values? Motivate. Finally, study the qualitative effect of increasing the training data, by similarly comparing vectors trained with the best setup on the texts from the awards_2020 directory against vectors trained on the whole set of abstracts (1990-2002).\n",
    "\n",
    "b. Repeat the experiment with ELMo from the lecture, with a different target word and diferent sentences. Choose a word that can have multiple senses, and construct 10 sentences that express 2-3 different senses of the word. Produce ELMo embeddings for the target word in each sentence and measure the similarity between the vectors. Evaluate in how many cases the measured similarities can be used to successfully distinguish between the different senses. Comment on the results, e.g. are you able to identify a particular way in which the model fails?\n",
    "\n",
    "[top](#first-part)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "import os\n",
    "import re\n",
    "import binascii\n",
    "import itertools\n",
    "import heapq\n",
    "import gensim\n",
    "import logging\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from time import time\n",
    "from collections import Counter\n",
    "from gensim import corpora, models\n",
    "from scipy.spatial.distance import cosine\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage, fcluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions\n",
    "def get_fnames(path='./Data'):\n",
    "    \"\"\"Read all text files in a folder.\n",
    "    \"\"\"\n",
    "    fnames = []\n",
    "    for root, _, files in os.walk(path):\n",
    "        for fname in files:\n",
    "            if fname[-4:] == '.txt':\n",
    "                fnames.append(os.path.join(root, fname))\n",
    "    return fnames\n",
    "\n",
    "\n",
    "def read_file(fname):\n",
    "    with open(fname, 'rt', encoding='latin-1') as f:\n",
    "        # skip all lines until abstract\n",
    "        for line in f:\n",
    "            if \"Abstract    :\" in line:\n",
    "                break\n",
    "\n",
    "        # get abstract as a single string\n",
    "        abstract = ' '.join([line[:-1].strip() for line in f])\n",
    "        abstract = re.sub(' +', ' ', abstract)\n",
    "        return abstract\n",
    "\n",
    "\n",
    "def print_clusters(matrix, clusters, features, n_keywords=10):\n",
    "    for cluster in range(min(clusters), max(clusters)+1):\n",
    "        cluster_docs = [i for i, c in enumerate(clusters) if c == cluster]\n",
    "        print(\"Cluster: %d (%d docs)\" % (cluster, len(cluster_docs)))\n",
    "        \n",
    "        # Keep scores for top n terms\n",
    "        new_matrix = np.zeros((len(cluster_docs), matrix.shape[1]))\n",
    "        for cluster_i, doc_vec in enumerate(matrix[cluster_docs].toarray()):\n",
    "            for idx, score in heapq.nlargest(n_keywords, enumerate(doc_vec), key=lambda x:x[1]):\n",
    "                new_matrix[cluster_i][idx] = score\n",
    "\n",
    "        # Aggregate scores for kept top terms\n",
    "        keywords = heapq.nlargest(n_keywords, zip(new_matrix.sum(axis=0), features))\n",
    "        print(', '.join([w for s,w in keywords]))\n",
    "        print()\n",
    "\n",
    "\n",
    "def vectorize_cluster(\n",
    "    documents,\n",
    "    sample_size=None,\n",
    "    ngram_range=(1, 1),\n",
    "    analyzer='word',\n",
    "    max_df=1.0,\n",
    "    min_df=0.0,\n",
    "    max_features=None,\n",
    "    use_idf=True,\n",
    "    sublinear_tf=True,\n",
    "    n_clusters=10,\n",
    "    tol=0.0001\n",
    "):\n",
    "    vectorizer = TfidfVectorizer(\n",
    "        ngram_range=ngram_range,\n",
    "        analyzer=analyzer,\n",
    "        max_df=max_df,\n",
    "        min_df=min_df,\n",
    "        max_features=max_features,\n",
    "        use_idf=use_idf,\n",
    "        sublinear_tf=sublinear_tf\n",
    "    )\n",
    "    token_matrix = vectorizer.fit_transform(documents)\n",
    "    print(\n",
    "        'The shape of the token matrix is:',\n",
    "        token_matrix.toarray().shape\n",
    "    )\n",
    "    print()\n",
    "    features = vectorizer.get_feature_names()\n",
    "    print('The top 20 tokens are:')\n",
    "    for feature, idf in sorted(\n",
    "        zip(features, vectorizer._tfidf.idf_),\n",
    "        key=lambda x:x[1]\n",
    "    )[:20]:\n",
    "        print('{:.2f}\\t{}'.format(idf, feature))\n",
    "    print()\n",
    "    print('For the first 5 docs, the top tokens are:')\n",
    "    for i in range(5):\n",
    "        print('\\nDocument {}, top terms by TF-IDF'.format(i))\n",
    "        for feature, score in sorted(\n",
    "            zip(features, token_matrix.toarray()[i]),\n",
    "            key=lambda x: -x[1]\n",
    "        )[:5]:\n",
    "            print('{:.2f}\\t{}'.format(score, feature))\n",
    "    print()\n",
    "    if not sample_size:\n",
    "        matrix_to_cluster = token_matrix\n",
    "    else:\n",
    "        matrix_to_cluster = token_matrix[:sample_size]\n",
    "    km = KMeans(\n",
    "        n_clusters=n_clusters,\n",
    "        tol=tol,\n",
    "        random_state=44,\n",
    "        verbose=False\n",
    "    )\n",
    "    km.fit(matrix_to_cluster)\n",
    "    print_clusters(matrix_to_cluster, km.labels_, features)\n",
    "    return (\n",
    "        vectorizer,\n",
    "        token_matrix,\n",
    "        features,\n",
    "        matrix_to_cluster,\n",
    "        km\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.18 s, sys: 139 ms, total: 1.32 s\n",
      "Wall time: 1.32 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "documents = [read_file(file) for file in get_fnames('./Data/Arcada_BigDataAnalytic/')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"topic-part\"></a>\n",
    "### Topic analysis\n",
    "\n",
    "[top](#first-part)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of the token matrix is: (9923, 53816)\n",
      "\n",
      "The top 20 tokens are:\n",
      "1.03\tthe\n",
      "1.03\tof\n",
      "1.03\tand\n",
      "1.04\tto\n",
      "1.05\tin\n",
      "1.13\tthis\n",
      "1.14\tfor\n",
      "1.18\tis\n",
      "1.19\twill\n",
      "1.26\tbe\n",
      "1.29\ton\n",
      "1.31\twith\n",
      "1.33\tthat\n",
      "1.40\tare\n",
      "1.40\tresearch\n",
      "1.41\tby\n",
      "1.43\tas\n",
      "1.51\tfrom\n",
      "1.55\tan\n",
      "1.61\tthese\n",
      "\n",
      "For the first 5 docs, the top tokens are:\n",
      "\n",
      "Document 0, top terms by TF-IDF\n",
      "0.34\ttrafficking\n",
      "0.22\tdrug\n",
      "0.20\tdatabase\n",
      "0.19\tdiscovery\n",
      "0.18\tmislocalization\n",
      "\n",
      "Document 1, top terms by TF-IDF\n",
      "0.25\tnmr\n",
      "0.23\toptically\n",
      "0.20\tingap\n",
      "0.18\thayes\n",
      "0.17\tinp\n",
      "\n",
      "Document 2, top terms by TF-IDF\n",
      "0.25\tfabric\n",
      "0.19\ttextiles\n",
      "0.18\tpesticides\n",
      "0.18\tweapons\n",
      "0.18\tdrapes\n",
      "\n",
      "Document 3, top terms by TF-IDF\n",
      "0.33\tthundersnow\n",
      "0.19\trawinsonde\n",
      "0.18\tlightning\n",
      "0.16\tsnow\n",
      "0.15\tforecasting\n",
      "\n",
      "Document 4, top terms by TF-IDF\n",
      "0.20\tdots\n",
      "0.19\ttutorials\n",
      "0.18\thelium\n",
      "0.17\tcondensates\n",
      "0.17\tquantum\n",
      "Cluster: 0 (1505 docs)\n",
      "polymer, magnetic, spin, nano, laser, films, quantum, optical, nanoscale, nanoparticles\n",
      "\n",
      "Cluster: 1 (1494 docs)\n",
      "species, birds, political, social, genetic, populations, evolutionary, language, decision, firms\n",
      "\n",
      "Cluster: 2 (834 docs)\n",
      "protein, proteins, genes, nmr, arabidopsis, gene, complexes, genome, dna, cell\n",
      "\n",
      "Cluster: 3 (1791 docs)\n",
      "ice, ocean, mantle, solar, climate, galaxies, arctic, seismic, stars, fault\n",
      "\n",
      "Cluster: 4 (682 docs)\n",
      "equations, manifolds, spaces, algebraic, quantum, algebras, hyperbolic, geometry, topology, random\n",
      "\n",
      "Cluster: 5 (1064 docs)\n",
      "teachers, reu, stem, engineering, mathematics, curriculum, learning, scholarship, college, fellows\n",
      "\n",
      "Cluster: 6 (62 docs)\n",
      "fellowship, mathematical, sciences, postdoctoral, informatics, training, fellowships, minority, biological, genetic\n",
      "\n",
      "Cluster: 7 (1608 docs)\n",
      "wireless, software, grid, sensor, mobile, networks, power, security, computing, visualization\n",
      "\n",
      "Cluster: 8 (225 docs)\n",
      "available, not, zyss, zylstra, zygotically, zygotic, zygote, zygomycota, zygomycetes, zygomorphy\n",
      "\n",
      "Cluster: 9 (658 docs)\n",
      "workshop, conference, symposium, meeting, gordon, 2002, sessions, 2003, session, eu\n",
      "\n",
      "CPU times: user 11min 27s, sys: 3min 11s, total: 14min 39s\n",
      "Wall time: 10min 37s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "default_setup = vectorize_cluster(\n",
    "    documents,\n",
    "    sample_size=None,\n",
    "    ngram_range=(1, 1),\n",
    "    analyzer='word',\n",
    "    max_df=1.0,\n",
    "    min_df=0.0,\n",
    "    max_features=None,\n",
    "    use_idf=True,\n",
    "    sublinear_tf=True,\n",
    "    n_clusters=10,\n",
    "    tol=0.0001\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of the token matrix is: (9923, 10000)\n",
      "\n",
      "The top 20 tokens are:\n",
      "1.13\tthis\n",
      "1.14\tfor\n",
      "1.18\tis\n",
      "1.19\twill\n",
      "1.26\tbe\n",
      "1.29\ton\n",
      "1.31\twith\n",
      "1.33\tthat\n",
      "1.40\tare\n",
      "1.40\tresearch\n",
      "1.41\tby\n",
      "1.43\tas\n",
      "1.51\tfrom\n",
      "1.55\tan\n",
      "1.61\tthese\n",
      "1.61\tproject\n",
      "1.61\tat\n",
      "1.84\thave\n",
      "1.88\twhich\n",
      "1.92\tnew\n",
      "\n",
      "For the first 5 docs, the top tokens are:\n",
      "\n",
      "Document 0, top terms by TF-IDF\n",
      "0.36\ttrafficking\n",
      "0.24\tdrug\n",
      "0.22\tdatabase\n",
      "0.20\tdiscovery\n",
      "0.19\tdiseases\n",
      "\n",
      "Document 1, top terms by TF-IDF\n",
      "0.27\tnmr\n",
      "0.24\toptically\n",
      "0.15\twells\n",
      "0.15\tgaas\n",
      "0.15\theterostructures\n",
      "\n",
      "Document 2, top terms by TF-IDF\n",
      "0.29\tfabric\n",
      "0.21\ttextiles\n",
      "0.20\tpesticides\n",
      "0.20\tweapons\n",
      "0.16\tprotect\n",
      "\n",
      "Document 3, top terms by TF-IDF\n",
      "0.21\tlightning\n",
      "0.18\tsnow\n",
      "0.17\tforecasting\n",
      "0.17\tsynoptic\n",
      "0.17\tdangerous\n",
      "\n",
      "Document 4, top terms by TF-IDF\n",
      "0.21\tdots\n",
      "0.20\ttutorials\n",
      "0.19\thelium\n",
      "0.18\tquantum\n",
      "0.17\tpath\n",
      "\n",
      "Cluster: 0 (427 docs)\n",
      "phase, fuel, sensor, market, polymer, optical, cell, devices, manufacturing, drug\n",
      "\n",
      "Cluster: 1 (225 docs)\n",
      "available, not, zooplankton, zoology, zones, zone, zonal, zno, zn, zircon\n",
      "\n",
      "Cluster: 2 (378 docs)\n",
      "chemistry, reactions, molecules, nmr, metal, complexes, organic, reaction, professor, catalysts\n",
      "\n",
      "Cluster: 3 (768 docs)\n",
      "ice, ocean, climate, arctic, carbon, soil, sea, variability, forest, atmospheric\n",
      "\n",
      "Cluster: 4 (359 docs)\n",
      "solar, galaxies, stars, wind, galaxy, star, waves, magnetosphere, magnetic, ionosphere\n",
      "\n",
      "Cluster: 5 (84 docs)\n",
      "fellowship, mathematical, sciences, postdoctoral, informatics, training, microbial, biology, minority, fellowships\n",
      "\n",
      "Cluster: 6 (379 docs)\n",
      "mantle, seismic, fault, deformation, magma, crust, subduction, earthquake, arc, lithosphere\n",
      "\n",
      "Cluster: 7 (157 docs)\n",
      "teachers, school, fellows, mathematics, districts, middle, teacher, schools, district, inquiry\n",
      "\n",
      "Cluster: 8 (580 docs)\n",
      "workshop, conference, meeting, symposium, 2002, 2003, participants, gordon, held, speakers\n",
      "\n",
      "Cluster: 9 (1070 docs)\n",
      "algorithms, estimation, visual, brain, optimization, recognition, stochastic, models, 3d, inference\n",
      "\n",
      "Cluster: 10 (320 docs)\n",
      "stem, college, scholarship, scholarships, csems, igert, colleges, minority, csem, scholars\n",
      "\n",
      "Cluster: 11 (154 docs)\n",
      "reu, summer, site, experience, week, chemistry, ten, 2003, undergraduates, colleges\n",
      "\n",
      "Cluster: 12 (477 docs)\n",
      "learning, curriculum, engineering, nsdl, digital, library, teachers, assessment, mathematics, stem\n",
      "\n",
      "Cluster: 13 (765 docs)\n",
      "wireless, software, power, sensor, distributed, network, networks, grid, computing, mobile\n",
      "\n",
      "Cluster: 14 (638 docs)\n",
      "equipment, french, center, contract, cnrs, dr, france, facility, abroad, us\n",
      "\n",
      "Cluster: 15 (498 docs)\n",
      "proteins, protein, genes, gene, cell, arabidopsis, expression, cells, plant, signaling\n",
      "\n",
      "Cluster: 16 (597 docs)\n",
      "species, genetic, evolutionary, populations, birds, plant, plants, reproductive, diversity, females\n",
      "\n",
      "Cluster: 17 (507 docs)\n",
      "equations, manifolds, algebraic, spaces, theory, geometry, geometric, algebras, hyperbolic, topology\n",
      "\n",
      "Cluster: 18 (825 docs)\n",
      "magnetic, spin, nanoscale, polymer, quantum, films, nano, optical, polymers, laser\n",
      "\n",
      "Cluster: 19 (715 docs)\n",
      "social, political, archaeological, cultural, language, women, policy, organizational, decision, firms\n",
      "\n",
      "CPU times: user 10min 38s, sys: 3min 23s, total: 14min 2s\n",
      "Wall time: 9min 3s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# So for this, I'll take out the top and bottom words\n",
    "# Essentially stop words and\n",
    "# words that are way too niche to be classified as descriptive\n",
    "# By enforcing max_features=10000\n",
    "# min_df is there to make sure of those 10000, there's no leftover\n",
    "# Also looser clusters because too many things got put together\n",
    "picky_setup = vectorize_cluster(\n",
    "    documents,\n",
    "    sample_size=None,\n",
    "    ngram_range=(1, 1),\n",
    "    analyzer='word',\n",
    "    max_df=0.95,\n",
    "    min_df=0.001,\n",
    "    max_features=10000,\n",
    "    use_idf=True,\n",
    "    sublinear_tf=True,\n",
    "    n_clusters=20,\n",
    "    tol=0.0001\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of the token matrix is: (9923, 2378)\n",
      "\n",
      "The top 20 tokens are:\n",
      "1.13\tthis\n",
      "1.14\tfor\n",
      "1.18\tis\n",
      "1.19\twill\n",
      "1.26\tbe\n",
      "1.29\ton\n",
      "1.31\twith\n",
      "1.33\tthat\n",
      "1.40\tare\n",
      "1.40\tresearch\n",
      "1.41\tby\n",
      "1.43\tas\n",
      "1.51\tfrom\n",
      "1.55\tan\n",
      "1.61\tthese\n",
      "1.61\tproject\n",
      "1.61\tat\n",
      "1.84\thave\n",
      "1.88\twhich\n",
      "1.92\tnew\n",
      "\n",
      "For the first 5 docs, the top tokens are:\n",
      "\n",
      "Document 0, top terms by TF-IDF\n",
      "0.28\tdrug\n",
      "0.26\tdatabase\n",
      "0.24\tdiscovery\n",
      "0.23\tdiseases\n",
      "0.21\tprotein\n",
      "\n",
      "Document 1, top terms by TF-IDF\n",
      "0.15\tchemistry\n",
      "0.15\tnanostructures\n",
      "0.15\tassociates\n",
      "0.15\tdetected\n",
      "0.15\tacademia\n",
      "\n",
      "Document 2, top terms by TF-IDF\n",
      "0.23\tmedical\n",
      "0.18\ttesting\n",
      "0.17\tworkers\n",
      "0.17\tcould\n",
      "0.17\tpersonnel\n",
      "\n",
      "Document 3, top terms by TF-IDF\n",
      "0.18\tobservations\n",
      "0.17\tcollection\n",
      "0.17\tvertical\n",
      "0.16\tweather\n",
      "0.16\tsummer\n",
      "\n",
      "Document 4, top terms by TF-IDF\n",
      "0.22\tquantum\n",
      "0.22\tpath\n",
      "0.21\tnanostructures\n",
      "0.18\tsemiconductor\n",
      "0.17\tintegral\n",
      "\n",
      "Cluster: 0 (398 docs)\n",
      "chemistry, molecules, reactions, organic, metal, complexes, professor, compounds, reaction, chemical\n",
      "\n",
      "Cluster: 1 (631 docs)\n",
      "solar, stars, wind, magnetic, formation, waves, observations, plasma, radar, flow\n",
      "\n",
      "Cluster: 2 (470 docs)\n",
      "protein, proteins, genes, gene, cell, expression, plant, cells, plants, dna\n",
      "\n",
      "Cluster: 3 (426 docs)\n",
      "learning, engineering, curriculum, courses, course, faculty, assessment, education, modules, design\n",
      "\n",
      "Cluster: 4 (640 docs)\n",
      "software, wireless, network, distributed, power, computing, performance, networks, sensor, grid\n",
      "\n",
      "Cluster: 5 (428 docs)\n",
      "phase, commercial, ii, cost, optical, market, devices, fuel, sensor, power\n",
      "\n",
      "Cluster: 6 (620 docs)\n",
      "species, evolutionary, populations, genetic, plant, plants, diversity, population, tree, ecological\n",
      "\n",
      "Cluster: 7 (485 docs)\n",
      "theory, equations, geometry, manifolds, spaces, algebraic, geometric, topology, differential, dimensional\n",
      "\n",
      "Cluster: 8 (335 docs)\n",
      "mantle, seismic, fault, deformation, crust, rocks, tectonic, plate, continental, zone\n",
      "\n",
      "Cluster: 9 (263 docs)\n",
      "college, stem, scholarship, mathematics, faculty, academic, programs, engineering, students, minority\n",
      "\n",
      "Cluster: 10 (627 docs)\n",
      "climate, ice, ocean, carbon, arctic, sea, water, circulation, atmospheric, soil\n",
      "\n",
      "Cluster: 11 (780 docs)\n",
      "materials, magnetic, quantum, nanoscale, polymer, spin, films, nano, optical, electron\n",
      "\n",
      "Cluster: 12 (225 docs)\n",
      "available, not, zones, zone, young, york, yield, yet, years, year\n",
      "\n",
      "Cluster: 13 (545 docs)\n",
      "workshop, conference, meeting, symposium, 2002, participants, held, speakers, 2003, travel\n",
      "\n",
      "Cluster: 14 (788 docs)\n",
      "center, equipment, dr, us, facility, digital, marine, industry, university, international\n",
      "\n",
      "Cluster: 15 (1007 docs)\n",
      "algorithms, models, optimization, problems, estimation, computational, control, we, visual, statistical\n",
      "\n",
      "Cluster: 16 (84 docs)\n",
      "fellowship, mathematical, sciences, postdoctoral, training, microbial, biology, biological, minority, further\n",
      "\n",
      "Cluster: 17 (751 docs)\n",
      "social, political, policy, archaeological, decision, cultural, economic, market, labor, children\n",
      "\n",
      "Cluster: 18 (267 docs)\n",
      "teachers, school, mathematics, teacher, fellows, schools, middle, districts, 12, inquiry\n",
      "\n",
      "Cluster: 19 (153 docs)\n",
      "reu, site, summer, students, experience, recruited, week, projects, chemistry, undergraduates\n",
      "\n",
      "CPU times: user 8min 2s, sys: 3min 14s, total: 11min 16s\n",
      "Wall time: 6min 33s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# So for this, I'll take out the top and bottom words\n",
    "# Essentially stop words and\n",
    "# words that are way too niche to be classified as descriptive\n",
    "# By enforcing max_features=10000\n",
    "# min_df is there to make sure of those 10000, there's no leftover\n",
    "# Also looser clusters because too many things got put together\n",
    "pickier_setup = vectorize_cluster(\n",
    "    documents,\n",
    "    sample_size=None,\n",
    "    ngram_range=(1, 1),\n",
    "    analyzer='word',\n",
    "    max_df=0.95,\n",
    "    min_df=0.01,\n",
    "    max_features=10000,\n",
    "    use_idf=True,\n",
    "    sublinear_tf=True,\n",
    "    n_clusters=20,\n",
    "    tol=0.0001\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I tried with even stricter setup and it's a bit better, not too clear but I can identify topic much quicker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of the token matrix is: (9923, 10000)\n",
      "\n",
      "The top 20 tokens are:\n",
      "1.13\tthis\n",
      "1.14\tfor\n",
      "1.18\tis\n",
      "1.19\twill\n",
      "1.22\tof the\n",
      "1.26\tbe\n",
      "1.29\ton\n",
      "1.31\twith\n",
      "1.33\tthat\n",
      "1.40\tare\n",
      "1.40\tresearch\n",
      "1.40\tin the\n",
      "1.41\tby\n",
      "1.43\tas\n",
      "1.51\tfrom\n",
      "1.55\tan\n",
      "1.57\twill be\n",
      "1.61\tthese\n",
      "1.61\tproject\n",
      "1.61\tat\n",
      "\n",
      "For the first 5 docs, the top tokens are:\n",
      "\n",
      "Document 0, top terms by TF-IDF\n",
      "0.19\tdrug\n",
      "0.18\tthe database\n",
      "0.17\tdatabase\n",
      "0.16\tdiscovery\n",
      "0.15\tdiseases\n",
      "\n",
      "Document 1, top terms by TF-IDF\n",
      "0.20\tnmr\n",
      "0.13\tand undergraduate\n",
      "0.12\tgraduate and\n",
      "0.11\tgaas\n",
      "0.11\theterostructures\n",
      "\n",
      "Document 2, top terms by TF-IDF\n",
      "0.28\tfabric\n",
      "0.16\tprotect\n",
      "0.15\tmedical\n",
      "0.15\tmilitary\n",
      "0.13\tpolymerization\n",
      "\n",
      "Document 3, top terms by TF-IDF\n",
      "0.19\tlightning\n",
      "0.16\tsnow\n",
      "0.15\tforecasting\n",
      "0.12\twinter\n",
      "0.12\tcloud\n",
      "\n",
      "Document 4, top terms by TF-IDF\n",
      "0.17\tdots\n",
      "0.16\thelium\n",
      "0.14\tquantum\n",
      "0.14\tpath\n",
      "0.14\tstatistical mechanics\n",
      "\n",
      "Cluster: 0 (351 docs)\n",
      "mantle, seismic, fault, deformation, magma, subduction, lithosphere, crust, the mantle, rocks\n",
      "\n",
      "Cluster: 1 (774 docs)\n",
      "galaxies, stars, star, quantum, galaxy, stellar, star formation, laser, particles, dark\n",
      "\n",
      "Cluster: 2 (791 docs)\n",
      "social, political, archaeological, decision, firms, policy, cultural, children, language, organizational\n",
      "\n",
      "Cluster: 3 (876 docs)\n",
      "software, wireless, distributed, power, sensor, grid, computing, network, networks, mobile\n",
      "\n",
      "Cluster: 4 (568 docs)\n",
      "workshop, conference, the workshop, the conference, meeting, symposium, workshop will, conference will, 2002, this workshop\n",
      "\n",
      "Cluster: 5 (545 docs)\n",
      "species, evolutionary, genetic, populations, birds, plants, reproductive, plant, females, diversity\n",
      "\n",
      "Cluster: 6 (538 docs)\n",
      "genes, proteins, protein, gene, fellowship, cell, plant, signaling, cells, expression\n",
      "\n",
      "Cluster: 7 (667 docs)\n",
      "algorithms, estimation, optimization, nonlinear, stochastic, inference, regression, linear, image, equations\n",
      "\n",
      "Cluster: 8 (225 docs)\n",
      "not available, available, not, zooplankton, zones, zone, zero, zealand, yr, younger\n",
      "\n",
      "Cluster: 9 (295 docs)\n",
      "stem, scholarship, scholarships, csems, college, community college, igert, computer science, the program, the college\n",
      "\n",
      "Cluster: 10 (32 docs)\n",
      "fellowship, mathematical sciences, mathematical, sciences, postdoctoral, fellowships, zooplankton, zones, zone, zero\n",
      "\n",
      "Cluster: 11 (439 docs)\n",
      "phase ii, phase, cost, optical, sensor, fuel, ii project, commercial, market, ii\n",
      "\n",
      "Cluster: 12 (560 docs)\n",
      "spin, magnetic, nanoscale, nano, films, polymer, nanotubes, thin, optical, quantum\n",
      "\n",
      "Cluster: 13 (434 docs)\n",
      "manifolds, equations, spaces, algebraic, geometry, theory, geometric, the investigator, algebras, hyperbolic\n",
      "\n",
      "Cluster: 14 (279 docs)\n",
      "chemistry, molecules, reactions, complexes, metal, organic, nmr, catalysts, compounds, professor\n",
      "\n",
      "Cluster: 15 (152 docs)\n",
      "reu, reu site, students will, research experience, the program, summer, the reu, chemistry, 2003, the chemistry\n",
      "\n",
      "Cluster: 16 (493 docs)\n",
      "ice, climate, carbon, soil, forest, arctic, ecosystem, co2, ocean, lake\n",
      "\n",
      "Cluster: 17 (502 docs)\n",
      "teachers, learning, mathematics, curriculum, teacher, fellows, engineering, middle, school, districts\n",
      "\n",
      "Cluster: 18 (613 docs)\n",
      "solar, ocean, wind, ice, this is, ionosphere, waves, solar wind, radar, variability\n",
      "\n",
      "Cluster: 19 (789 docs)\n",
      "equipment, french, center, the center, cnrs, dr, the us, us, france, facility\n",
      "\n",
      "CPU times: user 12min 20s, sys: 2min 46s, total: 15min 7s\n",
      "Wall time: 11min 3s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# So for this, it will be the same as above\n",
    "# but with 2 words as well\n",
    "two_words_setup = vectorize_cluster(\n",
    "    documents,\n",
    "    sample_size=None,\n",
    "    ngram_range=(1, 2),\n",
    "    analyzer='word',\n",
    "    max_df=0.95,\n",
    "    min_df=0.001,\n",
    "    max_features=10000,\n",
    "    use_idf=True,\n",
    "    sublinear_tf=True,\n",
    "    n_clusters=20,\n",
    "    tol=0.0001\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This one is good too but at some point it's quite messed up so I'll go with the `pickier_setup`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0\n",
      "0.0058\treef\n",
      "0.0044\tcoral\n",
      "0.0044\tcorals\n",
      "0.0036\trig\n",
      "0.0035\tpreK\n",
      "0.0030\tWGBH\n",
      "0.0028\tPM\n",
      "0.0025\treefs\n",
      "0.0020\tjob\n",
      "0.0012\tjudicial\n",
      "\n",
      "Topic 1\n",
      "0.0374\tAvailable\n",
      "0.0111\trouting\n",
      "0.0052\tprotocol\n",
      "0.0050\tquery\n",
      "0.0028\tterminals\n",
      "0.0025\tadaptivity\n",
      "0.0023\tbox\n",
      "0.0022\tRPI\n",
      "0.0020\tvolatility\n",
      "0.0019\tInternet\n",
      "\n",
      "Topic 2\n",
      "0.0088\tresearch\n",
      "0.0035\tproject\n",
      "0.0034\ttheir\n",
      "0.0027\tworkshop\n",
      "0.0025\twhich\n",
      "0.0024\tstudy\n",
      "0.0022\tinformation\n",
      "0.0022\tdevelopment\n",
      "0.0021\thow\n",
      "0.0021\tdata\n",
      "\n",
      "Topic 3\n",
      "0.0132\tgirls\n",
      "0.0093\tchildren\n",
      "0.0083\tscience\n",
      "0.0036\ttheir\n",
      "0.0034\tproject\n",
      "0.0033\tresearch\n",
      "0.0033\tstudents\n",
      "0.0023\tgeoscience\n",
      "0.0020\tabout\n",
      "0.0019\tfMRI\n",
      "\n",
      "Topic 4\n",
      "0.0090\tdata\n",
      "0.0042\tproject\n",
      "0.0028\tsuch\n",
      "0.0028\tmodel\n",
      "0.0026\twhich\n",
      "0.0026\tbetween\n",
      "0.0026\tresearch\n",
      "0.0025\tmodels\n",
      "0.0025\thow\n",
      "0.0024\tinformation\n",
      "\n",
      "Topic 5\n",
      "0.0088\ttheory\n",
      "0.0058\tproblems\n",
      "0.0053\tstudy\n",
      "0.0053\twhich\n",
      "0.0049\tsystems\n",
      "0.0048\tequations\n",
      "0.0043\tproject\n",
      "0.0040\tsuch\n",
      "0.0034\tgeometry\n",
      "0.0034\tmathematical\n",
      "\n",
      "Topic 6\n",
      "0.0060\tproject\n",
      "0.0041\tPhase\n",
      "0.0037\thigh\n",
      "0.0034\tDNA\n",
      "0.0033\tresearch\n",
      "0.0030\tdevelopment\n",
      "0.0028\tbased\n",
      "0.0027\tsystem\n",
      "0.0026\tgene\n",
      "0.0024\tdevelop\n",
      "\n",
      "Topic 7\n",
      "0.0036\tproject\n",
      "0.0030\twhich\n",
      "0.0029\tresearch\n",
      "0.0026\tcell\n",
      "0.0026\tgenes\n",
      "0.0025\tspecies\n",
      "0.0025\tunderstanding\n",
      "0.0023\tbetween\n",
      "0.0023\ttheir\n",
      "0.0022\tused\n",
      "\n",
      "Topic 8\n",
      "0.0083\tresearch\n",
      "0.0043\tproject\n",
      "0.0033\tsystems\n",
      "0.0028\tsystem\n",
      "0.0027\tbased\n",
      "0.0027\thigh\n",
      "0.0026\tsuch\n",
      "0.0026\tdesign\n",
      "0.0026\twhich\n",
      "0.0022\tuse\n",
      "\n",
      "Topic 9\n",
      "0.0104\tnoncommutative\n",
      "0.0042\tKaehler\n",
      "0.0042\tadic\n",
      "0.0027\tBaum\n",
      "0.0027\tmanifolds\n",
      "0.0021\tstring\n",
      "0.0020\tlattices\n",
      "0.0015\tDiophantine\n",
      "0.0013\tHopf\n",
      "0.0011\ttheory\n",
      "\n",
      "Topic 10\n",
      "0.0031\tpush\n",
      "0.0013\tdata\n",
      "0.0013\tmobile\n",
      "0.0012\tbroadcast\n",
      "0.0008\twireless\n",
      "0.0007\tscience\n",
      "0.0006\tstudents\n",
      "0.0005\twhich\n",
      "0.0004\tresearch\n",
      "0.0004\tusers\n",
      "\n",
      "Topic 11\n",
      "0.0048\tXML\n",
      "0.0047\tvesicles\n",
      "0.0044\tdata\n",
      "0.0039\tproject\n",
      "0.0033\tcompressed\n",
      "0.0032\tmanifold\n",
      "0.0026\tfusion\n",
      "0.0024\tresearch\n",
      "0.0023\ttext\n",
      "0.0022\twhich\n",
      "\n",
      "Topic 12\n",
      "0.0038\tretreat\n",
      "0.0034\tDFT\n",
      "0.0034\tresearch\n",
      "0.0034\tCLR\n",
      "0.0025\tproject\n",
      "0.0025\tproprietary\n",
      "0.0018\tsystem\n",
      "0.0018\tlegume\n",
      "0.0017\tsubducting\n",
      "0.0015\tsystems\n",
      "\n",
      "Topic 13\n",
      "0.0129\tUniversity\n",
      "0.0101\tresearch\n",
      "0.0098\tchemistry\n",
      "0.0083\tChemistry\n",
      "0.0067\torganic\n",
      "0.0064\treactions\n",
      "0.0059\taward\n",
      "0.0057\tstudents\n",
      "0.0051\tmolecules\n",
      "0.0048\tmaterials\n",
      "\n",
      "Topic 14\n",
      "0.0076\teruptions\n",
      "0.0066\tcrystallization\n",
      "0.0045\teruptive\n",
      "0.0045\tdegassing\n",
      "0.0043\tmagma\n",
      "0.0023\talkaline\n",
      "0.0018\teruption\n",
      "0.0017\tmagmas\n",
      "0.0016\tEAR\n",
      "0.0015\tOs\n",
      "\n",
      "Topic 15\n",
      "0.0192\tstudents\n",
      "0.0109\tresearch\n",
      "0.0098\tprogram\n",
      "0.0080\tscience\n",
      "0.0066\tUniversity\n",
      "0.0055\tfaculty\n",
      "0.0054\tproject\n",
      "0.0053\tengineering\n",
      "0.0051\ttheir\n",
      "0.0050\tprograms\n",
      "\n",
      "Topic 16\n",
      "0.0051\tmodels\n",
      "0.0043\tresearch\n",
      "0.0042\tproject\n",
      "0.0039\tflow\n",
      "0.0038\tdynamics\n",
      "0.0036\tmodeling\n",
      "0.0036\tfluid\n",
      "0.0031\tcomputational\n",
      "0.0029\tscale\n",
      "0.0028\twaves\n",
      "\n",
      "Topic 17\n",
      "0.0048\tresearch\n",
      "0.0048\tdata\n",
      "0.0037\tproject\n",
      "0.0036\tstudy\n",
      "0.0025\tbetween\n",
      "0.0025\twhich\n",
      "0.0023\tprovide\n",
      "0.0023\tclimate\n",
      "0.0022\tspecies\n",
      "0.0022\thow\n",
      "\n",
      "Topic 18\n",
      "0.0112\tmaterials\n",
      "0.0057\tresearch\n",
      "0.0053\tproperties\n",
      "0.0049\toptical\n",
      "0.0043\tproject\n",
      "0.0043\tdevices\n",
      "0.0041\tsurface\n",
      "0.0039\tmagnetic\n",
      "0.0037\thigh\n",
      "0.0034\tused\n",
      "\n",
      "Topic 19\n",
      "0.0035\tRAM\n",
      "0.0015\ttape\n",
      "0.0012\tPipeline\n",
      "0.0012\tbackup\n",
      "0.0011\treflectance\n",
      "0.0009\tdata\n",
      "0.0009\tproject\n",
      "0.0007\tresearch\n",
      "0.0007\tCPU\n",
      "0.0006\tscience\n",
      "\n",
      "CPU times: user 26.3 s, sys: 16.7 ms, total: 26.3 s\n",
      "Wall time: 26.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "new_vectorizer = TfidfVectorizer(\n",
    "    documents,\n",
    "    ngram_range=(1, 1),\n",
    "    analyzer='word',\n",
    "    max_df=0.95,\n",
    "    min_df=0.01,\n",
    "    max_features=10000,\n",
    "    use_idf=True,\n",
    "    sublinear_tf=True\n",
    ")\n",
    "# new_vectorizer = TfidfVectorizer()\n",
    "word_tokenizer = new_vectorizer.build_tokenizer()\n",
    "tokenized_text = [word_tokenizer(doc) for doc in documents]\n",
    "\n",
    "dictionary = corpora.Dictionary(tokenized_text)\n",
    "lda_corpus = [dictionary.doc2bow(text) for text in tokenized_text]\n",
    "lda_model = models.LdaModel(lda_corpus, id2word=dictionary, num_topics=20)\n",
    "\n",
    "for i, topic in lda_model.show_topics(num_topics=20, num_words=50, formatted=False):\n",
    "    print(\"Topic\", i)\n",
    "    printed_terms = 0\n",
    "    for term, score in topic:\n",
    "        if printed_terms >= 10:\n",
    "            break\n",
    "        elif term in \"this This that That these These have will Will\\\n",
    "        the of and to for in or The is be may an a with at are on by as from can \\\n",
    "        In it It has Has also not Not new\".split():\n",
    "            continue\n",
    "        printed_terms += 1\n",
    "        print(\"%.4f\\t%s\" % (score,term))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's rather quick and output the results quite nicely. But with a little help of preprocessing on the tokenizer to filter out the noises, it's more meaningful than on its own"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"word-vector\"></a>\n",
    "### Word vectors\n",
    "\n",
    "[top](#first-part)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_vectorizer = TfidfVectorizer()\n",
    "# new_vectorizer = TfidfVectorizer()\n",
    "word_tokenizer = new_vectorizer.build_tokenizer()\n",
    "tokenized_text = [word_tokenizer(doc) for doc in documents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-03-12 18:34:57,449 : INFO : collecting all words and their counts\n",
      "2020-03-12 18:34:57,450 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2020-03-12 18:34:57,855 : INFO : collected 63538 word types from a corpus of 2656274 raw words and 9923 sentences\n",
      "2020-03-12 18:34:57,856 : INFO : Loading a fresh vocabulary\n",
      "2020-03-12 18:34:57,908 : INFO : effective_min_count=5 retains 20752 unique words (32% of original 63538, drops 42786)\n",
      "2020-03-12 18:34:57,908 : INFO : effective_min_count=5 leaves 2585188 word corpus (97% of original 2656274, drops 71086)\n",
      "2020-03-12 18:34:57,966 : INFO : deleting the raw counts dictionary of 63538 items\n",
      "2020-03-12 18:34:57,968 : INFO : sample=0.001 downsamples 26 most-common words\n",
      "2020-03-12 18:34:57,969 : INFO : downsampling leaves estimated 2005620 word corpus (77.6% of prior 2585188)\n",
      "2020-03-12 18:34:58,017 : INFO : estimated required memory for 20752 words and 100 dimensions: 26977600 bytes\n",
      "2020-03-12 18:34:58,018 : INFO : resetting layer weights\n",
      "2020-03-12 18:35:01,199 : INFO : training model with 4 workers on 20752 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2020-03-12 18:35:02,207 : INFO : EPOCH 1 - PROGRESS: at 97.34% examples, 1948335 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-12 18:35:02,222 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-03-12 18:35:02,223 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-03-12 18:35:02,226 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-03-12 18:35:02,228 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-03-12 18:35:02,229 : INFO : EPOCH - 1 : training on 2656274 raw words (2005187 effective words) took 1.0s, 1957031 effective words/s\n",
      "2020-03-12 18:35:03,236 : INFO : EPOCH 2 - PROGRESS: at 98.53% examples, 1968354 words/s, in_qsize 5, out_qsize 0\n",
      "2020-03-12 18:35:03,241 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-03-12 18:35:03,242 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-03-12 18:35:03,244 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-03-12 18:35:03,247 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-03-12 18:35:03,248 : INFO : EPOCH - 2 : training on 2656274 raw words (2005271 effective words) took 1.0s, 1975306 effective words/s\n",
      "2020-03-12 18:35:04,225 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-03-12 18:35:04,226 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-03-12 18:35:04,228 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-03-12 18:35:04,230 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-03-12 18:35:04,231 : INFO : EPOCH - 3 : training on 2656274 raw words (2005467 effective words) took 1.0s, 2047978 effective words/s\n",
      "2020-03-12 18:35:05,210 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-03-12 18:35:05,212 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-03-12 18:35:05,214 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-03-12 18:35:05,217 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-03-12 18:35:05,217 : INFO : EPOCH - 4 : training on 2656274 raw words (2006166 effective words) took 1.0s, 2041543 effective words/s\n",
      "2020-03-12 18:35:06,224 : INFO : EPOCH 5 - PROGRESS: at 64.59% examples, 1282007 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-12 18:35:06,731 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-03-12 18:35:06,735 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-03-12 18:35:06,737 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-03-12 18:35:06,738 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-03-12 18:35:06,739 : INFO : EPOCH - 5 : training on 2656274 raw words (2006157 effective words) took 1.5s, 1322164 effective words/s\n",
      "2020-03-12 18:35:06,739 : INFO : training on a 13281370 raw words (10028248 effective words) took 5.5s, 1810297 effective words/s\n",
      "2020-03-12 18:35:06,740 : INFO : precomputing L2-norms of word weight vectors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most similar to: silicon\n",
      "[('nanoparticles', 0.9142018556594849), ('films', 0.9072830677032471), ('thin', 0.9039884805679321), ('fibers', 0.9034739136695862), ('composites', 0.8895953893661499), ('film', 0.8893566131591797), ('amorphous', 0.882348895072937), ('aluminum', 0.8821524381637573), ('doped', 0.8797479867935181), ('oxide', 0.8784518241882324)]\n",
      "\n",
      "Most similar to: flux\n",
      "[('fluxes', 0.9093732833862305), ('thickness', 0.8973891735076904), ('concentration', 0.8948220014572144), ('melting', 0.8926695585250854), ('melt', 0.8925642371177673), ('clouds', 0.8725210428237915), ('accumulation', 0.8716062307357788), ('momentum', 0.8660423755645752), ('precipitation', 0.8651330471038818), ('dust', 0.8647468090057373)]\n",
      "\n",
      "Most similar to: stratosphere\n",
      "[('troposphere', 0.9066334366798401), ('porewaters', 0.9060134291648865), ('thickening', 0.9047499299049377), ('basaltic', 0.8800020217895508), ('plume', 0.879138708114624), ('thermosphere', 0.8738328218460083), ('mesosphere', 0.8577731847763062), ('ionosphere', 0.8565584421157837), ('aerosols', 0.8561145663261414), ('Ba', 0.8551013469696045)]\n",
      "\n",
      "Most similar to: music\n",
      "[('lineups', 0.9048930406570435), ('ESC', 0.9003894329071045), ('checks', 0.8960487842559814), ('statically', 0.8923394680023193), ('Website', 0.889431357383728), ('sci', 0.885297417640686), ('Error', 0.8832237720489502), ('CdSe', 0.8826782703399658), ('hopping', 0.8823786973953247), ('neuro', 0.8812859058380127)]\n",
      "\n",
      "Most similar to: pitch\n",
      "[('multipole', 0.9094412922859192), ('steerable', 0.9017374515533447), ('Size', 0.9011666774749756), ('malignant', 0.8997153043746948), ('Squares', 0.8950706720352173), ('cylinder', 0.8888410329818726), ('Least', 0.8865343928337097), ('your', 0.8853967785835266), ('Helmholtz', 0.8838983774185181), ('LC', 0.8835553526878357)]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "vectors = gensim.models.Word2Vec(tokenized_text, size=100, window=5, min_count=5, sg=0, workers=4)\n",
    "\n",
    "print(\"Most similar to:\", 'silicon')\n",
    "print(vectors.wv.most_similar('silicon'))\n",
    "print()\n",
    "\n",
    "print(\"Most similar to:\", 'flux')\n",
    "print(vectors.wv.most_similar('flux'))\n",
    "print()\n",
    "\n",
    "print(\"Most similar to:\", 'stratosphere')\n",
    "print(vectors.wv.most_similar('stratosphere'))\n",
    "print()\n",
    "\n",
    "print(\"Most similar to:\", 'music')\n",
    "print(vectors.wv.most_similar('music'))\n",
    "print()\n",
    "\n",
    "print(\"Most similar to:\", 'pitch')\n",
    "print(vectors.wv.most_similar('pitch'))\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-03-12 18:36:14,779 : INFO : collecting all words and their counts\n",
      "2020-03-12 18:36:14,780 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2020-03-12 18:36:15,170 : INFO : collected 63538 word types from a corpus of 2656274 raw words and 9923 sentences\n",
      "2020-03-12 18:36:15,171 : INFO : Loading a fresh vocabulary\n",
      "2020-03-12 18:36:15,373 : INFO : effective_min_count=5 retains 20752 unique words (32% of original 63538, drops 42786)\n",
      "2020-03-12 18:36:15,374 : INFO : effective_min_count=5 leaves 2585188 word corpus (97% of original 2656274, drops 71086)\n",
      "2020-03-12 18:36:15,437 : INFO : deleting the raw counts dictionary of 63538 items\n",
      "2020-03-12 18:36:15,439 : INFO : sample=0.001 downsamples 26 most-common words\n",
      "2020-03-12 18:36:15,440 : INFO : downsampling leaves estimated 2005620 word corpus (77.6% of prior 2585188)\n",
      "2020-03-12 18:36:15,489 : INFO : estimated required memory for 20752 words and 100 dimensions: 26977600 bytes\n",
      "2020-03-12 18:36:15,490 : INFO : resetting layer weights\n",
      "2020-03-12 18:36:18,673 : INFO : training model with 4 workers on 20752 vocabulary and 100 features, using sg=1 hs=0 sample=0.001 negative=5 window=5\n",
      "2020-03-12 18:36:19,693 : INFO : EPOCH 1 - PROGRESS: at 19.00% examples, 366841 words/s, in_qsize 8, out_qsize 0\n",
      "2020-03-12 18:36:20,693 : INFO : EPOCH 1 - PROGRESS: at 37.34% examples, 368804 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-12 18:36:21,699 : INFO : EPOCH 1 - PROGRESS: at 58.02% examples, 383577 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-12 18:36:22,714 : INFO : EPOCH 1 - PROGRESS: at 84.23% examples, 415944 words/s, in_qsize 8, out_qsize 0\n",
      "2020-03-12 18:36:23,274 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-03-12 18:36:23,281 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-03-12 18:36:23,284 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-03-12 18:36:23,300 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-03-12 18:36:23,300 : INFO : EPOCH - 1 : training on 2656274 raw words (2005187 effective words) took 4.6s, 433900 effective words/s\n",
      "2020-03-12 18:36:24,315 : INFO : EPOCH 2 - PROGRESS: at 18.24% examples, 353670 words/s, in_qsize 8, out_qsize 0\n",
      "2020-03-12 18:36:25,333 : INFO : EPOCH 2 - PROGRESS: at 43.22% examples, 431886 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-12 18:36:26,342 : INFO : EPOCH 2 - PROGRESS: at 71.50% examples, 466972 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-12 18:36:27,346 : INFO : EPOCH 2 - PROGRESS: at 97.73% examples, 485023 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-12 18:36:27,397 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-03-12 18:36:27,399 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-03-12 18:36:27,403 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-03-12 18:36:27,410 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-03-12 18:36:27,411 : INFO : EPOCH - 2 : training on 2656274 raw words (2005219 effective words) took 4.1s, 488426 effective words/s\n",
      "2020-03-12 18:36:28,440 : INFO : EPOCH 3 - PROGRESS: at 23.39% examples, 450140 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-12 18:36:29,450 : INFO : EPOCH 3 - PROGRESS: at 43.22% examples, 430666 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-12 18:36:30,497 : INFO : EPOCH 3 - PROGRESS: at 62.71% examples, 404922 words/s, in_qsize 8, out_qsize 0\n",
      "2020-03-12 18:36:31,497 : INFO : EPOCH 3 - PROGRESS: at 82.34% examples, 402069 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-12 18:36:32,309 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-03-12 18:36:32,318 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-03-12 18:36:32,357 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-03-12 18:36:32,358 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-03-12 18:36:32,359 : INFO : EPOCH - 3 : training on 2656274 raw words (2005231 effective words) took 4.9s, 405639 effective words/s\n",
      "2020-03-12 18:36:33,367 : INFO : EPOCH 4 - PROGRESS: at 21.18% examples, 414300 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-12 18:36:34,381 : INFO : EPOCH 4 - PROGRESS: at 46.97% examples, 470816 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-12 18:36:35,407 : INFO : EPOCH 4 - PROGRESS: at 69.58% examples, 453776 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-12 18:36:36,422 : INFO : EPOCH 4 - PROGRESS: at 87.16% examples, 430084 words/s, in_qsize 8, out_qsize 0\n",
      "2020-03-12 18:36:37,028 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-03-12 18:36:37,033 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-03-12 18:36:37,038 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-03-12 18:36:37,058 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-03-12 18:36:37,059 : INFO : EPOCH - 4 : training on 2656274 raw words (2005703 effective words) took 4.7s, 427090 effective words/s\n",
      "2020-03-12 18:36:38,082 : INFO : EPOCH 5 - PROGRESS: at 19.38% examples, 372498 words/s, in_qsize 8, out_qsize 0\n",
      "2020-03-12 18:36:39,089 : INFO : EPOCH 5 - PROGRESS: at 42.16% examples, 421492 words/s, in_qsize 8, out_qsize 0\n",
      "2020-03-12 18:36:40,092 : INFO : EPOCH 5 - PROGRESS: at 62.71% examples, 412092 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-12 18:36:41,138 : INFO : EPOCH 5 - PROGRESS: at 80.97% examples, 397479 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-12 18:36:42,140 : INFO : EPOCH 5 - PROGRESS: at 98.53% examples, 389040 words/s, in_qsize 5, out_qsize 0\n",
      "2020-03-12 18:36:42,158 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-03-12 18:36:42,161 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-03-12 18:36:42,167 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-03-12 18:36:42,197 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-03-12 18:36:42,198 : INFO : EPOCH - 5 : training on 2656274 raw words (2005917 effective words) took 5.1s, 390679 effective words/s\n",
      "2020-03-12 18:36:42,198 : INFO : training on a 13281370 raw words (10027257 effective words) took 23.5s, 426255 effective words/s\n",
      "2020-03-12 18:36:42,205 : INFO : precomputing L2-norms of word weight vectors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most similar to: silicon\n",
      "[('carbide', 0.8787307739257812), ('nitride', 0.8575783967971802), ('wafers', 0.8548914194107056), ('SOI', 0.8428232669830322), ('SiC', 0.8398675322532654), ('LEDs', 0.8305456042289734), ('semiconducting', 0.8267442584037781), ('nanocomposite', 0.8259124159812927), ('germanium', 0.8222129344940186), ('wafer', 0.8169799447059631)]\n",
      "\n",
      "Most similar to: flux\n",
      "[('fluxes', 0.8159793615341187), ('heat', 0.786853551864624), ('latent', 0.7678372263908386), ('canopies', 0.7632498741149902), ('denitrification', 0.7597169280052185), ('firn', 0.7594362497329712), ('transports', 0.7594219446182251), ('momentum', 0.759409487247467), ('Hg', 0.7592573165893555), ('longwave', 0.7577768564224243)]\n",
      "\n",
      "Most similar to: stratosphere\n",
      "[('mesosphere', 0.8974559903144836), ('troposphere', 0.8770797252655029), ('tropopause', 0.8468611240386963), ('transports', 0.8443900942802429), ('thermosphere', 0.8377493619918823), ('midlatitude', 0.8325599431991577), ('remineralization', 0.8293002843856812), ('outflow', 0.8257754445075989), ('circulating', 0.824863076210022), ('neck', 0.8236010074615479)]\n",
      "\n",
      "Most similar to: music\n",
      "[('textual', 0.8909928202629089), ('applets', 0.8862156271934509), ('aids', 0.8861073851585388), ('graphic', 0.8853927254676819), ('lexical', 0.8718645572662354), ('musical', 0.8705242872238159), ('references', 0.8703022599220276), ('summarization', 0.8687652945518494), ('pointers', 0.8686345815658569), ('multimodal', 0.8677190542221069)]\n",
      "\n",
      "Most similar to: pitch\n",
      "[('differencing', 0.8925780057907104), ('gravitationally', 0.8914796113967896), ('MB', 0.8904963731765747), ('smoother', 0.8898231983184814), ('compress', 0.8894486427307129), ('multipole', 0.8873230218887329), ('bump', 0.8840577006340027), ('spurious', 0.8800601959228516), ('decoder', 0.8797399401664734), ('coherently', 0.8788646459579468)]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "vectors = gensim.models.Word2Vec(tokenized_text, size=100, window=5, min_count=5, sg=1, workers=4)\n",
    "\n",
    "print(\"Most similar to:\", 'silicon')\n",
    "print(vectors.wv.most_similar('silicon'))\n",
    "print()\n",
    "\n",
    "print(\"Most similar to:\", 'flux')\n",
    "print(vectors.wv.most_similar('flux'))\n",
    "print()\n",
    "\n",
    "print(\"Most similar to:\", 'stratosphere')\n",
    "print(vectors.wv.most_similar('stratosphere'))\n",
    "print()\n",
    "\n",
    "print(\"Most similar to:\", 'music')\n",
    "print(vectors.wv.most_similar('music'))\n",
    "print()\n",
    "\n",
    "print(\"Most similar to:\", 'pitch')\n",
    "print(vectors.wv.most_similar('pitch'))\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This makes more sense to me, though the music is still a bump since there's nothing about classical music out of all these abstract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-03-12 18:38:59,749 : INFO : collecting all words and their counts\n",
      "2020-03-12 18:38:59,752 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2020-03-12 18:39:00,141 : INFO : collected 63538 word types from a corpus of 2656274 raw words and 9923 sentences\n",
      "2020-03-12 18:39:00,142 : INFO : Loading a fresh vocabulary\n",
      "2020-03-12 18:39:00,188 : INFO : effective_min_count=10 retains 13531 unique words (21% of original 63538, drops 50007)\n",
      "2020-03-12 18:39:00,189 : INFO : effective_min_count=10 leaves 2538020 word corpus (95% of original 2656274, drops 118254)\n",
      "2020-03-12 18:39:00,222 : INFO : deleting the raw counts dictionary of 63538 items\n",
      "2020-03-12 18:39:00,224 : INFO : sample=0.001 downsamples 26 most-common words\n",
      "2020-03-12 18:39:00,224 : INFO : downsampling leaves estimated 1955256 word corpus (77.0% of prior 2538020)\n",
      "2020-03-12 18:39:00,250 : INFO : estimated required memory for 13531 words and 100 dimensions: 17590300 bytes\n",
      "2020-03-12 18:39:00,250 : INFO : resetting layer weights\n",
      "2020-03-12 18:39:02,380 : INFO : training model with 4 workers on 13531 vocabulary and 100 features, using sg=1 hs=0 sample=0.001 negative=5 window=3\n",
      "2020-03-12 18:39:03,401 : INFO : EPOCH 1 - PROGRESS: at 28.41% examples, 543705 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-12 18:39:04,418 : INFO : EPOCH 1 - PROGRESS: at 67.00% examples, 638471 words/s, in_qsize 8, out_qsize 1\n",
      "2020-03-12 18:39:05,261 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-03-12 18:39:05,275 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-03-12 18:39:05,276 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-03-12 18:39:05,279 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-03-12 18:39:05,280 : INFO : EPOCH - 1 : training on 2656274 raw words (1955786 effective words) took 2.9s, 675919 effective words/s\n",
      "2020-03-12 18:39:06,287 : INFO : EPOCH 2 - PROGRESS: at 27.33% examples, 528535 words/s, in_qsize 8, out_qsize 0\n",
      "2020-03-12 18:39:07,291 : INFO : EPOCH 2 - PROGRESS: at 64.97% examples, 628272 words/s, in_qsize 8, out_qsize 0\n",
      "2020-03-12 18:39:08,300 : INFO : EPOCH 2 - PROGRESS: at 96.98% examples, 629084 words/s, in_qsize 8, out_qsize 0\n",
      "2020-03-12 18:39:08,358 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-03-12 18:39:08,363 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-03-12 18:39:08,368 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-03-12 18:39:08,370 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-03-12 18:39:08,370 : INFO : EPOCH - 2 : training on 2656274 raw words (1955449 effective words) took 3.1s, 633714 effective words/s\n",
      "2020-03-12 18:39:09,379 : INFO : EPOCH 3 - PROGRESS: at 28.41% examples, 548990 words/s, in_qsize 8, out_qsize 0\n",
      "2020-03-12 18:39:10,399 : INFO : EPOCH 3 - PROGRESS: at 61.58% examples, 590091 words/s, in_qsize 8, out_qsize 0\n",
      "2020-03-12 18:39:11,400 : INFO : EPOCH 3 - PROGRESS: at 93.91% examples, 607440 words/s, in_qsize 6, out_qsize 0\n",
      "2020-03-12 18:39:11,543 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-03-12 18:39:11,547 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-03-12 18:39:11,548 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-03-12 18:39:11,549 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-03-12 18:39:11,550 : INFO : EPOCH - 3 : training on 2656274 raw words (1954657 effective words) took 3.2s, 615638 effective words/s\n",
      "2020-03-12 18:39:12,562 : INFO : EPOCH 4 - PROGRESS: at 39.12% examples, 758292 words/s, in_qsize 8, out_qsize 0\n",
      "2020-03-12 18:39:13,566 : INFO : EPOCH 4 - PROGRESS: at 77.35% examples, 754055 words/s, in_qsize 8, out_qsize 0\n",
      "2020-03-12 18:39:14,127 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-03-12 18:39:14,131 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-03-12 18:39:14,148 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-03-12 18:39:14,152 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-03-12 18:39:14,153 : INFO : EPOCH - 4 : training on 2656274 raw words (1955689 effective words) took 2.6s, 753930 effective words/s\n",
      "2020-03-12 18:39:15,169 : INFO : EPOCH 5 - PROGRESS: at 30.44% examples, 588665 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-12 18:39:16,189 : INFO : EPOCH 5 - PROGRESS: at 62.71% examples, 599516 words/s, in_qsize 8, out_qsize 0\n",
      "2020-03-12 18:39:17,191 : INFO : EPOCH 5 - PROGRESS: at 95.02% examples, 613606 words/s, in_qsize 8, out_qsize 0\n",
      "2020-03-12 18:39:17,279 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-03-12 18:39:17,284 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-03-12 18:39:17,293 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-03-12 18:39:17,295 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-03-12 18:39:17,296 : INFO : EPOCH - 5 : training on 2656274 raw words (1955646 effective words) took 3.1s, 623546 effective words/s\n",
      "2020-03-12 18:39:17,296 : INFO : training on a 13281370 raw words (9777227 effective words) took 14.9s, 655505 effective words/s\n",
      "2020-03-12 18:39:17,302 : INFO : precomputing L2-norms of word weight vectors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most similar to: silicon\n",
      "[('carbide', 0.8942508697509766), ('SiC', 0.8837494850158691), ('LEDs', 0.8702294826507568), ('nitride', 0.8609273433685303), ('SOI', 0.8567335605621338), ('wafers', 0.8488566875457764), ('GaN', 0.8457410931587219), ('semiconducting', 0.8403136730194092), ('doped', 0.8397879600524902), ('nanotubes', 0.8320607542991638)]\n",
      "\n",
      "Most similar to: flux\n",
      "[('fluxes', 0.8522653579711914), ('momentum', 0.8182167410850525), ('longwave', 0.8127259016036987), ('transports', 0.7976531982421875), ('salinity', 0.7946630716323853), ('O2', 0.7905195951461792), ('moisture', 0.7879533767700195), ('steep', 0.7848137617111206), ('outflow', 0.7828040719032288), ('thermosphere', 0.7814702987670898)]\n",
      "\n",
      "Most similar to: stratosphere\n",
      "[('mesosphere', 0.8834695816040039), ('troposphere', 0.8829984664916992), ('thermosphere', 0.875001072883606), ('EPS', 0.8548007011413574), ('Archean', 0.8498013019561768), ('overlying', 0.8486577868461609), ('carbonaceous', 0.848038375377655), ('transports', 0.8448255062103271), ('asthenosphere', 0.840819776058197), ('diapycnal', 0.8390008807182312)]\n",
      "\n",
      "Most similar to: music\n",
      "[('vocabulary', 0.870766282081604), ('graphic', 0.8691564798355103), ('aids', 0.8571217060089111), ('textual', 0.8543130159378052), ('relational', 0.8533315658569336), ('references', 0.8496170043945312), ('expressive', 0.8489760160446167), ('multimodal', 0.848456859588623), ('newer', 0.8477634191513062), ('standardization', 0.8475023508071899)]\n",
      "\n",
      "Most similar to: pitch\n",
      "[('pipe', 0.8961614370346069), ('towers', 0.8923881649971008), ('mask', 0.8918627500534058), ('pixels', 0.88997483253479), ('modality', 0.886391282081604), ('tunability', 0.8850734233856201), ('fetch', 0.8850722312927246), ('widths', 0.8843844532966614), ('symbol', 0.8835574984550476), ('diffusing', 0.8830643892288208)]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "vectors = gensim.models.Word2Vec(tokenized_text, size=100, window=3, min_count=10, sg=1, workers=4)\n",
    "\n",
    "print(\"Most similar to:\", 'silicon')\n",
    "print(vectors.wv.most_similar('silicon'))\n",
    "print()\n",
    "\n",
    "print(\"Most similar to:\", 'flux')\n",
    "print(vectors.wv.most_similar('flux'))\n",
    "print()\n",
    "\n",
    "print(\"Most similar to:\", 'stratosphere')\n",
    "print(vectors.wv.most_similar('stratosphere'))\n",
    "print()\n",
    "\n",
    "print(\"Most similar to:\", 'music')\n",
    "print(vectors.wv.most_similar('music'))\n",
    "print()\n",
    "\n",
    "print(\"Most similar to:\", 'pitch')\n",
    "print(vectors.wv.most_similar('pitch'))\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not much different but then it also looks pretty great. Especially in pitch, there's `tunability` now, that's what I was looking for actually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-03-12 18:41:55,212 : INFO : collecting all words and their counts\n",
      "2020-03-12 18:41:55,213 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2020-03-12 18:41:55,600 : INFO : collected 63538 word types from a corpus of 2656274 raw words and 9923 sentences\n",
      "2020-03-12 18:41:55,601 : INFO : Loading a fresh vocabulary\n",
      "2020-03-12 18:41:55,644 : INFO : effective_min_count=10 retains 13531 unique words (21% of original 63538, drops 50007)\n",
      "2020-03-12 18:41:55,645 : INFO : effective_min_count=10 leaves 2538020 word corpus (95% of original 2656274, drops 118254)\n",
      "2020-03-12 18:41:55,680 : INFO : deleting the raw counts dictionary of 63538 items\n",
      "2020-03-12 18:41:55,681 : INFO : sample=0.001 downsamples 26 most-common words\n",
      "2020-03-12 18:41:55,682 : INFO : downsampling leaves estimated 1955256 word corpus (77.0% of prior 2538020)\n",
      "2020-03-12 18:41:55,707 : INFO : estimated required memory for 13531 words and 1000 dimensions: 115013500 bytes\n",
      "2020-03-12 18:41:55,708 : INFO : resetting layer weights\n",
      "2020-03-12 18:41:57,898 : INFO : training model with 4 workers on 13531 vocabulary and 1000 features, using sg=1 hs=0 sample=0.001 negative=5 window=3\n",
      "2020-03-12 18:41:58,929 : INFO : EPOCH 1 - PROGRESS: at 8.69% examples, 162237 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-12 18:41:59,929 : INFO : EPOCH 1 - PROGRESS: at 18.65% examples, 175802 words/s, in_qsize 8, out_qsize 0\n",
      "2020-03-12 18:42:00,933 : INFO : EPOCH 1 - PROGRESS: at 30.43% examples, 196061 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-12 18:42:01,958 : INFO : EPOCH 1 - PROGRESS: at 41.59% examples, 201829 words/s, in_qsize 8, out_qsize 0\n",
      "2020-03-12 18:42:02,971 : INFO : EPOCH 1 - PROGRESS: at 51.03% examples, 197171 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-12 18:42:03,975 : INFO : EPOCH 1 - PROGRESS: at 59.76% examples, 192019 words/s, in_qsize 8, out_qsize 0\n",
      "2020-03-12 18:42:05,042 : INFO : EPOCH 1 - PROGRESS: at 69.13% examples, 187733 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-12 18:42:06,050 : INFO : EPOCH 1 - PROGRESS: at 77.35% examples, 185840 words/s, in_qsize 8, out_qsize 0\n",
      "2020-03-12 18:42:07,110 : INFO : EPOCH 1 - PROGRESS: at 87.48% examples, 185773 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-12 18:42:08,032 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-03-12 18:42:08,072 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-03-12 18:42:08,074 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-03-12 18:42:08,076 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-03-12 18:42:08,077 : INFO : EPOCH - 1 : training on 2656274 raw words (1955244 effective words) took 10.2s, 192225 effective words/s\n",
      "2020-03-12 18:42:09,180 : INFO : EPOCH 2 - PROGRESS: at 10.65% examples, 184254 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-12 18:42:10,211 : INFO : EPOCH 2 - PROGRESS: at 20.11% examples, 180568 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-12 18:42:11,225 : INFO : EPOCH 2 - PROGRESS: at 29.48% examples, 182203 words/s, in_qsize 8, out_qsize 0\n",
      "2020-03-12 18:42:12,259 : INFO : EPOCH 2 - PROGRESS: at 39.13% examples, 182098 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-12 18:42:13,267 : INFO : EPOCH 2 - PROGRESS: at 48.08% examples, 182889 words/s, in_qsize 8, out_qsize 0\n",
      "2020-03-12 18:42:14,271 : INFO : EPOCH 2 - PROGRESS: at 58.44% examples, 183617 words/s, in_qsize 8, out_qsize 0\n",
      "2020-03-12 18:42:15,285 : INFO : EPOCH 2 - PROGRESS: at 68.06% examples, 183010 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-12 18:42:16,288 : INFO : EPOCH 2 - PROGRESS: at 76.19% examples, 180877 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-12 18:42:17,318 : INFO : EPOCH 2 - PROGRESS: at 85.80% examples, 181212 words/s, in_qsize 8, out_qsize 0\n",
      "2020-03-12 18:42:18,323 : INFO : EPOCH 2 - PROGRESS: at 95.38% examples, 182332 words/s, in_qsize 8, out_qsize 0\n",
      "2020-03-12 18:42:18,710 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-03-12 18:42:18,729 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-03-12 18:42:18,741 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-03-12 18:42:18,788 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-03-12 18:42:18,789 : INFO : EPOCH - 2 : training on 2656274 raw words (1955236 effective words) took 10.7s, 182597 effective words/s\n",
      "2020-03-12 18:42:19,809 : INFO : EPOCH 3 - PROGRESS: at 8.69% examples, 163707 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-12 18:42:20,833 : INFO : EPOCH 3 - PROGRESS: at 18.63% examples, 174528 words/s, in_qsize 8, out_qsize 0\n",
      "2020-03-12 18:42:21,841 : INFO : EPOCH 3 - PROGRESS: at 30.14% examples, 192650 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-12 18:42:22,841 : INFO : EPOCH 3 - PROGRESS: at 42.54% examples, 207477 words/s, in_qsize 8, out_qsize 0\n",
      "2020-03-12 18:42:23,867 : INFO : EPOCH 3 - PROGRESS: at 55.15% examples, 212750 words/s, in_qsize 8, out_qsize 0\n",
      "2020-03-12 18:42:24,885 : INFO : EPOCH 3 - PROGRESS: at 68.80% examples, 218753 words/s, in_qsize 8, out_qsize 0\n",
      "2020-03-12 18:42:25,902 : INFO : EPOCH 3 - PROGRESS: at 80.05% examples, 220050 words/s, in_qsize 8, out_qsize 0\n",
      "2020-03-12 18:42:26,903 : INFO : EPOCH 3 - PROGRESS: at 91.71% examples, 221413 words/s, in_qsize 8, out_qsize 0\n",
      "2020-03-12 18:42:27,669 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-03-12 18:42:27,677 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-03-12 18:42:27,684 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-03-12 18:42:27,746 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-03-12 18:42:27,746 : INFO : EPOCH - 3 : training on 2656274 raw words (1954830 effective words) took 9.0s, 218411 effective words/s\n",
      "2020-03-12 18:42:28,759 : INFO : EPOCH 4 - PROGRESS: at 8.69% examples, 164753 words/s, in_qsize 8, out_qsize 1\n",
      "2020-03-12 18:42:29,786 : INFO : EPOCH 4 - PROGRESS: at 18.65% examples, 174805 words/s, in_qsize 8, out_qsize 2\n",
      "2020-03-12 18:42:30,787 : INFO : EPOCH 4 - PROGRESS: at 28.06% examples, 179276 words/s, in_qsize 8, out_qsize 0\n",
      "2020-03-12 18:42:31,795 : INFO : EPOCH 4 - PROGRESS: at 37.74% examples, 180820 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-12 18:42:32,810 : INFO : EPOCH 4 - PROGRESS: at 46.62% examples, 181736 words/s, in_qsize 8, out_qsize 0\n",
      "2020-03-12 18:42:33,830 : INFO : EPOCH 4 - PROGRESS: at 56.91% examples, 182216 words/s, in_qsize 8, out_qsize 1\n",
      "2020-03-12 18:42:34,870 : INFO : EPOCH 4 - PROGRESS: at 66.57% examples, 181108 words/s, in_qsize 8, out_qsize 0\n",
      "2020-03-12 18:42:35,889 : INFO : EPOCH 4 - PROGRESS: at 75.58% examples, 180622 words/s, in_qsize 8, out_qsize 0\n",
      "2020-03-12 18:42:36,914 : INFO : EPOCH 4 - PROGRESS: at 84.56% examples, 179462 words/s, in_qsize 8, out_qsize 0\n",
      "2020-03-12 18:42:37,958 : INFO : EPOCH 4 - PROGRESS: at 92.91% examples, 178039 words/s, in_qsize 8, out_qsize 0\n",
      "2020-03-12 18:42:38,659 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-03-12 18:42:38,682 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-03-12 18:42:38,702 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-03-12 18:42:38,722 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-03-12 18:42:38,723 : INFO : EPOCH - 4 : training on 2656274 raw words (1955503 effective words) took 11.0s, 178245 effective words/s\n",
      "2020-03-12 18:42:39,792 : INFO : EPOCH 5 - PROGRESS: at 12.37% examples, 223948 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-12 18:42:40,860 : INFO : EPOCH 5 - PROGRESS: at 25.85% examples, 234431 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-12 18:42:41,882 : INFO : EPOCH 5 - PROGRESS: at 38.81% examples, 238572 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-12 18:42:42,903 : INFO : EPOCH 5 - PROGRESS: at 50.62% examples, 237280 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-12 18:42:43,926 : INFO : EPOCH 5 - PROGRESS: at 63.08% examples, 235275 words/s, in_qsize 8, out_qsize 0\n",
      "2020-03-12 18:42:44,953 : INFO : EPOCH 5 - PROGRESS: at 72.61% examples, 225569 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-12 18:42:45,984 : INFO : EPOCH 5 - PROGRESS: at 85.16% examples, 228544 words/s, in_qsize 8, out_qsize 0\n",
      "2020-03-12 18:42:47,033 : INFO : EPOCH 5 - PROGRESS: at 94.61% examples, 223005 words/s, in_qsize 8, out_qsize 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-03-12 18:42:47,472 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-03-12 18:42:47,501 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-03-12 18:42:47,504 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-03-12 18:42:47,542 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-03-12 18:42:47,543 : INFO : EPOCH - 5 : training on 2656274 raw words (1955442 effective words) took 8.8s, 221779 effective words/s\n",
      "2020-03-12 18:42:47,543 : INFO : training on a 13281370 raw words (9776255 effective words) took 49.6s, 196927 effective words/s\n",
      "2020-03-12 18:42:47,548 : INFO : precomputing L2-norms of word weight vectors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most similar to: silicon\n",
      "[('carbide', 0.8683127164840698), ('SiC', 0.8661246299743652), ('LEDs', 0.858808159828186), ('wafers', 0.8447800874710083), ('GaN', 0.8418112397193909), ('wafer', 0.826755702495575), ('nitride', 0.8264325857162476), ('conductive', 0.8259358406066895), ('ultrathin', 0.8248092532157898), ('nanocomposite', 0.8241363763809204)]\n",
      "\n",
      "Most similar to: flux\n",
      "[('fluxes', 0.8358415961265564), ('momentum', 0.8162760138511658), ('transports', 0.7871787548065186), ('O2', 0.7836939096450806), ('haze', 0.7819864749908447), ('vorticity', 0.7814186215400696), ('mesosphere', 0.7802945375442505), ('diapycnal', 0.7741315364837646), ('outflow', 0.7718337178230286), ('asthenosphere', 0.77115797996521)]\n",
      "\n",
      "Most similar to: stratosphere\n",
      "[('mesosphere', 0.9094099998474121), ('thermosphere', 0.8826446533203125), ('troposphere', 0.8724294900894165), ('H2O', 0.8540778160095215), ('circulating', 0.8538209199905396), ('diapycnal', 0.853712260723114), ('remineralization', 0.8531094789505005), ('asthenosphere', 0.8517158031463623), ('mineralogy', 0.8511455059051514), ('thermocline', 0.849853515625)]\n",
      "\n",
      "Most similar to: music\n",
      "[('vocabulary', 0.8750628232955933), ('accessing', 0.8746378421783447), ('videos', 0.8694682121276855), ('relational', 0.8658239245414734), ('lexical', 0.865169882774353), ('textual', 0.8605393767356873), ('hiding', 0.8579472303390503), ('categorization', 0.8523973822593689), ('aggregating', 0.8523606657981873), ('graphic', 0.8521576523780823)]\n",
      "\n",
      "Most similar to: pitch\n",
      "[('variant', 0.9017078876495361), ('tunability', 0.8993591666221619), ('thermodynamically', 0.8952896595001221), ('loose', 0.8936942219734192), ('faint', 0.893287181854248), ('supersonic', 0.8931238651275635), ('parity', 0.891822099685669), ('lock', 0.8908010721206665), ('intensities', 0.8906475305557251), ('coarser', 0.8900716304779053)]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "vectors = gensim.models.Word2Vec(tokenized_text, size=1000, window=3, min_count=10, sg=1, workers=4)\n",
    "\n",
    "print(\"Most similar to:\", 'silicon')\n",
    "print(vectors.wv.most_similar('silicon'))\n",
    "print()\n",
    "\n",
    "print(\"Most similar to:\", 'flux')\n",
    "print(vectors.wv.most_similar('flux'))\n",
    "print()\n",
    "\n",
    "print(\"Most similar to:\", 'stratosphere')\n",
    "print(vectors.wv.most_similar('stratosphere'))\n",
    "print()\n",
    "\n",
    "print(\"Most similar to:\", 'music')\n",
    "print(vectors.wv.most_similar('music'))\n",
    "print()\n",
    "\n",
    "print(\"Most similar to:\", 'pitch')\n",
    "print(vectors.wv.most_similar('pitch'))\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I like this setup mostly because of the stricter boundaries and increase dimensionality (somehow). But this setup makes the most sense when it comes to expectation. I would want to see these information appears together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "[top](#first-part)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
