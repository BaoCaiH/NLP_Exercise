{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"first-part\"></a>\n",
    "\n",
    "#!/usr/bin/env python<br># coding: utf-8\n",
    "\n",
    "Author: Bao Cai\n",
    "\n",
    "Course: Machine Learning for Descriptive Problems\n",
    "\n",
    "Topic: NLP-Unsupervised\n",
    "\n",
    "Start Date: 2020-03-11\n",
    "\n",
    "Last Save: 2020-03-12"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[1. Topic analysis](#topic-part)\n",
    "\n",
    "There are quantitative measures for evaluating various aspects of clustering results and topic models intrinsically, and they can also be evaluated extrinsically by how well the clusters/topics serve some supervised task as features. In this exervise, however, we well fovus on qualitative evaluation of the results in terms of their descriptiveness. As in the example code, you may limit yourself to the 1000 first documents of ther corpus when performing clustering, in order to simplify the task and speed up experimentation, but use the whole corpus to calculate tf-idf features.\n",
    "\n",
    "a. Experiment with different setups of the tf-idf feature extraction and clustering (k-means or hierarchical). In order to obtain meaningful results. When you arrive at a good configuration, describe it and motivate your chosen setup/parameters.\n",
    "\n",
    "b. Inspect the keywords of the clusters. List the 10 first clusters out of all (i.e. not cherry picked examples) and privde an as descriptive label as possible for each of them.\n",
    "\n",
    "c. Select one or two good clusters (that can be clearly interpreted) and one or two bad clusters (that might be difficult to interpret or distinguish). Motivate your choise (clusters may for instance, be overlapping, to broad/narrow or incoherent).\n",
    "\n",
    "d. Repeat the experiment in (a) with LDA topic modelling instead (on the whole corpus), and explain briefly how the results compare to your previously chosen clustering setup. A few concrete examples may be helpful. Do your best to make sure the list of topic keywords are informative through appropriate post-processing.\n",
    "\n",
    "[2. Word vectors](#word-vector)\n",
    "\n",
    "a. Choose about 5 words (arbitrary) to use as seed words in the following experiment. Train word2vec vectors on the corpus while trying out variations on the parameters. Evaluate the vector models by inspecting the most similar words for each of the seed words, and try to identify qualitative differences between different parameter choices. Which parameters seem to have the most interesting effect? At what values? Motivate. Finally, study the qualitative effect of increasing the training data, by similarly comparing vectors trained with the best setup on the texts from the awards_2020 directory against vectors trained on the whole set of abstracts (1990-2002).\n",
    "\n",
    "b. Repeat the experiment with ELMo from the lecture, with a different target word and diferent sentences. Choose a word that can have multiple senses, and construct 10 sentences that express 2-3 different senses of the word. Produce ELMo embeddings for the target word in each sentence and measure the similarity between the vectors. Evaluate in how many cases the measured similarities can be used to successfully distinguish between the different senses. Comment on the results, e.g. are you able to identify a particular way in which the model fails?\n",
    "\n",
    "[top](#first-part)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "import os\n",
    "import re\n",
    "import binascii\n",
    "import itertools\n",
    "import heapq\n",
    "import gensim\n",
    "import logging\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow as tf\n",
    "from time import time\n",
    "from collections import Counter\n",
    "from gensim import corpora, models\n",
    "from scipy.spatial.distance import cosine\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage, fcluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions\n",
    "def get_fnames(path='./Data'):\n",
    "    \"\"\"Read all text files in a folder.\n",
    "    \"\"\"\n",
    "    fnames = []\n",
    "    for root, _, files in os.walk(path):\n",
    "        for fname in files:\n",
    "            if fname[-4:] == '.txt':\n",
    "                fnames.append(os.path.join(root, fname))\n",
    "    return fnames\n",
    "\n",
    "\n",
    "def read_file(fname):\n",
    "    with open(fname, 'rt', encoding='latin-1') as f:\n",
    "        # skip all lines until abstract\n",
    "        for line in f:\n",
    "            if \"Abstract    :\" in line:\n",
    "                break\n",
    "\n",
    "        # get abstract as a single string\n",
    "        abstract = ' '.join([line[:-1].strip() for line in f])\n",
    "        abstract = re.sub(' +', ' ', abstract)\n",
    "        return abstract\n",
    "\n",
    "\n",
    "def print_clusters(matrix, clusters, features, n_keywords=10):\n",
    "    for cluster in range(min(clusters), max(clusters)+1):\n",
    "        cluster_docs = [i for i, c in enumerate(clusters) if c == cluster]\n",
    "        print(\"Cluster: %d (%d docs)\" % (cluster, len(cluster_docs)))\n",
    "        \n",
    "        # Keep scores for top n terms\n",
    "        new_matrix = np.zeros((len(cluster_docs), matrix.shape[1]))\n",
    "        for cluster_i, doc_vec in enumerate(matrix[cluster_docs].toarray()):\n",
    "            for idx, score in heapq.nlargest(n_keywords, enumerate(doc_vec), key=lambda x:x[1]):\n",
    "                new_matrix[cluster_i][idx] = score\n",
    "\n",
    "        # Aggregate scores for kept top terms\n",
    "        keywords = heapq.nlargest(n_keywords, zip(new_matrix.sum(axis=0), features))\n",
    "        print(', '.join([w for s,w in keywords]))\n",
    "        print()\n",
    "\n",
    "\n",
    "def vectorize_cluster(\n",
    "    documents,\n",
    "    sample_size=None,\n",
    "    ngram_range=(1, 1),\n",
    "    analyzer='word',\n",
    "    max_df=1.0,\n",
    "    min_df=0.0,\n",
    "    max_features=None,\n",
    "    use_idf=True,\n",
    "    sublinear_tf=True,\n",
    "    n_clusters=10,\n",
    "    tol=0.0001\n",
    "):\n",
    "    vectorizer = TfidfVectorizer(\n",
    "        ngram_range=ngram_range,\n",
    "        analyzer=analyzer,\n",
    "        max_df=max_df,\n",
    "        min_df=min_df,\n",
    "        max_features=max_features,\n",
    "        use_idf=use_idf,\n",
    "        sublinear_tf=sublinear_tf\n",
    "    )\n",
    "    token_matrix = vectorizer.fit_transform(documents)\n",
    "    print(\n",
    "        'The shape of the token matrix is:',\n",
    "        token_matrix.toarray().shape\n",
    "    )\n",
    "    print()\n",
    "    features = vectorizer.get_feature_names()\n",
    "    print('The top 20 tokens are:')\n",
    "    for feature, idf in sorted(\n",
    "        zip(features, vectorizer._tfidf.idf_),\n",
    "        key=lambda x:x[1]\n",
    "    )[:20]:\n",
    "        print('{:.2f}\\t{}'.format(idf, feature))\n",
    "    print()\n",
    "    print('For the first 5 docs, the top tokens are:')\n",
    "    for i in range(5):\n",
    "        print('\\nDocument {}, top terms by TF-IDF'.format(i))\n",
    "        for feature, score in sorted(\n",
    "            zip(features, token_matrix.toarray()[i]),\n",
    "            key=lambda x: -x[1]\n",
    "        )[:5]:\n",
    "            print('{:.2f}\\t{}'.format(score, feature))\n",
    "    print()\n",
    "    if not sample_size:\n",
    "        matrix_to_cluster = token_matrix\n",
    "    else:\n",
    "        matrix_to_cluster = token_matrix[:sample_size]\n",
    "    km = KMeans(\n",
    "        n_clusters=n_clusters,\n",
    "        tol=tol,\n",
    "        random_state=44,\n",
    "        verbose=False\n",
    "    )\n",
    "    km.fit(matrix_to_cluster)\n",
    "    print_clusters(matrix_to_cluster, km.labels_, features)\n",
    "    return (\n",
    "        vectorizer,\n",
    "        token_matrix,\n",
    "        features,\n",
    "        matrix_to_cluster,\n",
    "        km\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.23 s, sys: 254 ms, total: 1.48 s\n",
      "Wall time: 2.25 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "documents = [read_file(file) for file in get_fnames('./Data/Arcada_BigDataAnalytic/')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"topic-part\"></a>\n",
    "### Topic analysis\n",
    "\n",
    "[top](#first-part)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of the token matrix is: (9923, 53816)\n",
      "\n",
      "The top 20 tokens are:\n",
      "1.03\tthe\n",
      "1.03\tof\n",
      "1.03\tand\n",
      "1.04\tto\n",
      "1.05\tin\n",
      "1.13\tthis\n",
      "1.14\tfor\n",
      "1.18\tis\n",
      "1.19\twill\n",
      "1.26\tbe\n",
      "1.29\ton\n",
      "1.31\twith\n",
      "1.33\tthat\n",
      "1.40\tare\n",
      "1.40\tresearch\n",
      "1.41\tby\n",
      "1.43\tas\n",
      "1.51\tfrom\n",
      "1.55\tan\n",
      "1.61\tthese\n",
      "\n",
      "For the first 5 docs, the top tokens are:\n",
      "\n",
      "Document 0, top terms by TF-IDF\n",
      "0.34\ttrafficking\n",
      "0.22\tdrug\n",
      "0.20\tdatabase\n",
      "0.19\tdiscovery\n",
      "0.18\tmislocalization\n",
      "\n",
      "Document 1, top terms by TF-IDF\n",
      "0.25\tnmr\n",
      "0.23\toptically\n",
      "0.20\tingap\n",
      "0.18\thayes\n",
      "0.17\tinp\n",
      "\n",
      "Document 2, top terms by TF-IDF\n",
      "0.25\tfabric\n",
      "0.19\ttextiles\n",
      "0.18\tpesticides\n",
      "0.18\tweapons\n",
      "0.18\tdrapes\n",
      "\n",
      "Document 3, top terms by TF-IDF\n",
      "0.33\tthundersnow\n",
      "0.19\trawinsonde\n",
      "0.18\tlightning\n",
      "0.16\tsnow\n",
      "0.15\tforecasting\n",
      "\n",
      "Document 4, top terms by TF-IDF\n",
      "0.20\tdots\n",
      "0.19\ttutorials\n",
      "0.18\thelium\n",
      "0.17\tcondensates\n",
      "0.17\tquantum\n",
      "Cluster: 0 (1505 docs)\n",
      "polymer, magnetic, spin, nano, laser, films, quantum, optical, nanoscale, nanoparticles\n",
      "\n",
      "Cluster: 1 (1494 docs)\n",
      "species, birds, political, social, genetic, populations, evolutionary, language, decision, firms\n",
      "\n",
      "Cluster: 2 (834 docs)\n",
      "protein, proteins, genes, nmr, arabidopsis, gene, complexes, genome, dna, cell\n",
      "\n",
      "Cluster: 3 (1791 docs)\n",
      "ice, ocean, mantle, solar, climate, galaxies, arctic, seismic, stars, fault\n",
      "\n",
      "Cluster: 4 (682 docs)\n",
      "equations, manifolds, spaces, algebraic, quantum, algebras, hyperbolic, geometry, topology, random\n",
      "\n",
      "Cluster: 5 (1064 docs)\n",
      "teachers, reu, stem, engineering, mathematics, curriculum, learning, scholarship, college, fellows\n",
      "\n",
      "Cluster: 6 (62 docs)\n",
      "fellowship, mathematical, sciences, postdoctoral, informatics, training, fellowships, minority, biological, genetic\n",
      "\n",
      "Cluster: 7 (1608 docs)\n",
      "wireless, software, grid, sensor, mobile, networks, power, security, computing, visualization\n",
      "\n",
      "Cluster: 8 (225 docs)\n",
      "available, not, zyss, zylstra, zygotically, zygotic, zygote, zygomycota, zygomycetes, zygomorphy\n",
      "\n",
      "Cluster: 9 (658 docs)\n",
      "workshop, conference, symposium, meeting, gordon, 2002, sessions, 2003, session, eu\n",
      "\n",
      "CPU times: user 11min 27s, sys: 3min 11s, total: 14min 39s\n",
      "Wall time: 10min 37s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "default_setup = vectorize_cluster(\n",
    "    documents,\n",
    "    sample_size=None,\n",
    "    ngram_range=(1, 1),\n",
    "    analyzer='word',\n",
    "    max_df=1.0,\n",
    "    min_df=0.0,\n",
    "    max_features=None,\n",
    "    use_idf=True,\n",
    "    sublinear_tf=True,\n",
    "    n_clusters=10,\n",
    "    tol=0.0001\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of the token matrix is: (9923, 10000)\n",
      "\n",
      "The top 20 tokens are:\n",
      "1.13\tthis\n",
      "1.14\tfor\n",
      "1.18\tis\n",
      "1.19\twill\n",
      "1.26\tbe\n",
      "1.29\ton\n",
      "1.31\twith\n",
      "1.33\tthat\n",
      "1.40\tare\n",
      "1.40\tresearch\n",
      "1.41\tby\n",
      "1.43\tas\n",
      "1.51\tfrom\n",
      "1.55\tan\n",
      "1.61\tthese\n",
      "1.61\tproject\n",
      "1.61\tat\n",
      "1.84\thave\n",
      "1.88\twhich\n",
      "1.92\tnew\n",
      "\n",
      "For the first 5 docs, the top tokens are:\n",
      "\n",
      "Document 0, top terms by TF-IDF\n",
      "0.36\ttrafficking\n",
      "0.24\tdrug\n",
      "0.22\tdatabase\n",
      "0.20\tdiscovery\n",
      "0.19\tdiseases\n",
      "\n",
      "Document 1, top terms by TF-IDF\n",
      "0.27\tnmr\n",
      "0.24\toptically\n",
      "0.15\twells\n",
      "0.15\tgaas\n",
      "0.15\theterostructures\n",
      "\n",
      "Document 2, top terms by TF-IDF\n",
      "0.29\tfabric\n",
      "0.21\ttextiles\n",
      "0.20\tpesticides\n",
      "0.20\tweapons\n",
      "0.16\tprotect\n",
      "\n",
      "Document 3, top terms by TF-IDF\n",
      "0.21\tlightning\n",
      "0.18\tsnow\n",
      "0.17\tforecasting\n",
      "0.17\tsynoptic\n",
      "0.17\tdangerous\n",
      "\n",
      "Document 4, top terms by TF-IDF\n",
      "0.21\tdots\n",
      "0.20\ttutorials\n",
      "0.19\thelium\n",
      "0.18\tquantum\n",
      "0.17\tpath\n",
      "\n",
      "Cluster: 0 (427 docs)\n",
      "phase, fuel, sensor, market, polymer, optical, cell, devices, manufacturing, drug\n",
      "\n",
      "Cluster: 1 (225 docs)\n",
      "available, not, zooplankton, zoology, zones, zone, zonal, zno, zn, zircon\n",
      "\n",
      "Cluster: 2 (378 docs)\n",
      "chemistry, reactions, molecules, nmr, metal, complexes, organic, reaction, professor, catalysts\n",
      "\n",
      "Cluster: 3 (768 docs)\n",
      "ice, ocean, climate, arctic, carbon, soil, sea, variability, forest, atmospheric\n",
      "\n",
      "Cluster: 4 (359 docs)\n",
      "solar, galaxies, stars, wind, galaxy, star, waves, magnetosphere, magnetic, ionosphere\n",
      "\n",
      "Cluster: 5 (84 docs)\n",
      "fellowship, mathematical, sciences, postdoctoral, informatics, training, microbial, biology, minority, fellowships\n",
      "\n",
      "Cluster: 6 (379 docs)\n",
      "mantle, seismic, fault, deformation, magma, crust, subduction, earthquake, arc, lithosphere\n",
      "\n",
      "Cluster: 7 (157 docs)\n",
      "teachers, school, fellows, mathematics, districts, middle, teacher, schools, district, inquiry\n",
      "\n",
      "Cluster: 8 (580 docs)\n",
      "workshop, conference, meeting, symposium, 2002, 2003, participants, gordon, held, speakers\n",
      "\n",
      "Cluster: 9 (1070 docs)\n",
      "algorithms, estimation, visual, brain, optimization, recognition, stochastic, models, 3d, inference\n",
      "\n",
      "Cluster: 10 (320 docs)\n",
      "stem, college, scholarship, scholarships, csems, igert, colleges, minority, csem, scholars\n",
      "\n",
      "Cluster: 11 (154 docs)\n",
      "reu, summer, site, experience, week, chemistry, ten, 2003, undergraduates, colleges\n",
      "\n",
      "Cluster: 12 (477 docs)\n",
      "learning, curriculum, engineering, nsdl, digital, library, teachers, assessment, mathematics, stem\n",
      "\n",
      "Cluster: 13 (765 docs)\n",
      "wireless, software, power, sensor, distributed, network, networks, grid, computing, mobile\n",
      "\n",
      "Cluster: 14 (638 docs)\n",
      "equipment, french, center, contract, cnrs, dr, france, facility, abroad, us\n",
      "\n",
      "Cluster: 15 (498 docs)\n",
      "proteins, protein, genes, gene, cell, arabidopsis, expression, cells, plant, signaling\n",
      "\n",
      "Cluster: 16 (597 docs)\n",
      "species, genetic, evolutionary, populations, birds, plant, plants, reproductive, diversity, females\n",
      "\n",
      "Cluster: 17 (507 docs)\n",
      "equations, manifolds, algebraic, spaces, theory, geometry, geometric, algebras, hyperbolic, topology\n",
      "\n",
      "Cluster: 18 (825 docs)\n",
      "magnetic, spin, nanoscale, polymer, quantum, films, nano, optical, polymers, laser\n",
      "\n",
      "Cluster: 19 (715 docs)\n",
      "social, political, archaeological, cultural, language, women, policy, organizational, decision, firms\n",
      "\n",
      "CPU times: user 10min 38s, sys: 3min 23s, total: 14min 2s\n",
      "Wall time: 9min 3s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# So for this, I'll take out the top and bottom words\n",
    "# Essentially stop words and\n",
    "# words that are way too niche to be classified as descriptive\n",
    "# By enforcing max_features=10000\n",
    "# min_df is there to make sure of those 10000, there's no leftover\n",
    "# Also looser clusters because too many things got put together\n",
    "picky_setup = vectorize_cluster(\n",
    "    documents,\n",
    "    sample_size=None,\n",
    "    ngram_range=(1, 1),\n",
    "    analyzer='word',\n",
    "    max_df=0.95,\n",
    "    min_df=0.001,\n",
    "    max_features=10000,\n",
    "    use_idf=True,\n",
    "    sublinear_tf=True,\n",
    "    n_clusters=20,\n",
    "    tol=0.0001\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of the token matrix is: (9923, 2378)\n",
      "\n",
      "The top 20 tokens are:\n",
      "1.13\tthis\n",
      "1.14\tfor\n",
      "1.18\tis\n",
      "1.19\twill\n",
      "1.26\tbe\n",
      "1.29\ton\n",
      "1.31\twith\n",
      "1.33\tthat\n",
      "1.40\tare\n",
      "1.40\tresearch\n",
      "1.41\tby\n",
      "1.43\tas\n",
      "1.51\tfrom\n",
      "1.55\tan\n",
      "1.61\tthese\n",
      "1.61\tproject\n",
      "1.61\tat\n",
      "1.84\thave\n",
      "1.88\twhich\n",
      "1.92\tnew\n",
      "\n",
      "For the first 5 docs, the top tokens are:\n",
      "\n",
      "Document 0, top terms by TF-IDF\n",
      "0.28\tdrug\n",
      "0.26\tdatabase\n",
      "0.24\tdiscovery\n",
      "0.23\tdiseases\n",
      "0.21\tprotein\n",
      "\n",
      "Document 1, top terms by TF-IDF\n",
      "0.15\tchemistry\n",
      "0.15\tnanostructures\n",
      "0.15\tassociates\n",
      "0.15\tdetected\n",
      "0.15\tacademia\n",
      "\n",
      "Document 2, top terms by TF-IDF\n",
      "0.23\tmedical\n",
      "0.18\ttesting\n",
      "0.17\tworkers\n",
      "0.17\tcould\n",
      "0.17\tpersonnel\n",
      "\n",
      "Document 3, top terms by TF-IDF\n",
      "0.18\tobservations\n",
      "0.17\tcollection\n",
      "0.17\tvertical\n",
      "0.16\tweather\n",
      "0.16\tsummer\n",
      "\n",
      "Document 4, top terms by TF-IDF\n",
      "0.22\tquantum\n",
      "0.22\tpath\n",
      "0.21\tnanostructures\n",
      "0.18\tsemiconductor\n",
      "0.17\tintegral\n",
      "\n",
      "Cluster: 0 (398 docs)\n",
      "chemistry, molecules, reactions, organic, metal, complexes, professor, compounds, reaction, chemical\n",
      "\n",
      "Cluster: 1 (631 docs)\n",
      "solar, stars, wind, magnetic, formation, waves, observations, plasma, radar, flow\n",
      "\n",
      "Cluster: 2 (470 docs)\n",
      "protein, proteins, genes, gene, cell, expression, plant, cells, plants, dna\n",
      "\n",
      "Cluster: 3 (426 docs)\n",
      "learning, engineering, curriculum, courses, course, faculty, assessment, education, modules, design\n",
      "\n",
      "Cluster: 4 (640 docs)\n",
      "software, wireless, network, distributed, power, computing, performance, networks, sensor, grid\n",
      "\n",
      "Cluster: 5 (428 docs)\n",
      "phase, commercial, ii, cost, optical, market, devices, fuel, sensor, power\n",
      "\n",
      "Cluster: 6 (620 docs)\n",
      "species, evolutionary, populations, genetic, plant, plants, diversity, population, tree, ecological\n",
      "\n",
      "Cluster: 7 (485 docs)\n",
      "theory, equations, geometry, manifolds, spaces, algebraic, geometric, topology, differential, dimensional\n",
      "\n",
      "Cluster: 8 (335 docs)\n",
      "mantle, seismic, fault, deformation, crust, rocks, tectonic, plate, continental, zone\n",
      "\n",
      "Cluster: 9 (263 docs)\n",
      "college, stem, scholarship, mathematics, faculty, academic, programs, engineering, students, minority\n",
      "\n",
      "Cluster: 10 (627 docs)\n",
      "climate, ice, ocean, carbon, arctic, sea, water, circulation, atmospheric, soil\n",
      "\n",
      "Cluster: 11 (780 docs)\n",
      "materials, magnetic, quantum, nanoscale, polymer, spin, films, nano, optical, electron\n",
      "\n",
      "Cluster: 12 (225 docs)\n",
      "available, not, zones, zone, young, york, yield, yet, years, year\n",
      "\n",
      "Cluster: 13 (545 docs)\n",
      "workshop, conference, meeting, symposium, 2002, participants, held, speakers, 2003, travel\n",
      "\n",
      "Cluster: 14 (788 docs)\n",
      "center, equipment, dr, us, facility, digital, marine, industry, university, international\n",
      "\n",
      "Cluster: 15 (1007 docs)\n",
      "algorithms, models, optimization, problems, estimation, computational, control, we, visual, statistical\n",
      "\n",
      "Cluster: 16 (84 docs)\n",
      "fellowship, mathematical, sciences, postdoctoral, training, microbial, biology, biological, minority, further\n",
      "\n",
      "Cluster: 17 (751 docs)\n",
      "social, political, policy, archaeological, decision, cultural, economic, market, labor, children\n",
      "\n",
      "Cluster: 18 (267 docs)\n",
      "teachers, school, mathematics, teacher, fellows, schools, middle, districts, 12, inquiry\n",
      "\n",
      "Cluster: 19 (153 docs)\n",
      "reu, site, summer, students, experience, recruited, week, projects, chemistry, undergraduates\n",
      "\n",
      "CPU times: user 8min 2s, sys: 3min 14s, total: 11min 16s\n",
      "Wall time: 6min 33s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# So for this, I'll take out the top and bottom words\n",
    "# Essentially stop words and\n",
    "# words that are way too niche to be classified as descriptive\n",
    "# By enforcing max_features=10000\n",
    "# min_df is there to make sure of those 10000, there's no leftover\n",
    "# Also looser clusters because too many things got put together\n",
    "pickier_setup = vectorize_cluster(\n",
    "    documents,\n",
    "    sample_size=None,\n",
    "    ngram_range=(1, 1),\n",
    "    analyzer='word',\n",
    "    max_df=0.95,\n",
    "    min_df=0.01,\n",
    "    max_features=10000,\n",
    "    use_idf=True,\n",
    "    sublinear_tf=True,\n",
    "    n_clusters=20,\n",
    "    tol=0.0001\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I tried with even stricter setup and it's a bit better, not too clear but I can identify topic much quicker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of the token matrix is: (9923, 10000)\n",
      "\n",
      "The top 20 tokens are:\n",
      "1.13\tthis\n",
      "1.14\tfor\n",
      "1.18\tis\n",
      "1.19\twill\n",
      "1.22\tof the\n",
      "1.26\tbe\n",
      "1.29\ton\n",
      "1.31\twith\n",
      "1.33\tthat\n",
      "1.40\tare\n",
      "1.40\tresearch\n",
      "1.40\tin the\n",
      "1.41\tby\n",
      "1.43\tas\n",
      "1.51\tfrom\n",
      "1.55\tan\n",
      "1.57\twill be\n",
      "1.61\tthese\n",
      "1.61\tproject\n",
      "1.61\tat\n",
      "\n",
      "For the first 5 docs, the top tokens are:\n",
      "\n",
      "Document 0, top terms by TF-IDF\n",
      "0.19\tdrug\n",
      "0.18\tthe database\n",
      "0.17\tdatabase\n",
      "0.16\tdiscovery\n",
      "0.15\tdiseases\n",
      "\n",
      "Document 1, top terms by TF-IDF\n",
      "0.20\tnmr\n",
      "0.13\tand undergraduate\n",
      "0.12\tgraduate and\n",
      "0.11\tgaas\n",
      "0.11\theterostructures\n",
      "\n",
      "Document 2, top terms by TF-IDF\n",
      "0.28\tfabric\n",
      "0.16\tprotect\n",
      "0.15\tmedical\n",
      "0.15\tmilitary\n",
      "0.13\tpolymerization\n",
      "\n",
      "Document 3, top terms by TF-IDF\n",
      "0.19\tlightning\n",
      "0.16\tsnow\n",
      "0.15\tforecasting\n",
      "0.12\twinter\n",
      "0.12\tcloud\n",
      "\n",
      "Document 4, top terms by TF-IDF\n",
      "0.17\tdots\n",
      "0.16\thelium\n",
      "0.14\tquantum\n",
      "0.14\tpath\n",
      "0.14\tstatistical mechanics\n",
      "\n",
      "Cluster: 0 (351 docs)\n",
      "mantle, seismic, fault, deformation, magma, subduction, lithosphere, crust, the mantle, rocks\n",
      "\n",
      "Cluster: 1 (774 docs)\n",
      "galaxies, stars, star, quantum, galaxy, stellar, star formation, laser, particles, dark\n",
      "\n",
      "Cluster: 2 (791 docs)\n",
      "social, political, archaeological, decision, firms, policy, cultural, children, language, organizational\n",
      "\n",
      "Cluster: 3 (876 docs)\n",
      "software, wireless, distributed, power, sensor, grid, computing, network, networks, mobile\n",
      "\n",
      "Cluster: 4 (568 docs)\n",
      "workshop, conference, the workshop, the conference, meeting, symposium, workshop will, conference will, 2002, this workshop\n",
      "\n",
      "Cluster: 5 (545 docs)\n",
      "species, evolutionary, genetic, populations, birds, plants, reproductive, plant, females, diversity\n",
      "\n",
      "Cluster: 6 (538 docs)\n",
      "genes, proteins, protein, gene, fellowship, cell, plant, signaling, cells, expression\n",
      "\n",
      "Cluster: 7 (667 docs)\n",
      "algorithms, estimation, optimization, nonlinear, stochastic, inference, regression, linear, image, equations\n",
      "\n",
      "Cluster: 8 (225 docs)\n",
      "not available, available, not, zooplankton, zones, zone, zero, zealand, yr, younger\n",
      "\n",
      "Cluster: 9 (295 docs)\n",
      "stem, scholarship, scholarships, csems, college, community college, igert, computer science, the program, the college\n",
      "\n",
      "Cluster: 10 (32 docs)\n",
      "fellowship, mathematical sciences, mathematical, sciences, postdoctoral, fellowships, zooplankton, zones, zone, zero\n",
      "\n",
      "Cluster: 11 (439 docs)\n",
      "phase ii, phase, cost, optical, sensor, fuel, ii project, commercial, market, ii\n",
      "\n",
      "Cluster: 12 (560 docs)\n",
      "spin, magnetic, nanoscale, nano, films, polymer, nanotubes, thin, optical, quantum\n",
      "\n",
      "Cluster: 13 (434 docs)\n",
      "manifolds, equations, spaces, algebraic, geometry, theory, geometric, the investigator, algebras, hyperbolic\n",
      "\n",
      "Cluster: 14 (279 docs)\n",
      "chemistry, molecules, reactions, complexes, metal, organic, nmr, catalysts, compounds, professor\n",
      "\n",
      "Cluster: 15 (152 docs)\n",
      "reu, reu site, students will, research experience, the program, summer, the reu, chemistry, 2003, the chemistry\n",
      "\n",
      "Cluster: 16 (493 docs)\n",
      "ice, climate, carbon, soil, forest, arctic, ecosystem, co2, ocean, lake\n",
      "\n",
      "Cluster: 17 (502 docs)\n",
      "teachers, learning, mathematics, curriculum, teacher, fellows, engineering, middle, school, districts\n",
      "\n",
      "Cluster: 18 (613 docs)\n",
      "solar, ocean, wind, ice, this is, ionosphere, waves, solar wind, radar, variability\n",
      "\n",
      "Cluster: 19 (789 docs)\n",
      "equipment, french, center, the center, cnrs, dr, the us, us, france, facility\n",
      "\n",
      "CPU times: user 12min 20s, sys: 2min 46s, total: 15min 7s\n",
      "Wall time: 11min 3s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# So for this, it will be the same as above\n",
    "# but with 2 words as well\n",
    "two_words_setup = vectorize_cluster(\n",
    "    documents,\n",
    "    sample_size=None,\n",
    "    ngram_range=(1, 2),\n",
    "    analyzer='word',\n",
    "    max_df=0.95,\n",
    "    min_df=0.001,\n",
    "    max_features=10000,\n",
    "    use_idf=True,\n",
    "    sublinear_tf=True,\n",
    "    n_clusters=20,\n",
    "    tol=0.0001\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This one is good too but at some point it's quite messed up so I'll go with the `pickier_setup`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0\n",
      "0.0058\treef\n",
      "0.0044\tcoral\n",
      "0.0044\tcorals\n",
      "0.0036\trig\n",
      "0.0035\tpreK\n",
      "0.0030\tWGBH\n",
      "0.0028\tPM\n",
      "0.0025\treefs\n",
      "0.0020\tjob\n",
      "0.0012\tjudicial\n",
      "\n",
      "Topic 1\n",
      "0.0374\tAvailable\n",
      "0.0111\trouting\n",
      "0.0052\tprotocol\n",
      "0.0050\tquery\n",
      "0.0028\tterminals\n",
      "0.0025\tadaptivity\n",
      "0.0023\tbox\n",
      "0.0022\tRPI\n",
      "0.0020\tvolatility\n",
      "0.0019\tInternet\n",
      "\n",
      "Topic 2\n",
      "0.0088\tresearch\n",
      "0.0035\tproject\n",
      "0.0034\ttheir\n",
      "0.0027\tworkshop\n",
      "0.0025\twhich\n",
      "0.0024\tstudy\n",
      "0.0022\tinformation\n",
      "0.0022\tdevelopment\n",
      "0.0021\thow\n",
      "0.0021\tdata\n",
      "\n",
      "Topic 3\n",
      "0.0132\tgirls\n",
      "0.0093\tchildren\n",
      "0.0083\tscience\n",
      "0.0036\ttheir\n",
      "0.0034\tproject\n",
      "0.0033\tresearch\n",
      "0.0033\tstudents\n",
      "0.0023\tgeoscience\n",
      "0.0020\tabout\n",
      "0.0019\tfMRI\n",
      "\n",
      "Topic 4\n",
      "0.0090\tdata\n",
      "0.0042\tproject\n",
      "0.0028\tsuch\n",
      "0.0028\tmodel\n",
      "0.0026\twhich\n",
      "0.0026\tbetween\n",
      "0.0026\tresearch\n",
      "0.0025\tmodels\n",
      "0.0025\thow\n",
      "0.0024\tinformation\n",
      "\n",
      "Topic 5\n",
      "0.0088\ttheory\n",
      "0.0058\tproblems\n",
      "0.0053\tstudy\n",
      "0.0053\twhich\n",
      "0.0049\tsystems\n",
      "0.0048\tequations\n",
      "0.0043\tproject\n",
      "0.0040\tsuch\n",
      "0.0034\tgeometry\n",
      "0.0034\tmathematical\n",
      "\n",
      "Topic 6\n",
      "0.0060\tproject\n",
      "0.0041\tPhase\n",
      "0.0037\thigh\n",
      "0.0034\tDNA\n",
      "0.0033\tresearch\n",
      "0.0030\tdevelopment\n",
      "0.0028\tbased\n",
      "0.0027\tsystem\n",
      "0.0026\tgene\n",
      "0.0024\tdevelop\n",
      "\n",
      "Topic 7\n",
      "0.0036\tproject\n",
      "0.0030\twhich\n",
      "0.0029\tresearch\n",
      "0.0026\tcell\n",
      "0.0026\tgenes\n",
      "0.0025\tspecies\n",
      "0.0025\tunderstanding\n",
      "0.0023\tbetween\n",
      "0.0023\ttheir\n",
      "0.0022\tused\n",
      "\n",
      "Topic 8\n",
      "0.0083\tresearch\n",
      "0.0043\tproject\n",
      "0.0033\tsystems\n",
      "0.0028\tsystem\n",
      "0.0027\tbased\n",
      "0.0027\thigh\n",
      "0.0026\tsuch\n",
      "0.0026\tdesign\n",
      "0.0026\twhich\n",
      "0.0022\tuse\n",
      "\n",
      "Topic 9\n",
      "0.0104\tnoncommutative\n",
      "0.0042\tKaehler\n",
      "0.0042\tadic\n",
      "0.0027\tBaum\n",
      "0.0027\tmanifolds\n",
      "0.0021\tstring\n",
      "0.0020\tlattices\n",
      "0.0015\tDiophantine\n",
      "0.0013\tHopf\n",
      "0.0011\ttheory\n",
      "\n",
      "Topic 10\n",
      "0.0031\tpush\n",
      "0.0013\tdata\n",
      "0.0013\tmobile\n",
      "0.0012\tbroadcast\n",
      "0.0008\twireless\n",
      "0.0007\tscience\n",
      "0.0006\tstudents\n",
      "0.0005\twhich\n",
      "0.0004\tresearch\n",
      "0.0004\tusers\n",
      "\n",
      "Topic 11\n",
      "0.0048\tXML\n",
      "0.0047\tvesicles\n",
      "0.0044\tdata\n",
      "0.0039\tproject\n",
      "0.0033\tcompressed\n",
      "0.0032\tmanifold\n",
      "0.0026\tfusion\n",
      "0.0024\tresearch\n",
      "0.0023\ttext\n",
      "0.0022\twhich\n",
      "\n",
      "Topic 12\n",
      "0.0038\tretreat\n",
      "0.0034\tDFT\n",
      "0.0034\tresearch\n",
      "0.0034\tCLR\n",
      "0.0025\tproject\n",
      "0.0025\tproprietary\n",
      "0.0018\tsystem\n",
      "0.0018\tlegume\n",
      "0.0017\tsubducting\n",
      "0.0015\tsystems\n",
      "\n",
      "Topic 13\n",
      "0.0129\tUniversity\n",
      "0.0101\tresearch\n",
      "0.0098\tchemistry\n",
      "0.0083\tChemistry\n",
      "0.0067\torganic\n",
      "0.0064\treactions\n",
      "0.0059\taward\n",
      "0.0057\tstudents\n",
      "0.0051\tmolecules\n",
      "0.0048\tmaterials\n",
      "\n",
      "Topic 14\n",
      "0.0076\teruptions\n",
      "0.0066\tcrystallization\n",
      "0.0045\teruptive\n",
      "0.0045\tdegassing\n",
      "0.0043\tmagma\n",
      "0.0023\talkaline\n",
      "0.0018\teruption\n",
      "0.0017\tmagmas\n",
      "0.0016\tEAR\n",
      "0.0015\tOs\n",
      "\n",
      "Topic 15\n",
      "0.0192\tstudents\n",
      "0.0109\tresearch\n",
      "0.0098\tprogram\n",
      "0.0080\tscience\n",
      "0.0066\tUniversity\n",
      "0.0055\tfaculty\n",
      "0.0054\tproject\n",
      "0.0053\tengineering\n",
      "0.0051\ttheir\n",
      "0.0050\tprograms\n",
      "\n",
      "Topic 16\n",
      "0.0051\tmodels\n",
      "0.0043\tresearch\n",
      "0.0042\tproject\n",
      "0.0039\tflow\n",
      "0.0038\tdynamics\n",
      "0.0036\tmodeling\n",
      "0.0036\tfluid\n",
      "0.0031\tcomputational\n",
      "0.0029\tscale\n",
      "0.0028\twaves\n",
      "\n",
      "Topic 17\n",
      "0.0048\tresearch\n",
      "0.0048\tdata\n",
      "0.0037\tproject\n",
      "0.0036\tstudy\n",
      "0.0025\tbetween\n",
      "0.0025\twhich\n",
      "0.0023\tprovide\n",
      "0.0023\tclimate\n",
      "0.0022\tspecies\n",
      "0.0022\thow\n",
      "\n",
      "Topic 18\n",
      "0.0112\tmaterials\n",
      "0.0057\tresearch\n",
      "0.0053\tproperties\n",
      "0.0049\toptical\n",
      "0.0043\tproject\n",
      "0.0043\tdevices\n",
      "0.0041\tsurface\n",
      "0.0039\tmagnetic\n",
      "0.0037\thigh\n",
      "0.0034\tused\n",
      "\n",
      "Topic 19\n",
      "0.0035\tRAM\n",
      "0.0015\ttape\n",
      "0.0012\tPipeline\n",
      "0.0012\tbackup\n",
      "0.0011\treflectance\n",
      "0.0009\tdata\n",
      "0.0009\tproject\n",
      "0.0007\tresearch\n",
      "0.0007\tCPU\n",
      "0.0006\tscience\n",
      "\n",
      "CPU times: user 26.3 s, sys: 16.7 ms, total: 26.3 s\n",
      "Wall time: 26.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "new_vectorizer = TfidfVectorizer(\n",
    "    documents,\n",
    "    ngram_range=(1, 1),\n",
    "    analyzer='word',\n",
    "    max_df=0.95,\n",
    "    min_df=0.01,\n",
    "    max_features=10000,\n",
    "    use_idf=True,\n",
    "    sublinear_tf=True\n",
    ")\n",
    "# new_vectorizer = TfidfVectorizer()\n",
    "word_tokenizer = new_vectorizer.build_tokenizer()\n",
    "tokenized_text = [word_tokenizer(doc) for doc in documents]\n",
    "\n",
    "dictionary = corpora.Dictionary(tokenized_text)\n",
    "lda_corpus = [dictionary.doc2bow(text) for text in tokenized_text]\n",
    "lda_model = models.LdaModel(lda_corpus, id2word=dictionary, num_topics=20)\n",
    "\n",
    "for i, topic in lda_model.show_topics(num_topics=20, num_words=50, formatted=False):\n",
    "    print(\"Topic\", i)\n",
    "    printed_terms = 0\n",
    "    for term, score in topic:\n",
    "        if printed_terms >= 10:\n",
    "            break\n",
    "        elif term in \"this This that That these These have will Will\\\n",
    "        the of and to for in or The is be may an a with at are on by as from can \\\n",
    "        In it It has Has also not Not new\".split():\n",
    "            continue\n",
    "        printed_terms += 1\n",
    "        print(\"%.4f\\t%s\" % (score,term))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's rather quick and output the results quite nicely. But with a little help of preprocessing on the tokenizer to filter out the noises, it's more meaningful than on its own"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"word-vector\"></a>\n",
    "### Word vectors\n",
    "\n",
    "[top](#first-part)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_vectorizer = TfidfVectorizer()\n",
    "# new_vectorizer = TfidfVectorizer()\n",
    "word_tokenizer = new_vectorizer.build_tokenizer()\n",
    "tokenized_text = [word_tokenizer(doc) for doc in documents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-03-12 18:34:57,449 : INFO : collecting all words and their counts\n",
      "2020-03-12 18:34:57,450 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2020-03-12 18:34:57,855 : INFO : collected 63538 word types from a corpus of 2656274 raw words and 9923 sentences\n",
      "2020-03-12 18:34:57,856 : INFO : Loading a fresh vocabulary\n",
      "2020-03-12 18:34:57,908 : INFO : effective_min_count=5 retains 20752 unique words (32% of original 63538, drops 42786)\n",
      "2020-03-12 18:34:57,908 : INFO : effective_min_count=5 leaves 2585188 word corpus (97% of original 2656274, drops 71086)\n",
      "2020-03-12 18:34:57,966 : INFO : deleting the raw counts dictionary of 63538 items\n",
      "2020-03-12 18:34:57,968 : INFO : sample=0.001 downsamples 26 most-common words\n",
      "2020-03-12 18:34:57,969 : INFO : downsampling leaves estimated 2005620 word corpus (77.6% of prior 2585188)\n",
      "2020-03-12 18:34:58,017 : INFO : estimated required memory for 20752 words and 100 dimensions: 26977600 bytes\n",
      "2020-03-12 18:34:58,018 : INFO : resetting layer weights\n",
      "2020-03-12 18:35:01,199 : INFO : training model with 4 workers on 20752 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2020-03-12 18:35:02,207 : INFO : EPOCH 1 - PROGRESS: at 97.34% examples, 1948335 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-12 18:35:02,222 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-03-12 18:35:02,223 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-03-12 18:35:02,226 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-03-12 18:35:02,228 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-03-12 18:35:02,229 : INFO : EPOCH - 1 : training on 2656274 raw words (2005187 effective words) took 1.0s, 1957031 effective words/s\n",
      "2020-03-12 18:35:03,236 : INFO : EPOCH 2 - PROGRESS: at 98.53% examples, 1968354 words/s, in_qsize 5, out_qsize 0\n",
      "2020-03-12 18:35:03,241 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-03-12 18:35:03,242 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-03-12 18:35:03,244 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-03-12 18:35:03,247 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-03-12 18:35:03,248 : INFO : EPOCH - 2 : training on 2656274 raw words (2005271 effective words) took 1.0s, 1975306 effective words/s\n",
      "2020-03-12 18:35:04,225 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-03-12 18:35:04,226 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-03-12 18:35:04,228 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-03-12 18:35:04,230 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-03-12 18:35:04,231 : INFO : EPOCH - 3 : training on 2656274 raw words (2005467 effective words) took 1.0s, 2047978 effective words/s\n",
      "2020-03-12 18:35:05,210 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-03-12 18:35:05,212 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-03-12 18:35:05,214 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-03-12 18:35:05,217 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-03-12 18:35:05,217 : INFO : EPOCH - 4 : training on 2656274 raw words (2006166 effective words) took 1.0s, 2041543 effective words/s\n",
      "2020-03-12 18:35:06,224 : INFO : EPOCH 5 - PROGRESS: at 64.59% examples, 1282007 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-12 18:35:06,731 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-03-12 18:35:06,735 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-03-12 18:35:06,737 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-03-12 18:35:06,738 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-03-12 18:35:06,739 : INFO : EPOCH - 5 : training on 2656274 raw words (2006157 effective words) took 1.5s, 1322164 effective words/s\n",
      "2020-03-12 18:35:06,739 : INFO : training on a 13281370 raw words (10028248 effective words) took 5.5s, 1810297 effective words/s\n",
      "2020-03-12 18:35:06,740 : INFO : precomputing L2-norms of word weight vectors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most similar to: silicon\n",
      "[('nanoparticles', 0.9142018556594849), ('films', 0.9072830677032471), ('thin', 0.9039884805679321), ('fibers', 0.9034739136695862), ('composites', 0.8895953893661499), ('film', 0.8893566131591797), ('amorphous', 0.882348895072937), ('aluminum', 0.8821524381637573), ('doped', 0.8797479867935181), ('oxide', 0.8784518241882324)]\n",
      "\n",
      "Most similar to: flux\n",
      "[('fluxes', 0.9093732833862305), ('thickness', 0.8973891735076904), ('concentration', 0.8948220014572144), ('melting', 0.8926695585250854), ('melt', 0.8925642371177673), ('clouds', 0.8725210428237915), ('accumulation', 0.8716062307357788), ('momentum', 0.8660423755645752), ('precipitation', 0.8651330471038818), ('dust', 0.8647468090057373)]\n",
      "\n",
      "Most similar to: stratosphere\n",
      "[('troposphere', 0.9066334366798401), ('porewaters', 0.9060134291648865), ('thickening', 0.9047499299049377), ('basaltic', 0.8800020217895508), ('plume', 0.879138708114624), ('thermosphere', 0.8738328218460083), ('mesosphere', 0.8577731847763062), ('ionosphere', 0.8565584421157837), ('aerosols', 0.8561145663261414), ('Ba', 0.8551013469696045)]\n",
      "\n",
      "Most similar to: music\n",
      "[('lineups', 0.9048930406570435), ('ESC', 0.9003894329071045), ('checks', 0.8960487842559814), ('statically', 0.8923394680023193), ('Website', 0.889431357383728), ('sci', 0.885297417640686), ('Error', 0.8832237720489502), ('CdSe', 0.8826782703399658), ('hopping', 0.8823786973953247), ('neuro', 0.8812859058380127)]\n",
      "\n",
      "Most similar to: pitch\n",
      "[('multipole', 0.9094412922859192), ('steerable', 0.9017374515533447), ('Size', 0.9011666774749756), ('malignant', 0.8997153043746948), ('Squares', 0.8950706720352173), ('cylinder', 0.8888410329818726), ('Least', 0.8865343928337097), ('your', 0.8853967785835266), ('Helmholtz', 0.8838983774185181), ('LC', 0.8835553526878357)]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "vectors = gensim.models.Word2Vec(tokenized_text, size=100, window=5, min_count=5, sg=0, workers=4)\n",
    "\n",
    "print(\"Most similar to:\", 'silicon')\n",
    "print(vectors.wv.most_similar('silicon'))\n",
    "print()\n",
    "\n",
    "print(\"Most similar to:\", 'flux')\n",
    "print(vectors.wv.most_similar('flux'))\n",
    "print()\n",
    "\n",
    "print(\"Most similar to:\", 'stratosphere')\n",
    "print(vectors.wv.most_similar('stratosphere'))\n",
    "print()\n",
    "\n",
    "print(\"Most similar to:\", 'music')\n",
    "print(vectors.wv.most_similar('music'))\n",
    "print()\n",
    "\n",
    "print(\"Most similar to:\", 'pitch')\n",
    "print(vectors.wv.most_similar('pitch'))\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-03-12 18:36:14,779 : INFO : collecting all words and their counts\n",
      "2020-03-12 18:36:14,780 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2020-03-12 18:36:15,170 : INFO : collected 63538 word types from a corpus of 2656274 raw words and 9923 sentences\n",
      "2020-03-12 18:36:15,171 : INFO : Loading a fresh vocabulary\n",
      "2020-03-12 18:36:15,373 : INFO : effective_min_count=5 retains 20752 unique words (32% of original 63538, drops 42786)\n",
      "2020-03-12 18:36:15,374 : INFO : effective_min_count=5 leaves 2585188 word corpus (97% of original 2656274, drops 71086)\n",
      "2020-03-12 18:36:15,437 : INFO : deleting the raw counts dictionary of 63538 items\n",
      "2020-03-12 18:36:15,439 : INFO : sample=0.001 downsamples 26 most-common words\n",
      "2020-03-12 18:36:15,440 : INFO : downsampling leaves estimated 2005620 word corpus (77.6% of prior 2585188)\n",
      "2020-03-12 18:36:15,489 : INFO : estimated required memory for 20752 words and 100 dimensions: 26977600 bytes\n",
      "2020-03-12 18:36:15,490 : INFO : resetting layer weights\n",
      "2020-03-12 18:36:18,673 : INFO : training model with 4 workers on 20752 vocabulary and 100 features, using sg=1 hs=0 sample=0.001 negative=5 window=5\n",
      "2020-03-12 18:36:19,693 : INFO : EPOCH 1 - PROGRESS: at 19.00% examples, 366841 words/s, in_qsize 8, out_qsize 0\n",
      "2020-03-12 18:36:20,693 : INFO : EPOCH 1 - PROGRESS: at 37.34% examples, 368804 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-12 18:36:21,699 : INFO : EPOCH 1 - PROGRESS: at 58.02% examples, 383577 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-12 18:36:22,714 : INFO : EPOCH 1 - PROGRESS: at 84.23% examples, 415944 words/s, in_qsize 8, out_qsize 0\n",
      "2020-03-12 18:36:23,274 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-03-12 18:36:23,281 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-03-12 18:36:23,284 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-03-12 18:36:23,300 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-03-12 18:36:23,300 : INFO : EPOCH - 1 : training on 2656274 raw words (2005187 effective words) took 4.6s, 433900 effective words/s\n",
      "2020-03-12 18:36:24,315 : INFO : EPOCH 2 - PROGRESS: at 18.24% examples, 353670 words/s, in_qsize 8, out_qsize 0\n",
      "2020-03-12 18:36:25,333 : INFO : EPOCH 2 - PROGRESS: at 43.22% examples, 431886 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-12 18:36:26,342 : INFO : EPOCH 2 - PROGRESS: at 71.50% examples, 466972 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-12 18:36:27,346 : INFO : EPOCH 2 - PROGRESS: at 97.73% examples, 485023 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-12 18:36:27,397 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-03-12 18:36:27,399 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-03-12 18:36:27,403 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-03-12 18:36:27,410 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-03-12 18:36:27,411 : INFO : EPOCH - 2 : training on 2656274 raw words (2005219 effective words) took 4.1s, 488426 effective words/s\n",
      "2020-03-12 18:36:28,440 : INFO : EPOCH 3 - PROGRESS: at 23.39% examples, 450140 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-12 18:36:29,450 : INFO : EPOCH 3 - PROGRESS: at 43.22% examples, 430666 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-12 18:36:30,497 : INFO : EPOCH 3 - PROGRESS: at 62.71% examples, 404922 words/s, in_qsize 8, out_qsize 0\n",
      "2020-03-12 18:36:31,497 : INFO : EPOCH 3 - PROGRESS: at 82.34% examples, 402069 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-12 18:36:32,309 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-03-12 18:36:32,318 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-03-12 18:36:32,357 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-03-12 18:36:32,358 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-03-12 18:36:32,359 : INFO : EPOCH - 3 : training on 2656274 raw words (2005231 effective words) took 4.9s, 405639 effective words/s\n",
      "2020-03-12 18:36:33,367 : INFO : EPOCH 4 - PROGRESS: at 21.18% examples, 414300 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-12 18:36:34,381 : INFO : EPOCH 4 - PROGRESS: at 46.97% examples, 470816 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-12 18:36:35,407 : INFO : EPOCH 4 - PROGRESS: at 69.58% examples, 453776 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-12 18:36:36,422 : INFO : EPOCH 4 - PROGRESS: at 87.16% examples, 430084 words/s, in_qsize 8, out_qsize 0\n",
      "2020-03-12 18:36:37,028 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-03-12 18:36:37,033 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-03-12 18:36:37,038 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-03-12 18:36:37,058 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-03-12 18:36:37,059 : INFO : EPOCH - 4 : training on 2656274 raw words (2005703 effective words) took 4.7s, 427090 effective words/s\n",
      "2020-03-12 18:36:38,082 : INFO : EPOCH 5 - PROGRESS: at 19.38% examples, 372498 words/s, in_qsize 8, out_qsize 0\n",
      "2020-03-12 18:36:39,089 : INFO : EPOCH 5 - PROGRESS: at 42.16% examples, 421492 words/s, in_qsize 8, out_qsize 0\n",
      "2020-03-12 18:36:40,092 : INFO : EPOCH 5 - PROGRESS: at 62.71% examples, 412092 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-12 18:36:41,138 : INFO : EPOCH 5 - PROGRESS: at 80.97% examples, 397479 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-12 18:36:42,140 : INFO : EPOCH 5 - PROGRESS: at 98.53% examples, 389040 words/s, in_qsize 5, out_qsize 0\n",
      "2020-03-12 18:36:42,158 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-03-12 18:36:42,161 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-03-12 18:36:42,167 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-03-12 18:36:42,197 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-03-12 18:36:42,198 : INFO : EPOCH - 5 : training on 2656274 raw words (2005917 effective words) took 5.1s, 390679 effective words/s\n",
      "2020-03-12 18:36:42,198 : INFO : training on a 13281370 raw words (10027257 effective words) took 23.5s, 426255 effective words/s\n",
      "2020-03-12 18:36:42,205 : INFO : precomputing L2-norms of word weight vectors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most similar to: silicon\n",
      "[('carbide', 0.8787307739257812), ('nitride', 0.8575783967971802), ('wafers', 0.8548914194107056), ('SOI', 0.8428232669830322), ('SiC', 0.8398675322532654), ('LEDs', 0.8305456042289734), ('semiconducting', 0.8267442584037781), ('nanocomposite', 0.8259124159812927), ('germanium', 0.8222129344940186), ('wafer', 0.8169799447059631)]\n",
      "\n",
      "Most similar to: flux\n",
      "[('fluxes', 0.8159793615341187), ('heat', 0.786853551864624), ('latent', 0.7678372263908386), ('canopies', 0.7632498741149902), ('denitrification', 0.7597169280052185), ('firn', 0.7594362497329712), ('transports', 0.7594219446182251), ('momentum', 0.759409487247467), ('Hg', 0.7592573165893555), ('longwave', 0.7577768564224243)]\n",
      "\n",
      "Most similar to: stratosphere\n",
      "[('mesosphere', 0.8974559903144836), ('troposphere', 0.8770797252655029), ('tropopause', 0.8468611240386963), ('transports', 0.8443900942802429), ('thermosphere', 0.8377493619918823), ('midlatitude', 0.8325599431991577), ('remineralization', 0.8293002843856812), ('outflow', 0.8257754445075989), ('circulating', 0.824863076210022), ('neck', 0.8236010074615479)]\n",
      "\n",
      "Most similar to: music\n",
      "[('textual', 0.8909928202629089), ('applets', 0.8862156271934509), ('aids', 0.8861073851585388), ('graphic', 0.8853927254676819), ('lexical', 0.8718645572662354), ('musical', 0.8705242872238159), ('references', 0.8703022599220276), ('summarization', 0.8687652945518494), ('pointers', 0.8686345815658569), ('multimodal', 0.8677190542221069)]\n",
      "\n",
      "Most similar to: pitch\n",
      "[('differencing', 0.8925780057907104), ('gravitationally', 0.8914796113967896), ('MB', 0.8904963731765747), ('smoother', 0.8898231983184814), ('compress', 0.8894486427307129), ('multipole', 0.8873230218887329), ('bump', 0.8840577006340027), ('spurious', 0.8800601959228516), ('decoder', 0.8797399401664734), ('coherently', 0.8788646459579468)]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "vectors = gensim.models.Word2Vec(tokenized_text, size=100, window=5, min_count=5, sg=1, workers=4)\n",
    "\n",
    "print(\"Most similar to:\", 'silicon')\n",
    "print(vectors.wv.most_similar('silicon'))\n",
    "print()\n",
    "\n",
    "print(\"Most similar to:\", 'flux')\n",
    "print(vectors.wv.most_similar('flux'))\n",
    "print()\n",
    "\n",
    "print(\"Most similar to:\", 'stratosphere')\n",
    "print(vectors.wv.most_similar('stratosphere'))\n",
    "print()\n",
    "\n",
    "print(\"Most similar to:\", 'music')\n",
    "print(vectors.wv.most_similar('music'))\n",
    "print()\n",
    "\n",
    "print(\"Most similar to:\", 'pitch')\n",
    "print(vectors.wv.most_similar('pitch'))\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This makes more sense to me, though the music is still a bump since there's nothing about classical music out of all these abstract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-03-12 18:38:59,749 : INFO : collecting all words and their counts\n",
      "2020-03-12 18:38:59,752 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2020-03-12 18:39:00,141 : INFO : collected 63538 word types from a corpus of 2656274 raw words and 9923 sentences\n",
      "2020-03-12 18:39:00,142 : INFO : Loading a fresh vocabulary\n",
      "2020-03-12 18:39:00,188 : INFO : effective_min_count=10 retains 13531 unique words (21% of original 63538, drops 50007)\n",
      "2020-03-12 18:39:00,189 : INFO : effective_min_count=10 leaves 2538020 word corpus (95% of original 2656274, drops 118254)\n",
      "2020-03-12 18:39:00,222 : INFO : deleting the raw counts dictionary of 63538 items\n",
      "2020-03-12 18:39:00,224 : INFO : sample=0.001 downsamples 26 most-common words\n",
      "2020-03-12 18:39:00,224 : INFO : downsampling leaves estimated 1955256 word corpus (77.0% of prior 2538020)\n",
      "2020-03-12 18:39:00,250 : INFO : estimated required memory for 13531 words and 100 dimensions: 17590300 bytes\n",
      "2020-03-12 18:39:00,250 : INFO : resetting layer weights\n",
      "2020-03-12 18:39:02,380 : INFO : training model with 4 workers on 13531 vocabulary and 100 features, using sg=1 hs=0 sample=0.001 negative=5 window=3\n",
      "2020-03-12 18:39:03,401 : INFO : EPOCH 1 - PROGRESS: at 28.41% examples, 543705 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-12 18:39:04,418 : INFO : EPOCH 1 - PROGRESS: at 67.00% examples, 638471 words/s, in_qsize 8, out_qsize 1\n",
      "2020-03-12 18:39:05,261 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-03-12 18:39:05,275 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-03-12 18:39:05,276 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-03-12 18:39:05,279 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-03-12 18:39:05,280 : INFO : EPOCH - 1 : training on 2656274 raw words (1955786 effective words) took 2.9s, 675919 effective words/s\n",
      "2020-03-12 18:39:06,287 : INFO : EPOCH 2 - PROGRESS: at 27.33% examples, 528535 words/s, in_qsize 8, out_qsize 0\n",
      "2020-03-12 18:39:07,291 : INFO : EPOCH 2 - PROGRESS: at 64.97% examples, 628272 words/s, in_qsize 8, out_qsize 0\n",
      "2020-03-12 18:39:08,300 : INFO : EPOCH 2 - PROGRESS: at 96.98% examples, 629084 words/s, in_qsize 8, out_qsize 0\n",
      "2020-03-12 18:39:08,358 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-03-12 18:39:08,363 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-03-12 18:39:08,368 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-03-12 18:39:08,370 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-03-12 18:39:08,370 : INFO : EPOCH - 2 : training on 2656274 raw words (1955449 effective words) took 3.1s, 633714 effective words/s\n",
      "2020-03-12 18:39:09,379 : INFO : EPOCH 3 - PROGRESS: at 28.41% examples, 548990 words/s, in_qsize 8, out_qsize 0\n",
      "2020-03-12 18:39:10,399 : INFO : EPOCH 3 - PROGRESS: at 61.58% examples, 590091 words/s, in_qsize 8, out_qsize 0\n",
      "2020-03-12 18:39:11,400 : INFO : EPOCH 3 - PROGRESS: at 93.91% examples, 607440 words/s, in_qsize 6, out_qsize 0\n",
      "2020-03-12 18:39:11,543 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-03-12 18:39:11,547 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-03-12 18:39:11,548 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-03-12 18:39:11,549 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-03-12 18:39:11,550 : INFO : EPOCH - 3 : training on 2656274 raw words (1954657 effective words) took 3.2s, 615638 effective words/s\n",
      "2020-03-12 18:39:12,562 : INFO : EPOCH 4 - PROGRESS: at 39.12% examples, 758292 words/s, in_qsize 8, out_qsize 0\n",
      "2020-03-12 18:39:13,566 : INFO : EPOCH 4 - PROGRESS: at 77.35% examples, 754055 words/s, in_qsize 8, out_qsize 0\n",
      "2020-03-12 18:39:14,127 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-03-12 18:39:14,131 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-03-12 18:39:14,148 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-03-12 18:39:14,152 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-03-12 18:39:14,153 : INFO : EPOCH - 4 : training on 2656274 raw words (1955689 effective words) took 2.6s, 753930 effective words/s\n",
      "2020-03-12 18:39:15,169 : INFO : EPOCH 5 - PROGRESS: at 30.44% examples, 588665 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-12 18:39:16,189 : INFO : EPOCH 5 - PROGRESS: at 62.71% examples, 599516 words/s, in_qsize 8, out_qsize 0\n",
      "2020-03-12 18:39:17,191 : INFO : EPOCH 5 - PROGRESS: at 95.02% examples, 613606 words/s, in_qsize 8, out_qsize 0\n",
      "2020-03-12 18:39:17,279 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-03-12 18:39:17,284 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-03-12 18:39:17,293 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-03-12 18:39:17,295 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-03-12 18:39:17,296 : INFO : EPOCH - 5 : training on 2656274 raw words (1955646 effective words) took 3.1s, 623546 effective words/s\n",
      "2020-03-12 18:39:17,296 : INFO : training on a 13281370 raw words (9777227 effective words) took 14.9s, 655505 effective words/s\n",
      "2020-03-12 18:39:17,302 : INFO : precomputing L2-norms of word weight vectors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most similar to: silicon\n",
      "[('carbide', 0.8942508697509766), ('SiC', 0.8837494850158691), ('LEDs', 0.8702294826507568), ('nitride', 0.8609273433685303), ('SOI', 0.8567335605621338), ('wafers', 0.8488566875457764), ('GaN', 0.8457410931587219), ('semiconducting', 0.8403136730194092), ('doped', 0.8397879600524902), ('nanotubes', 0.8320607542991638)]\n",
      "\n",
      "Most similar to: flux\n",
      "[('fluxes', 0.8522653579711914), ('momentum', 0.8182167410850525), ('longwave', 0.8127259016036987), ('transports', 0.7976531982421875), ('salinity', 0.7946630716323853), ('O2', 0.7905195951461792), ('moisture', 0.7879533767700195), ('steep', 0.7848137617111206), ('outflow', 0.7828040719032288), ('thermosphere', 0.7814702987670898)]\n",
      "\n",
      "Most similar to: stratosphere\n",
      "[('mesosphere', 0.8834695816040039), ('troposphere', 0.8829984664916992), ('thermosphere', 0.875001072883606), ('EPS', 0.8548007011413574), ('Archean', 0.8498013019561768), ('overlying', 0.8486577868461609), ('carbonaceous', 0.848038375377655), ('transports', 0.8448255062103271), ('asthenosphere', 0.840819776058197), ('diapycnal', 0.8390008807182312)]\n",
      "\n",
      "Most similar to: music\n",
      "[('vocabulary', 0.870766282081604), ('graphic', 0.8691564798355103), ('aids', 0.8571217060089111), ('textual', 0.8543130159378052), ('relational', 0.8533315658569336), ('references', 0.8496170043945312), ('expressive', 0.8489760160446167), ('multimodal', 0.848456859588623), ('newer', 0.8477634191513062), ('standardization', 0.8475023508071899)]\n",
      "\n",
      "Most similar to: pitch\n",
      "[('pipe', 0.8961614370346069), ('towers', 0.8923881649971008), ('mask', 0.8918627500534058), ('pixels', 0.88997483253479), ('modality', 0.886391282081604), ('tunability', 0.8850734233856201), ('fetch', 0.8850722312927246), ('widths', 0.8843844532966614), ('symbol', 0.8835574984550476), ('diffusing', 0.8830643892288208)]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "vectors = gensim.models.Word2Vec(tokenized_text, size=100, window=3, min_count=10, sg=1, workers=4)\n",
    "\n",
    "print(\"Most similar to:\", 'silicon')\n",
    "print(vectors.wv.most_similar('silicon'))\n",
    "print()\n",
    "\n",
    "print(\"Most similar to:\", 'flux')\n",
    "print(vectors.wv.most_similar('flux'))\n",
    "print()\n",
    "\n",
    "print(\"Most similar to:\", 'stratosphere')\n",
    "print(vectors.wv.most_similar('stratosphere'))\n",
    "print()\n",
    "\n",
    "print(\"Most similar to:\", 'music')\n",
    "print(vectors.wv.most_similar('music'))\n",
    "print()\n",
    "\n",
    "print(\"Most similar to:\", 'pitch')\n",
    "print(vectors.wv.most_similar('pitch'))\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not much different but then it also looks pretty great. Especially in pitch, there's `tunability` now, that's what I was looking for actually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-03-12 18:41:55,212 : INFO : collecting all words and their counts\n",
      "2020-03-12 18:41:55,213 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2020-03-12 18:41:55,600 : INFO : collected 63538 word types from a corpus of 2656274 raw words and 9923 sentences\n",
      "2020-03-12 18:41:55,601 : INFO : Loading a fresh vocabulary\n",
      "2020-03-12 18:41:55,644 : INFO : effective_min_count=10 retains 13531 unique words (21% of original 63538, drops 50007)\n",
      "2020-03-12 18:41:55,645 : INFO : effective_min_count=10 leaves 2538020 word corpus (95% of original 2656274, drops 118254)\n",
      "2020-03-12 18:41:55,680 : INFO : deleting the raw counts dictionary of 63538 items\n",
      "2020-03-12 18:41:55,681 : INFO : sample=0.001 downsamples 26 most-common words\n",
      "2020-03-12 18:41:55,682 : INFO : downsampling leaves estimated 1955256 word corpus (77.0% of prior 2538020)\n",
      "2020-03-12 18:41:55,707 : INFO : estimated required memory for 13531 words and 1000 dimensions: 115013500 bytes\n",
      "2020-03-12 18:41:55,708 : INFO : resetting layer weights\n",
      "2020-03-12 18:41:57,898 : INFO : training model with 4 workers on 13531 vocabulary and 1000 features, using sg=1 hs=0 sample=0.001 negative=5 window=3\n",
      "2020-03-12 18:41:58,929 : INFO : EPOCH 1 - PROGRESS: at 8.69% examples, 162237 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-12 18:41:59,929 : INFO : EPOCH 1 - PROGRESS: at 18.65% examples, 175802 words/s, in_qsize 8, out_qsize 0\n",
      "2020-03-12 18:42:00,933 : INFO : EPOCH 1 - PROGRESS: at 30.43% examples, 196061 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-12 18:42:01,958 : INFO : EPOCH 1 - PROGRESS: at 41.59% examples, 201829 words/s, in_qsize 8, out_qsize 0\n",
      "2020-03-12 18:42:02,971 : INFO : EPOCH 1 - PROGRESS: at 51.03% examples, 197171 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-12 18:42:03,975 : INFO : EPOCH 1 - PROGRESS: at 59.76% examples, 192019 words/s, in_qsize 8, out_qsize 0\n",
      "2020-03-12 18:42:05,042 : INFO : EPOCH 1 - PROGRESS: at 69.13% examples, 187733 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-12 18:42:06,050 : INFO : EPOCH 1 - PROGRESS: at 77.35% examples, 185840 words/s, in_qsize 8, out_qsize 0\n",
      "2020-03-12 18:42:07,110 : INFO : EPOCH 1 - PROGRESS: at 87.48% examples, 185773 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-12 18:42:08,032 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-03-12 18:42:08,072 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-03-12 18:42:08,074 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-03-12 18:42:08,076 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-03-12 18:42:08,077 : INFO : EPOCH - 1 : training on 2656274 raw words (1955244 effective words) took 10.2s, 192225 effective words/s\n",
      "2020-03-12 18:42:09,180 : INFO : EPOCH 2 - PROGRESS: at 10.65% examples, 184254 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-12 18:42:10,211 : INFO : EPOCH 2 - PROGRESS: at 20.11% examples, 180568 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-12 18:42:11,225 : INFO : EPOCH 2 - PROGRESS: at 29.48% examples, 182203 words/s, in_qsize 8, out_qsize 0\n",
      "2020-03-12 18:42:12,259 : INFO : EPOCH 2 - PROGRESS: at 39.13% examples, 182098 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-12 18:42:13,267 : INFO : EPOCH 2 - PROGRESS: at 48.08% examples, 182889 words/s, in_qsize 8, out_qsize 0\n",
      "2020-03-12 18:42:14,271 : INFO : EPOCH 2 - PROGRESS: at 58.44% examples, 183617 words/s, in_qsize 8, out_qsize 0\n",
      "2020-03-12 18:42:15,285 : INFO : EPOCH 2 - PROGRESS: at 68.06% examples, 183010 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-12 18:42:16,288 : INFO : EPOCH 2 - PROGRESS: at 76.19% examples, 180877 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-12 18:42:17,318 : INFO : EPOCH 2 - PROGRESS: at 85.80% examples, 181212 words/s, in_qsize 8, out_qsize 0\n",
      "2020-03-12 18:42:18,323 : INFO : EPOCH 2 - PROGRESS: at 95.38% examples, 182332 words/s, in_qsize 8, out_qsize 0\n",
      "2020-03-12 18:42:18,710 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-03-12 18:42:18,729 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-03-12 18:42:18,741 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-03-12 18:42:18,788 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-03-12 18:42:18,789 : INFO : EPOCH - 2 : training on 2656274 raw words (1955236 effective words) took 10.7s, 182597 effective words/s\n",
      "2020-03-12 18:42:19,809 : INFO : EPOCH 3 - PROGRESS: at 8.69% examples, 163707 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-12 18:42:20,833 : INFO : EPOCH 3 - PROGRESS: at 18.63% examples, 174528 words/s, in_qsize 8, out_qsize 0\n",
      "2020-03-12 18:42:21,841 : INFO : EPOCH 3 - PROGRESS: at 30.14% examples, 192650 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-12 18:42:22,841 : INFO : EPOCH 3 - PROGRESS: at 42.54% examples, 207477 words/s, in_qsize 8, out_qsize 0\n",
      "2020-03-12 18:42:23,867 : INFO : EPOCH 3 - PROGRESS: at 55.15% examples, 212750 words/s, in_qsize 8, out_qsize 0\n",
      "2020-03-12 18:42:24,885 : INFO : EPOCH 3 - PROGRESS: at 68.80% examples, 218753 words/s, in_qsize 8, out_qsize 0\n",
      "2020-03-12 18:42:25,902 : INFO : EPOCH 3 - PROGRESS: at 80.05% examples, 220050 words/s, in_qsize 8, out_qsize 0\n",
      "2020-03-12 18:42:26,903 : INFO : EPOCH 3 - PROGRESS: at 91.71% examples, 221413 words/s, in_qsize 8, out_qsize 0\n",
      "2020-03-12 18:42:27,669 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-03-12 18:42:27,677 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-03-12 18:42:27,684 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-03-12 18:42:27,746 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-03-12 18:42:27,746 : INFO : EPOCH - 3 : training on 2656274 raw words (1954830 effective words) took 9.0s, 218411 effective words/s\n",
      "2020-03-12 18:42:28,759 : INFO : EPOCH 4 - PROGRESS: at 8.69% examples, 164753 words/s, in_qsize 8, out_qsize 1\n",
      "2020-03-12 18:42:29,786 : INFO : EPOCH 4 - PROGRESS: at 18.65% examples, 174805 words/s, in_qsize 8, out_qsize 2\n",
      "2020-03-12 18:42:30,787 : INFO : EPOCH 4 - PROGRESS: at 28.06% examples, 179276 words/s, in_qsize 8, out_qsize 0\n",
      "2020-03-12 18:42:31,795 : INFO : EPOCH 4 - PROGRESS: at 37.74% examples, 180820 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-12 18:42:32,810 : INFO : EPOCH 4 - PROGRESS: at 46.62% examples, 181736 words/s, in_qsize 8, out_qsize 0\n",
      "2020-03-12 18:42:33,830 : INFO : EPOCH 4 - PROGRESS: at 56.91% examples, 182216 words/s, in_qsize 8, out_qsize 1\n",
      "2020-03-12 18:42:34,870 : INFO : EPOCH 4 - PROGRESS: at 66.57% examples, 181108 words/s, in_qsize 8, out_qsize 0\n",
      "2020-03-12 18:42:35,889 : INFO : EPOCH 4 - PROGRESS: at 75.58% examples, 180622 words/s, in_qsize 8, out_qsize 0\n",
      "2020-03-12 18:42:36,914 : INFO : EPOCH 4 - PROGRESS: at 84.56% examples, 179462 words/s, in_qsize 8, out_qsize 0\n",
      "2020-03-12 18:42:37,958 : INFO : EPOCH 4 - PROGRESS: at 92.91% examples, 178039 words/s, in_qsize 8, out_qsize 0\n",
      "2020-03-12 18:42:38,659 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-03-12 18:42:38,682 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-03-12 18:42:38,702 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-03-12 18:42:38,722 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-03-12 18:42:38,723 : INFO : EPOCH - 4 : training on 2656274 raw words (1955503 effective words) took 11.0s, 178245 effective words/s\n",
      "2020-03-12 18:42:39,792 : INFO : EPOCH 5 - PROGRESS: at 12.37% examples, 223948 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-12 18:42:40,860 : INFO : EPOCH 5 - PROGRESS: at 25.85% examples, 234431 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-12 18:42:41,882 : INFO : EPOCH 5 - PROGRESS: at 38.81% examples, 238572 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-12 18:42:42,903 : INFO : EPOCH 5 - PROGRESS: at 50.62% examples, 237280 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-12 18:42:43,926 : INFO : EPOCH 5 - PROGRESS: at 63.08% examples, 235275 words/s, in_qsize 8, out_qsize 0\n",
      "2020-03-12 18:42:44,953 : INFO : EPOCH 5 - PROGRESS: at 72.61% examples, 225569 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-12 18:42:45,984 : INFO : EPOCH 5 - PROGRESS: at 85.16% examples, 228544 words/s, in_qsize 8, out_qsize 0\n",
      "2020-03-12 18:42:47,033 : INFO : EPOCH 5 - PROGRESS: at 94.61% examples, 223005 words/s, in_qsize 8, out_qsize 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-03-12 18:42:47,472 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-03-12 18:42:47,501 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-03-12 18:42:47,504 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-03-12 18:42:47,542 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-03-12 18:42:47,543 : INFO : EPOCH - 5 : training on 2656274 raw words (1955442 effective words) took 8.8s, 221779 effective words/s\n",
      "2020-03-12 18:42:47,543 : INFO : training on a 13281370 raw words (9776255 effective words) took 49.6s, 196927 effective words/s\n",
      "2020-03-12 18:42:47,548 : INFO : precomputing L2-norms of word weight vectors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most similar to: silicon\n",
      "[('carbide', 0.8683127164840698), ('SiC', 0.8661246299743652), ('LEDs', 0.858808159828186), ('wafers', 0.8447800874710083), ('GaN', 0.8418112397193909), ('wafer', 0.826755702495575), ('nitride', 0.8264325857162476), ('conductive', 0.8259358406066895), ('ultrathin', 0.8248092532157898), ('nanocomposite', 0.8241363763809204)]\n",
      "\n",
      "Most similar to: flux\n",
      "[('fluxes', 0.8358415961265564), ('momentum', 0.8162760138511658), ('transports', 0.7871787548065186), ('O2', 0.7836939096450806), ('haze', 0.7819864749908447), ('vorticity', 0.7814186215400696), ('mesosphere', 0.7802945375442505), ('diapycnal', 0.7741315364837646), ('outflow', 0.7718337178230286), ('asthenosphere', 0.77115797996521)]\n",
      "\n",
      "Most similar to: stratosphere\n",
      "[('mesosphere', 0.9094099998474121), ('thermosphere', 0.8826446533203125), ('troposphere', 0.8724294900894165), ('H2O', 0.8540778160095215), ('circulating', 0.8538209199905396), ('diapycnal', 0.853712260723114), ('remineralization', 0.8531094789505005), ('asthenosphere', 0.8517158031463623), ('mineralogy', 0.8511455059051514), ('thermocline', 0.849853515625)]\n",
      "\n",
      "Most similar to: music\n",
      "[('vocabulary', 0.8750628232955933), ('accessing', 0.8746378421783447), ('videos', 0.8694682121276855), ('relational', 0.8658239245414734), ('lexical', 0.865169882774353), ('textual', 0.8605393767356873), ('hiding', 0.8579472303390503), ('categorization', 0.8523973822593689), ('aggregating', 0.8523606657981873), ('graphic', 0.8521576523780823)]\n",
      "\n",
      "Most similar to: pitch\n",
      "[('variant', 0.9017078876495361), ('tunability', 0.8993591666221619), ('thermodynamically', 0.8952896595001221), ('loose', 0.8936942219734192), ('faint', 0.893287181854248), ('supersonic', 0.8931238651275635), ('parity', 0.891822099685669), ('lock', 0.8908010721206665), ('intensities', 0.8906475305557251), ('coarser', 0.8900716304779053)]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "vectors = gensim.models.Word2Vec(tokenized_text, size=1000, window=3, min_count=10, sg=1, workers=4)\n",
    "\n",
    "print(\"Most similar to:\", 'silicon')\n",
    "print(vectors.wv.most_similar('silicon'))\n",
    "print()\n",
    "\n",
    "print(\"Most similar to:\", 'flux')\n",
    "print(vectors.wv.most_similar('flux'))\n",
    "print()\n",
    "\n",
    "print(\"Most similar to:\", 'stratosphere')\n",
    "print(vectors.wv.most_similar('stratosphere'))\n",
    "print()\n",
    "\n",
    "print(\"Most similar to:\", 'music')\n",
    "print(vectors.wv.most_similar('music'))\n",
    "print()\n",
    "\n",
    "print(\"Most similar to:\", 'pitch')\n",
    "print(vectors.wv.most_similar('pitch'))\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I like this setup mostly because of the stricter boundaries and increase dimensionality (somehow). But this setup makes the most sense when it comes to expectation. I would want to see these information appears together."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Move on to the whole dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 12.9 s, sys: 2.01 s, total: 14.9 s\n",
      "Wall time: 14.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "documents_full = [read_file(file) for file in get_fnames('../abstracts/')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_vectorizer = TfidfVectorizer()\n",
    "# new_vectorizer = TfidfVectorizer()\n",
    "full_word_tokenizer = full_vectorizer.build_tokenizer()\n",
    "full_tokenized_text = [full_word_tokenizer(doc) for doc in documents_full]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-03-12 18:55:40,081 : INFO : collecting all words and their counts\n",
      "2020-03-12 18:55:40,082 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2020-03-12 18:55:40,302 : INFO : PROGRESS: at sentence #10000, processed 1535501 words, keeping 46762 word types\n",
      "2020-03-12 18:55:40,533 : INFO : PROGRESS: at sentence #20000, processed 3047602 words, keeping 65659 word types\n",
      "2020-03-12 18:55:40,842 : INFO : PROGRESS: at sentence #30000, processed 4977757 words, keeping 93131 word types\n",
      "2020-03-12 18:55:41,218 : INFO : PROGRESS: at sentence #40000, processed 7438658 words, keeping 117620 word types\n",
      "2020-03-12 18:55:41,623 : INFO : PROGRESS: at sentence #50000, processed 9998121 words, keeping 137906 word types\n",
      "2020-03-12 18:55:41,899 : INFO : PROGRESS: at sentence #60000, processed 11734386 words, keeping 148537 word types\n",
      "2020-03-12 18:55:42,215 : INFO : PROGRESS: at sentence #70000, processed 13706872 words, keeping 165794 word types\n",
      "2020-03-12 18:55:42,654 : INFO : PROGRESS: at sentence #80000, processed 16324925 words, keeping 181537 word types\n",
      "2020-03-12 18:55:43,006 : INFO : PROGRESS: at sentence #90000, processed 18558083 words, keeping 197440 word types\n",
      "2020-03-12 18:55:43,324 : INFO : PROGRESS: at sentence #100000, processed 20492045 words, keeping 213201 word types\n",
      "2020-03-12 18:55:43,611 : INFO : PROGRESS: at sentence #110000, processed 22310144 words, keeping 228406 word types\n",
      "2020-03-12 18:55:43,936 : INFO : PROGRESS: at sentence #120000, processed 24391533 words, keeping 241845 word types\n",
      "2020-03-12 18:55:44,314 : INFO : PROGRESS: at sentence #130000, processed 26777045 words, keeping 254626 word types\n",
      "2020-03-12 18:55:44,412 : INFO : collected 257022 word types from a corpus of 27374377 raw words and 132372 sentences\n",
      "2020-03-12 18:55:44,413 : INFO : Loading a fresh vocabulary\n",
      "2020-03-12 18:55:44,594 : INFO : effective_min_count=10 retains 45616 unique words (17% of original 257022, drops 211406)\n",
      "2020-03-12 18:55:44,595 : INFO : effective_min_count=10 leaves 26930680 word corpus (98% of original 27374377, drops 443697)\n",
      "2020-03-12 18:55:44,720 : INFO : deleting the raw counts dictionary of 257022 items\n",
      "2020-03-12 18:55:44,728 : INFO : sample=0.001 downsamples 25 most-common words\n",
      "2020-03-12 18:55:44,729 : INFO : downsampling leaves estimated 20810152 word corpus (77.3% of prior 26930680)\n",
      "2020-03-12 18:55:44,847 : INFO : estimated required memory for 45616 words and 1000 dimensions: 387736000 bytes\n",
      "2020-03-12 18:55:44,848 : INFO : resetting layer weights\n",
      "2020-03-12 18:55:52,582 : INFO : training model with 4 workers on 45616 vocabulary and 1000 features, using sg=1 hs=0 sample=0.001 negative=5 window=3\n",
      "2020-03-12 18:55:53,977 : INFO : EPOCH 1 - PROGRESS: at 0.04% examples, 5357 words/s, in_qsize 6, out_qsize 2\n",
      "2020-03-12 18:55:55,018 : INFO : EPOCH 1 - PROGRESS: at 1.37% examples, 89102 words/s, in_qsize 8, out_qsize 0\n",
      "2020-03-12 18:55:56,137 : INFO : EPOCH 1 - PROGRESS: at 2.57% examples, 115899 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-12 18:55:57,228 : INFO : EPOCH 1 - PROGRESS: at 3.81% examples, 131972 words/s, in_qsize 8, out_qsize 1\n",
      "2020-03-12 18:55:58,279 : INFO : EPOCH 1 - PROGRESS: at 5.05% examples, 142854 words/s, in_qsize 8, out_qsize 0\n",
      "2020-03-12 18:55:59,324 : INFO : EPOCH 1 - PROGRESS: at 6.24% examples, 149423 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-12 18:56:00,386 : INFO : EPOCH 1 - PROGRESS: at 7.90% examples, 155336 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-12 18:56:01,387 : INFO : EPOCH 1 - PROGRESS: at 9.14% examples, 157942 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-12 18:56:02,446 : INFO : EPOCH 1 - PROGRESS: at 10.20% examples, 159249 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-12 18:56:03,518 : INFO : EPOCH 1 - PROGRESS: at 11.32% examples, 160142 words/s, in_qsize 8, out_qsize 0\n",
      "2020-03-12 18:56:04,634 : INFO : EPOCH 1 - PROGRESS: at 12.47% examples, 160112 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-12 18:56:05,778 : INFO : EPOCH 1 - PROGRESS: at 13.52% examples, 159773 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-12 18:56:06,788 : INFO : EPOCH 1 - PROGRESS: at 14.90% examples, 160693 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-12 18:56:07,839 : INFO : EPOCH 1 - PROGRESS: at 16.16% examples, 161511 words/s, in_qsize 8, out_qsize 0\n",
      "2020-03-12 18:56:08,855 : INFO : EPOCH 1 - PROGRESS: at 16.99% examples, 161972 words/s, in_qsize 8, out_qsize 0\n",
      "2020-03-12 18:56:09,860 : INFO : EPOCH 1 - PROGRESS: at 17.82% examples, 163665 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-12 18:56:10,954 : INFO : EPOCH 1 - PROGRESS: at 18.59% examples, 163254 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-12 18:56:12,071 : INFO : EPOCH 1 - PROGRESS: at 19.32% examples, 162964 words/s, in_qsize 8, out_qsize 1\n",
      "2020-03-12 18:56:13,083 : INFO : EPOCH 1 - PROGRESS: at 20.26% examples, 163612 words/s, in_qsize 8, out_qsize 0\n",
      "2020-03-12 18:56:14,134 : INFO : EPOCH 1 - PROGRESS: at 21.08% examples, 163571 words/s, in_qsize 8, out_qsize 0\n",
      "2020-03-12 18:56:15,154 : INFO : EPOCH 1 - PROGRESS: at 22.15% examples, 163891 words/s, in_qsize 8, out_qsize 0\n",
      "2020-03-12 18:56:16,191 : INFO : EPOCH 1 - PROGRESS: at 23.13% examples, 163798 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-12 18:56:17,213 : INFO : EPOCH 1 - PROGRESS: at 23.83% examples, 163375 words/s, in_qsize 8, out_qsize 1\n",
      "2020-03-12 18:56:18,215 : INFO : EPOCH 1 - PROGRESS: at 24.65% examples, 163704 words/s, in_qsize 8, out_qsize 0\n",
      "2020-03-12 18:56:19,288 : INFO : EPOCH 1 - PROGRESS: at 25.30% examples, 163068 words/s, in_qsize 8, out_qsize 0\n",
      "2020-03-12 18:56:20,388 : INFO : EPOCH 1 - PROGRESS: at 25.98% examples, 163107 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-12 18:56:21,413 : INFO : EPOCH 1 - PROGRESS: at 26.62% examples, 163300 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-12 18:56:22,443 : INFO : EPOCH 1 - PROGRESS: at 27.30% examples, 163461 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-12 18:56:23,448 : INFO : EPOCH 1 - PROGRESS: at 27.98% examples, 163990 words/s, in_qsize 8, out_qsize 0\n",
      "2020-03-12 18:56:24,489 : INFO : EPOCH 1 - PROGRESS: at 28.69% examples, 164353 words/s, in_qsize 6, out_qsize 1\n",
      "2020-03-12 18:56:25,503 : INFO : EPOCH 1 - PROGRESS: at 29.53% examples, 166154 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-12 18:56:26,523 : INFO : EPOCH 1 - PROGRESS: at 30.42% examples, 167820 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-12 18:56:27,532 : INFO : EPOCH 1 - PROGRESS: at 31.33% examples, 169226 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-12 18:56:28,572 : INFO : EPOCH 1 - PROGRESS: at 32.23% examples, 170849 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-12 18:56:29,592 : INFO : EPOCH 1 - PROGRESS: at 33.06% examples, 172029 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-12 18:56:30,601 : INFO : EPOCH 1 - PROGRESS: at 33.77% examples, 172788 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-12 18:56:31,649 : INFO : EPOCH 1 - PROGRESS: at 34.44% examples, 172760 words/s, in_qsize 8, out_qsize 0\n",
      "2020-03-12 18:56:32,659 : INFO : EPOCH 1 - PROGRESS: at 35.18% examples, 172752 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-12 18:56:33,690 : INFO : EPOCH 1 - PROGRESS: at 35.85% examples, 172636 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-12 18:56:34,725 : INFO : EPOCH 1 - PROGRESS: at 36.52% examples, 172334 words/s, in_qsize 8, out_qsize 0\n",
      "2020-03-12 18:56:35,737 : INFO : EPOCH 1 - PROGRESS: at 37.14% examples, 172316 words/s, in_qsize 8, out_qsize 0\n",
      "2020-03-12 18:56:36,753 : INFO : EPOCH 1 - PROGRESS: at 37.81% examples, 172274 words/s, in_qsize 8, out_qsize 0\n",
      "2020-03-12 18:56:37,800 : INFO : EPOCH 1 - PROGRESS: at 38.54% examples, 172296 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-12 18:56:38,870 : INFO : EPOCH 1 - PROGRESS: at 39.68% examples, 172195 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-12 18:56:39,880 : INFO : EPOCH 1 - PROGRESS: at 40.66% examples, 172152 words/s, in_qsize 8, out_qsize 0\n",
      "2020-03-12 18:56:40,966 : INFO : EPOCH 1 - PROGRESS: at 41.74% examples, 172011 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-12 18:56:42,050 : INFO : EPOCH 1 - PROGRESS: at 42.84% examples, 171855 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-12 18:56:43,069 : INFO : EPOCH 1 - PROGRESS: at 43.85% examples, 171923 words/s, in_qsize 8, out_qsize 0\n",
      "2020-03-12 18:56:44,070 : INFO : EPOCH 1 - PROGRESS: at 44.99% examples, 172247 words/s, in_qsize 8, out_qsize 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-03-12 18:56:45,138 : INFO : EPOCH 1 - PROGRESS: at 46.48% examples, 172085 words/s, in_qsize 8, out_qsize 2\n",
      "2020-03-12 18:56:46,179 : INFO : EPOCH 1 - PROGRESS: at 47.32% examples, 172221 words/s, in_qsize 6, out_qsize 1\n",
      "2020-03-12 18:56:47,192 : INFO : EPOCH 1 - PROGRESS: at 48.10% examples, 172265 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-12 18:56:48,214 : INFO : EPOCH 1 - PROGRESS: at 48.93% examples, 172183 words/s, in_qsize 8, out_qsize 0\n",
      "2020-03-12 18:56:49,219 : INFO : EPOCH 1 - PROGRESS: at 49.97% examples, 172434 words/s, in_qsize 8, out_qsize 0\n",
      "2020-03-12 18:56:50,233 : INFO : EPOCH 1 - PROGRESS: at 50.71% examples, 172386 words/s, in_qsize 8, out_qsize 0\n",
      "2020-03-12 18:56:51,244 : INFO : EPOCH 1 - PROGRESS: at 51.44% examples, 172106 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-12 18:56:52,257 : INFO : EPOCH 1 - PROGRESS: at 52.09% examples, 171817 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-12 18:56:53,290 : INFO : EPOCH 1 - PROGRESS: at 52.88% examples, 171498 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-12 18:56:54,344 : INFO : EPOCH 1 - PROGRESS: at 53.65% examples, 171414 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-12 18:56:55,345 : INFO : EPOCH 1 - PROGRESS: at 54.39% examples, 171554 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-12 18:56:56,397 : INFO : EPOCH 1 - PROGRESS: at 55.06% examples, 171575 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-12 18:56:57,476 : INFO : EPOCH 1 - PROGRESS: at 55.73% examples, 171505 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-12 18:56:58,481 : INFO : EPOCH 1 - PROGRESS: at 56.34% examples, 171624 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-12 18:56:59,495 : INFO : EPOCH 1 - PROGRESS: at 56.96% examples, 171511 words/s, in_qsize 8, out_qsize 0\n",
      "2020-03-12 18:57:00,519 : INFO : EPOCH 1 - PROGRESS: at 57.49% examples, 171351 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-12 18:57:01,548 : INFO : EPOCH 1 - PROGRESS: at 58.07% examples, 171100 words/s, in_qsize 8, out_qsize 0\n",
      "2020-03-12 18:57:02,569 : INFO : EPOCH 1 - PROGRESS: at 58.70% examples, 170971 words/s, in_qsize 8, out_qsize 0\n",
      "2020-03-12 18:57:03,597 : INFO : EPOCH 1 - PROGRESS: at 59.34% examples, 170939 words/s, in_qsize 8, out_qsize 0\n",
      "2020-03-12 18:57:04,644 : INFO : EPOCH 1 - PROGRESS: at 60.06% examples, 170973 words/s, in_qsize 8, out_qsize 0\n",
      "2020-03-12 18:57:05,659 : INFO : EPOCH 1 - PROGRESS: at 60.67% examples, 170960 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-12 18:57:06,676 : INFO : EPOCH 1 - PROGRESS: at 61.35% examples, 170961 words/s, in_qsize 8, out_qsize 1\n",
      "2020-03-12 18:57:07,731 : INFO : EPOCH 1 - PROGRESS: at 62.07% examples, 170851 words/s, in_qsize 8, out_qsize 0\n",
      "2020-03-12 18:57:08,741 : INFO : EPOCH 1 - PROGRESS: at 62.73% examples, 170746 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-12 18:57:09,781 : INFO : EPOCH 1 - PROGRESS: at 63.44% examples, 170680 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-12 18:57:10,784 : INFO : EPOCH 1 - PROGRESS: at 64.43% examples, 170796 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-12 18:57:11,842 : INFO : EPOCH 1 - PROGRESS: at 65.26% examples, 170597 words/s, in_qsize 8, out_qsize 0\n",
      "2020-03-12 18:57:12,947 : INFO : EPOCH 1 - PROGRESS: at 66.05% examples, 170466 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-12 18:57:13,948 : INFO : EPOCH 1 - PROGRESS: at 66.85% examples, 170383 words/s, in_qsize 8, out_qsize 0\n",
      "2020-03-12 18:57:14,954 : INFO : EPOCH 1 - PROGRESS: at 67.58% examples, 170298 words/s, in_qsize 8, out_qsize 0\n",
      "2020-03-12 18:57:15,964 : INFO : EPOCH 1 - PROGRESS: at 68.39% examples, 170210 words/s, in_qsize 8, out_qsize 1\n",
      "2020-03-12 18:57:17,074 : INFO : EPOCH 1 - PROGRESS: at 69.14% examples, 170045 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-12 18:57:18,114 : INFO : EPOCH 1 - PROGRESS: at 69.98% examples, 169925 words/s, in_qsize 8, out_qsize 1\n",
      "2020-03-12 18:57:19,119 : INFO : EPOCH 1 - PROGRESS: at 70.94% examples, 169937 words/s, in_qsize 8, out_qsize 0\n",
      "2020-03-12 18:57:20,181 : INFO : EPOCH 1 - PROGRESS: at 71.71% examples, 169666 words/s, in_qsize 8, out_qsize 0\n",
      "2020-03-12 18:57:21,272 : INFO : EPOCH 1 - PROGRESS: at 72.68% examples, 169594 words/s, in_qsize 8, out_qsize 0\n",
      "2020-03-12 18:57:22,294 : INFO : EPOCH 1 - PROGRESS: at 73.64% examples, 169546 words/s, in_qsize 8, out_qsize 0\n",
      "2020-03-12 18:57:23,324 : INFO : EPOCH 1 - PROGRESS: at 74.45% examples, 169508 words/s, in_qsize 8, out_qsize 0\n",
      "2020-03-12 18:57:24,332 : INFO : EPOCH 1 - PROGRESS: at 75.48% examples, 169673 words/s, in_qsize 8, out_qsize 0\n",
      "2020-03-12 18:57:25,364 : INFO : EPOCH 1 - PROGRESS: at 76.73% examples, 169661 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-12 18:57:26,423 : INFO : EPOCH 1 - PROGRESS: at 77.76% examples, 169664 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-12 18:57:27,433 : INFO : EPOCH 1 - PROGRESS: at 78.61% examples, 169506 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-12 18:57:28,533 : INFO : EPOCH 1 - PROGRESS: at 79.50% examples, 169421 words/s, in_qsize 7, out_qsize 1\n",
      "2020-03-12 18:57:29,541 : INFO : EPOCH 1 - PROGRESS: at 80.52% examples, 169504 words/s, in_qsize 7, out_qsize 1\n",
      "2020-03-12 18:57:30,553 : INFO : EPOCH 1 - PROGRESS: at 81.46% examples, 169566 words/s, in_qsize 8, out_qsize 0\n",
      "2020-03-12 18:57:31,575 : INFO : EPOCH 1 - PROGRESS: at 82.19% examples, 169464 words/s, in_qsize 8, out_qsize 0\n",
      "2020-03-12 18:57:32,595 : INFO : EPOCH 1 - PROGRESS: at 83.16% examples, 169606 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-12 18:57:33,644 : INFO : EPOCH 1 - PROGRESS: at 84.33% examples, 169875 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-12 18:57:34,653 : INFO : EPOCH 1 - PROGRESS: at 85.14% examples, 169828 words/s, in_qsize 7, out_qsize 1\n",
      "2020-03-12 18:57:35,672 : INFO : EPOCH 1 - PROGRESS: at 86.00% examples, 170040 words/s, in_qsize 8, out_qsize 0\n",
      "2020-03-12 18:57:36,699 : INFO : EPOCH 1 - PROGRESS: at 86.73% examples, 170018 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-12 18:57:37,731 : INFO : EPOCH 1 - PROGRESS: at 87.52% examples, 170192 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-12 18:57:38,733 : INFO : EPOCH 1 - PROGRESS: at 88.59% examples, 170351 words/s, in_qsize 8, out_qsize 0\n",
      "2020-03-12 18:57:39,735 : INFO : EPOCH 1 - PROGRESS: at 89.46% examples, 170630 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-12 18:57:40,743 : INFO : EPOCH 1 - PROGRESS: at 90.46% examples, 170992 words/s, in_qsize 8, out_qsize 0\n",
      "2020-03-12 18:57:41,757 : INFO : EPOCH 1 - PROGRESS: at 91.35% examples, 171348 words/s, in_qsize 8, out_qsize 0\n",
      "2020-03-12 18:57:42,783 : INFO : EPOCH 1 - PROGRESS: at 92.20% examples, 171728 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-12 18:57:43,799 : INFO : EPOCH 1 - PROGRESS: at 93.09% examples, 171857 words/s, in_qsize 8, out_qsize 1\n",
      "2020-03-12 18:57:44,867 : INFO : EPOCH 1 - PROGRESS: at 93.76% examples, 171746 words/s, in_qsize 6, out_qsize 1\n",
      "2020-03-12 18:57:45,891 : INFO : EPOCH 1 - PROGRESS: at 94.74% examples, 172031 words/s, in_qsize 8, out_qsize 0\n",
      "2020-03-12 18:57:46,949 : INFO : EPOCH 1 - PROGRESS: at 95.59% examples, 172020 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-12 18:57:47,953 : INFO : EPOCH 1 - PROGRESS: at 96.30% examples, 172045 words/s, in_qsize 8, out_qsize 0\n",
      "2020-03-12 18:57:49,022 : INFO : EPOCH 1 - PROGRESS: at 97.01% examples, 172148 words/s, in_qsize 8, out_qsize 0\n",
      "2020-03-12 18:57:50,064 : INFO : EPOCH 1 - PROGRESS: at 97.78% examples, 172347 words/s, in_qsize 8, out_qsize 0\n",
      "2020-03-12 18:57:51,102 : INFO : EPOCH 1 - PROGRESS: at 98.43% examples, 172298 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-12 18:57:52,137 : INFO : EPOCH 1 - PROGRESS: at 99.07% examples, 172256 words/s, in_qsize 8, out_qsize 1\n",
      "2020-03-12 18:57:53,247 : INFO : EPOCH 1 - PROGRESS: at 99.84% examples, 172175 words/s, in_qsize 6, out_qsize 0\n",
      "2020-03-12 18:57:53,315 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-03-12 18:57:53,350 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-03-12 18:57:53,400 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-03-12 18:57:53,429 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-03-12 18:57:53,430 : INFO : EPOCH - 1 : training on 27374377 raw words (20811099 effective words) took 120.8s, 172235 effective words/s\n",
      "2020-03-12 18:57:54,455 : INFO : EPOCH 2 - PROGRESS: at 1.28% examples, 197074 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-12 18:57:55,490 : INFO : EPOCH 2 - PROGRESS: at 2.39% examples, 185171 words/s, in_qsize 7, out_qsize 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-03-12 18:57:56,491 : INFO : EPOCH 2 - PROGRESS: at 3.33% examples, 175759 words/s, in_qsize 8, out_qsize 0\n",
      "2020-03-12 18:57:57,519 : INFO : EPOCH 2 - PROGRESS: at 4.46% examples, 175054 words/s, in_qsize 7, out_qsize 1\n",
      "2020-03-12 18:57:58,545 : INFO : EPOCH 2 - PROGRESS: at 5.58% examples, 176234 words/s, in_qsize 8, out_qsize 0\n",
      "2020-03-12 18:57:59,561 : INFO : EPOCH 2 - PROGRESS: at 6.67% examples, 174047 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-12 18:58:00,590 : INFO : EPOCH 2 - PROGRESS: at 8.10% examples, 173224 words/s, in_qsize 8, out_qsize 0\n",
      "2020-03-12 18:58:01,704 : INFO : EPOCH 2 - PROGRESS: at 9.36% examples, 172427 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-12 18:58:02,725 : INFO : EPOCH 2 - PROGRESS: at 10.44% examples, 172919 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-12 18:58:03,807 : INFO : EPOCH 2 - PROGRESS: at 11.61% examples, 172930 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-12 18:58:04,809 : INFO : EPOCH 2 - PROGRESS: at 12.78% examples, 174042 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-12 18:58:05,819 : INFO : EPOCH 2 - PROGRESS: at 13.82% examples, 173676 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-12 18:58:06,864 : INFO : EPOCH 2 - PROGRESS: at 15.43% examples, 173779 words/s, in_qsize 8, out_qsize 0\n",
      "2020-03-12 18:58:07,904 : INFO : EPOCH 2 - PROGRESS: at 16.40% examples, 173747 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-12 18:58:08,933 : INFO : EPOCH 2 - PROGRESS: at 17.24% examples, 173727 words/s, in_qsize 8, out_qsize 0\n",
      "2020-03-12 18:58:09,967 : INFO : EPOCH 2 - PROGRESS: at 18.05% examples, 173609 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-12 18:58:11,003 : INFO : EPOCH 2 - PROGRESS: at 18.79% examples, 173899 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-12 18:58:12,033 : INFO : EPOCH 2 - PROGRESS: at 19.71% examples, 174622 words/s, in_qsize 8, out_qsize 0\n",
      "2020-03-12 18:58:13,037 : INFO : EPOCH 2 - PROGRESS: at 20.63% examples, 174793 words/s, in_qsize 8, out_qsize 0\n",
      "2020-03-12 18:58:14,080 : INFO : EPOCH 2 - PROGRESS: at 21.44% examples, 173892 words/s, in_qsize 8, out_qsize 0\n",
      "2020-03-12 18:58:15,104 : INFO : EPOCH 2 - PROGRESS: at 22.64% examples, 173407 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-12 18:58:16,108 : INFO : EPOCH 2 - PROGRESS: at 23.54% examples, 174409 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-12 18:58:17,153 : INFO : EPOCH 2 - PROGRESS: at 24.36% examples, 174308 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-12 18:58:18,167 : INFO : EPOCH 2 - PROGRESS: at 25.14% examples, 174470 words/s, in_qsize 8, out_qsize 0\n",
      "2020-03-12 18:58:19,218 : INFO : EPOCH 2 - PROGRESS: at 25.77% examples, 173768 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-12 18:58:20,246 : INFO : EPOCH 2 - PROGRESS: at 26.40% examples, 173265 words/s, in_qsize 8, out_qsize 0\n",
      "2020-03-12 18:58:21,269 : INFO : EPOCH 2 - PROGRESS: at 27.04% examples, 173082 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-12 18:58:22,298 : INFO : EPOCH 2 - PROGRESS: at 27.72% examples, 172668 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-12 18:58:23,307 : INFO : EPOCH 2 - PROGRESS: at 28.46% examples, 173398 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-12 18:58:24,360 : INFO : EPOCH 2 - PROGRESS: at 29.35% examples, 175065 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-12 18:58:25,399 : INFO : EPOCH 2 - PROGRESS: at 30.07% examples, 175267 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-12 18:58:26,415 : INFO : EPOCH 2 - PROGRESS: at 30.95% examples, 176279 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-12 18:58:27,448 : INFO : EPOCH 2 - PROGRESS: at 31.80% examples, 177358 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-12 18:58:28,459 : INFO : EPOCH 2 - PROGRESS: at 32.58% examples, 178048 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-12 18:58:29,465 : INFO : EPOCH 2 - PROGRESS: at 33.22% examples, 177877 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-12 18:58:30,540 : INFO : EPOCH 2 - PROGRESS: at 33.77% examples, 176967 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-12 18:58:31,641 : INFO : EPOCH 2 - PROGRESS: at 34.44% examples, 176584 words/s, in_qsize 8, out_qsize 0\n",
      "2020-03-12 18:58:32,684 : INFO : EPOCH 2 - PROGRESS: at 35.15% examples, 176132 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-12 18:58:33,718 : INFO : EPOCH 2 - PROGRESS: at 35.81% examples, 175916 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-12 18:58:34,765 : INFO : EPOCH 2 - PROGRESS: at 36.48% examples, 175297 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-12 18:58:35,798 : INFO : EPOCH 2 - PROGRESS: at 37.05% examples, 174948 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-12 18:58:36,804 : INFO : EPOCH 2 - PROGRESS: at 37.70% examples, 174878 words/s, in_qsize 8, out_qsize 0\n",
      "2020-03-12 18:58:37,841 : INFO : EPOCH 2 - PROGRESS: at 38.43% examples, 174714 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-12 18:58:38,884 : INFO : EPOCH 2 - PROGRESS: at 39.52% examples, 174823 words/s, in_qsize 8, out_qsize 0\n",
      "2020-03-12 18:58:39,888 : INFO : EPOCH 2 - PROGRESS: at 40.53% examples, 174747 words/s, in_qsize 8, out_qsize 0\n",
      "2020-03-12 18:58:40,927 : INFO : EPOCH 2 - PROGRESS: at 41.57% examples, 174556 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-12 18:58:41,970 : INFO : EPOCH 2 - PROGRESS: at 42.56% examples, 174183 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-12 18:58:42,989 : INFO : EPOCH 2 - PROGRESS: at 43.52% examples, 173899 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-12 18:58:44,000 : INFO : EPOCH 2 - PROGRESS: at 44.72% examples, 174582 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-12 18:58:45,007 : INFO : EPOCH 2 - PROGRESS: at 46.58% examples, 175615 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-12 18:58:46,030 : INFO : EPOCH 2 - PROGRESS: at 47.54% examples, 176276 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-12 18:58:47,033 : INFO : EPOCH 2 - PROGRESS: at 48.50% examples, 176872 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-12 18:58:48,038 : INFO : EPOCH 2 - PROGRESS: at 49.67% examples, 177717 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-12 18:58:49,114 : INFO : EPOCH 2 - PROGRESS: at 50.56% examples, 177773 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-12 18:58:50,151 : INFO : EPOCH 2 - PROGRESS: at 51.36% examples, 177700 words/s, in_qsize 8, out_qsize 0\n",
      "2020-03-12 18:58:51,151 : INFO : EPOCH 2 - PROGRESS: at 52.25% examples, 178250 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-12 18:58:52,160 : INFO : EPOCH 2 - PROGRESS: at 53.24% examples, 178543 words/s, in_qsize 8, out_qsize 0\n",
      "2020-03-12 18:58:53,231 : INFO : EPOCH 2 - PROGRESS: at 53.99% examples, 178385 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-12 18:58:54,241 : INFO : EPOCH 2 - PROGRESS: at 54.71% examples, 178400 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-12 18:58:55,268 : INFO : EPOCH 2 - PROGRESS: at 55.37% examples, 178363 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-12 18:58:56,273 : INFO : EPOCH 2 - PROGRESS: at 56.05% examples, 178500 words/s, in_qsize 8, out_qsize 0\n",
      "2020-03-12 18:58:57,295 : INFO : EPOCH 2 - PROGRESS: at 56.84% examples, 179074 words/s, in_qsize 8, out_qsize 1\n",
      "2020-03-12 18:58:58,316 : INFO : EPOCH 2 - PROGRESS: at 57.56% examples, 179720 words/s, in_qsize 6, out_qsize 1\n",
      "2020-03-12 18:58:59,333 : INFO : EPOCH 2 - PROGRESS: at 58.41% examples, 180376 words/s, in_qsize 8, out_qsize 0\n",
      "2020-03-12 18:59:00,358 : INFO : EPOCH 2 - PROGRESS: at 59.25% examples, 180988 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-12 18:59:01,388 : INFO : EPOCH 2 - PROGRESS: at 59.94% examples, 180704 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-12 18:59:02,397 : INFO : EPOCH 2 - PROGRESS: at 60.50% examples, 180341 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-12 18:59:03,489 : INFO : EPOCH 2 - PROGRESS: at 61.12% examples, 179903 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-12 18:59:04,506 : INFO : EPOCH 2 - PROGRESS: at 61.82% examples, 179859 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-12 18:59:05,632 : INFO : EPOCH 2 - PROGRESS: at 62.62% examples, 179652 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-12 18:59:06,656 : INFO : EPOCH 2 - PROGRESS: at 63.47% examples, 180007 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-12 18:59:07,670 : INFO : EPOCH 2 - PROGRESS: at 64.43% examples, 179878 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-12 18:59:08,683 : INFO : EPOCH 2 - PROGRESS: at 65.30% examples, 179749 words/s, in_qsize 8, out_qsize 0\n",
      "2020-03-12 18:59:09,705 : INFO : EPOCH 2 - PROGRESS: at 66.03% examples, 179483 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-12 18:59:10,749 : INFO : EPOCH 2 - PROGRESS: at 66.82% examples, 179181 words/s, in_qsize 8, out_qsize 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-03-12 18:59:11,795 : INFO : EPOCH 2 - PROGRESS: at 67.54% examples, 178887 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-12 18:59:12,872 : INFO : EPOCH 2 - PROGRESS: at 68.39% examples, 178622 words/s, in_qsize 8, out_qsize 0\n",
      "2020-03-12 18:59:13,911 : INFO : EPOCH 2 - PROGRESS: at 69.14% examples, 178491 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-12 18:59:14,967 : INFO : EPOCH 2 - PROGRESS: at 70.12% examples, 178494 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-12 18:59:15,994 : INFO : EPOCH 2 - PROGRESS: at 71.09% examples, 178445 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-12 18:59:17,019 : INFO : EPOCH 2 - PROGRESS: at 72.00% examples, 178400 words/s, in_qsize 8, out_qsize 0\n",
      "2020-03-12 18:59:18,053 : INFO : EPOCH 2 - PROGRESS: at 72.97% examples, 178325 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-12 18:59:19,075 : INFO : EPOCH 2 - PROGRESS: at 74.03% examples, 178520 words/s, in_qsize 8, out_qsize 0\n",
      "2020-03-12 18:59:20,125 : INFO : EPOCH 2 - PROGRESS: at 75.21% examples, 179023 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-12 18:59:21,131 : INFO : EPOCH 2 - PROGRESS: at 76.73% examples, 179458 words/s, in_qsize 8, out_qsize 0\n",
      "2020-03-12 18:59:22,148 : INFO : EPOCH 2 - PROGRESS: at 78.03% examples, 179936 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-12 18:59:23,166 : INFO : EPOCH 2 - PROGRESS: at 79.11% examples, 180295 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-12 18:59:24,246 : INFO : EPOCH 2 - PROGRESS: at 80.44% examples, 180775 words/s, in_qsize 8, out_qsize 0\n",
      "2020-03-12 18:59:25,308 : INFO : EPOCH 2 - PROGRESS: at 81.65% examples, 181265 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-12 18:59:26,332 : INFO : EPOCH 2 - PROGRESS: at 82.49% examples, 181188 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-12 18:59:27,334 : INFO : EPOCH 2 - PROGRESS: at 83.40% examples, 181088 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-12 18:59:28,396 : INFO : EPOCH 2 - PROGRESS: at 84.45% examples, 180988 words/s, in_qsize 8, out_qsize 0\n",
      "2020-03-12 18:59:29,409 : INFO : EPOCH 2 - PROGRESS: at 85.30% examples, 180960 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-12 18:59:30,419 : INFO : EPOCH 2 - PROGRESS: at 86.03% examples, 180775 words/s, in_qsize 8, out_qsize 0\n",
      "2020-03-12 18:59:31,461 : INFO : EPOCH 2 - PROGRESS: at 86.76% examples, 180610 words/s, in_qsize 8, out_qsize 0\n",
      "2020-03-12 18:59:32,507 : INFO : EPOCH 2 - PROGRESS: at 87.38% examples, 180282 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-12 18:59:33,516 : INFO : EPOCH 2 - PROGRESS: at 88.36% examples, 180192 words/s, in_qsize 8, out_qsize 0\n",
      "2020-03-12 18:59:34,528 : INFO : EPOCH 2 - PROGRESS: at 89.17% examples, 180075 words/s, in_qsize 8, out_qsize 0\n",
      "2020-03-12 18:59:35,558 : INFO : EPOCH 2 - PROGRESS: at 89.93% examples, 179951 words/s, in_qsize 8, out_qsize 0\n",
      "2020-03-12 18:59:36,599 : INFO : EPOCH 2 - PROGRESS: at 90.71% examples, 179760 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-12 18:59:37,643 : INFO : EPOCH 2 - PROGRESS: at 91.43% examples, 179690 words/s, in_qsize 8, out_qsize 0\n",
      "2020-03-12 18:59:38,713 : INFO : EPOCH 2 - PROGRESS: at 92.17% examples, 179646 words/s, in_qsize 8, out_qsize 0\n",
      "2020-03-12 18:59:39,781 : INFO : EPOCH 2 - PROGRESS: at 93.02% examples, 179546 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-12 18:59:40,805 : INFO : EPOCH 2 - PROGRESS: at 93.68% examples, 179427 words/s, in_qsize 8, out_qsize 0\n",
      "2020-03-12 18:59:41,812 : INFO : EPOCH 2 - PROGRESS: at 94.50% examples, 179343 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-12 18:59:42,855 : INFO : EPOCH 2 - PROGRESS: at 95.25% examples, 179063 words/s, in_qsize 8, out_qsize 0\n",
      "2020-03-12 18:59:43,863 : INFO : EPOCH 2 - PROGRESS: at 95.94% examples, 178884 words/s, in_qsize 8, out_qsize 1\n",
      "2020-03-12 18:59:44,918 : INFO : EPOCH 2 - PROGRESS: at 96.56% examples, 178685 words/s, in_qsize 8, out_qsize 0\n",
      "2020-03-12 18:59:46,002 : INFO : EPOCH 2 - PROGRESS: at 97.20% examples, 178499 words/s, in_qsize 8, out_qsize 0\n",
      "2020-03-12 18:59:47,028 : INFO : EPOCH 2 - PROGRESS: at 97.95% examples, 178609 words/s, in_qsize 8, out_qsize 0\n",
      "2020-03-12 18:59:48,045 : INFO : EPOCH 2 - PROGRESS: at 98.73% examples, 178860 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-12 18:59:49,158 : INFO : EPOCH 2 - PROGRESS: at 99.43% examples, 178642 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-12 18:59:49,805 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-03-12 18:59:49,905 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-03-12 18:59:49,921 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-03-12 18:59:49,931 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-03-12 18:59:49,931 : INFO : EPOCH - 2 : training on 27374377 raw words (20809399 effective words) took 116.5s, 178631 effective words/s\n",
      "2020-03-12 18:59:50,999 : INFO : EPOCH 3 - PROGRESS: at 1.03% examples, 147057 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-12 18:59:52,023 : INFO : EPOCH 3 - PROGRESS: at 2.15% examples, 164074 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-12 18:59:53,062 : INFO : EPOCH 3 - PROGRESS: at 3.23% examples, 166856 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-12 18:59:54,067 : INFO : EPOCH 3 - PROGRESS: at 4.60% examples, 178314 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-12 18:59:55,075 : INFO : EPOCH 3 - PROGRESS: at 5.67% examples, 178119 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-12 18:59:56,128 : INFO : EPOCH 3 - PROGRESS: at 6.99% examples, 177084 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-12 18:59:57,140 : INFO : EPOCH 3 - PROGRESS: at 8.37% examples, 177212 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-12 18:59:58,197 : INFO : EPOCH 3 - PROGRESS: at 9.53% examples, 176206 words/s, in_qsize 8, out_qsize 0\n",
      "2020-03-12 18:59:59,240 : INFO : EPOCH 3 - PROGRESS: at 10.55% examples, 174328 words/s, in_qsize 8, out_qsize 0\n",
      "2020-03-12 19:00:00,371 : INFO : EPOCH 3 - PROGRESS: at 11.66% examples, 172621 words/s, in_qsize 8, out_qsize 0\n",
      "2020-03-12 19:00:01,474 : INFO : EPOCH 3 - PROGRESS: at 12.78% examples, 171587 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-12 19:00:02,477 : INFO : EPOCH 3 - PROGRESS: at 13.86% examples, 172106 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-12 19:00:03,584 : INFO : EPOCH 3 - PROGRESS: at 15.54% examples, 172155 words/s, in_qsize 8, out_qsize 0\n",
      "2020-03-12 19:00:04,591 : INFO : EPOCH 3 - PROGRESS: at 16.48% examples, 172621 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-12 19:00:05,652 : INFO : EPOCH 3 - PROGRESS: at 17.34% examples, 172752 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-12 19:00:06,704 : INFO : EPOCH 3 - PROGRESS: at 18.16% examples, 172539 words/s, in_qsize 6, out_qsize 1\n",
      "2020-03-12 19:00:07,720 : INFO : EPOCH 3 - PROGRESS: at 18.88% examples, 173073 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-12 19:00:08,731 : INFO : EPOCH 3 - PROGRESS: at 19.71% examples, 172826 words/s, in_qsize 8, out_qsize 0\n",
      "2020-03-12 19:00:09,757 : INFO : EPOCH 3 - PROGRESS: at 20.59% examples, 172513 words/s, in_qsize 8, out_qsize 0\n",
      "2020-03-12 19:00:10,784 : INFO : EPOCH 3 - PROGRESS: at 21.67% examples, 172949 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-12 19:00:11,788 : INFO : EPOCH 3 - PROGRESS: at 22.74% examples, 173012 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-12 19:00:12,833 : INFO : EPOCH 3 - PROGRESS: at 23.64% examples, 173696 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-12 19:00:13,860 : INFO : EPOCH 3 - PROGRESS: at 24.68% examples, 175623 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-12 19:00:14,890 : INFO : EPOCH 3 - PROGRESS: at 25.56% examples, 177433 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-12 19:00:15,890 : INFO : EPOCH 3 - PROGRESS: at 26.43% examples, 179286 words/s, in_qsize 6, out_qsize 0\n",
      "2020-03-12 19:00:16,929 : INFO : EPOCH 3 - PROGRESS: at 27.14% examples, 179321 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-12 19:00:17,930 : INFO : EPOCH 3 - PROGRESS: at 27.85% examples, 179386 words/s, in_qsize 8, out_qsize 0\n",
      "2020-03-12 19:00:18,991 : INFO : EPOCH 3 - PROGRESS: at 28.58% examples, 179349 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-12 19:00:19,995 : INFO : EPOCH 3 - PROGRESS: at 29.42% examples, 180868 words/s, in_qsize 8, out_qsize 1\n",
      "2020-03-12 19:00:21,084 : INFO : EPOCH 3 - PROGRESS: at 30.30% examples, 181806 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-12 19:00:22,102 : INFO : EPOCH 3 - PROGRESS: at 31.20% examples, 182842 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-12 19:00:23,105 : INFO : EPOCH 3 - PROGRESS: at 32.06% examples, 183915 words/s, in_qsize 6, out_qsize 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-03-12 19:00:24,130 : INFO : EPOCH 3 - PROGRESS: at 32.93% examples, 185007 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-12 19:00:25,145 : INFO : EPOCH 3 - PROGRESS: at 33.72% examples, 186062 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-12 19:00:26,145 : INFO : EPOCH 3 - PROGRESS: at 34.41% examples, 186105 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-12 19:00:27,176 : INFO : EPOCH 3 - PROGRESS: at 35.18% examples, 185827 words/s, in_qsize 8, out_qsize 0\n",
      "2020-03-12 19:00:28,177 : INFO : EPOCH 3 - PROGRESS: at 35.98% examples, 186291 words/s, in_qsize 8, out_qsize 0\n",
      "2020-03-12 19:00:29,187 : INFO : EPOCH 3 - PROGRESS: at 36.86% examples, 187258 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-12 19:00:30,209 : INFO : EPOCH 3 - PROGRESS: at 37.68% examples, 188113 words/s, in_qsize 8, out_qsize 0\n",
      "2020-03-12 19:00:31,238 : INFO : EPOCH 3 - PROGRESS: at 38.59% examples, 188727 words/s, in_qsize 6, out_qsize 1\n",
      "2020-03-12 19:00:32,263 : INFO : EPOCH 3 - PROGRESS: at 39.77% examples, 188582 words/s, in_qsize 8, out_qsize 0\n",
      "2020-03-12 19:00:33,264 : INFO : EPOCH 3 - PROGRESS: at 40.76% examples, 188198 words/s, in_qsize 8, out_qsize 0\n",
      "2020-03-12 19:00:34,322 : INFO : EPOCH 3 - PROGRESS: at 41.70% examples, 187268 words/s, in_qsize 8, out_qsize 0\n",
      "2020-03-12 19:00:35,431 : INFO : EPOCH 3 - PROGRESS: at 42.79% examples, 186626 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-12 19:00:36,479 : INFO : EPOCH 3 - PROGRESS: at 43.93% examples, 186740 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-12 19:00:37,530 : INFO : EPOCH 3 - PROGRESS: at 45.05% examples, 186430 words/s, in_qsize 8, out_qsize 1\n",
      "2020-03-12 19:00:38,570 : INFO : EPOCH 3 - PROGRESS: at 46.54% examples, 186054 words/s, in_qsize 8, out_qsize 0\n",
      "2020-03-12 19:00:39,571 : INFO : EPOCH 3 - PROGRESS: at 47.26% examples, 185606 words/s, in_qsize 8, out_qsize 0\n",
      "2020-03-12 19:00:40,603 : INFO : EPOCH 3 - PROGRESS: at 48.06% examples, 185460 words/s, in_qsize 8, out_qsize 0\n",
      "2020-03-12 19:00:41,628 : INFO : EPOCH 3 - PROGRESS: at 48.93% examples, 185249 words/s, in_qsize 8, out_qsize 0\n",
      "2020-03-12 19:00:42,718 : INFO : EPOCH 3 - PROGRESS: at 49.89% examples, 184681 words/s, in_qsize 8, out_qsize 0\n",
      "2020-03-12 19:00:43,781 : INFO : EPOCH 3 - PROGRESS: at 50.65% examples, 184237 words/s, in_qsize 8, out_qsize 0\n",
      "2020-03-12 19:00:44,793 : INFO : EPOCH 3 - PROGRESS: at 51.36% examples, 183709 words/s, in_qsize 8, out_qsize 0\n",
      "2020-03-12 19:00:45,821 : INFO : EPOCH 3 - PROGRESS: at 52.06% examples, 183278 words/s, in_qsize 6, out_qsize 1\n",
      "2020-03-12 19:00:46,847 : INFO : EPOCH 3 - PROGRESS: at 52.88% examples, 182881 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-12 19:00:47,881 : INFO : EPOCH 3 - PROGRESS: at 53.65% examples, 182654 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-12 19:00:48,910 : INFO : EPOCH 3 - PROGRESS: at 54.31% examples, 182267 words/s, in_qsize 8, out_qsize 0\n",
      "2020-03-12 19:00:50,021 : INFO : EPOCH 3 - PROGRESS: at 55.03% examples, 182044 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-12 19:00:51,038 : INFO : EPOCH 3 - PROGRESS: at 55.70% examples, 181973 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-12 19:00:52,042 : INFO : EPOCH 3 - PROGRESS: at 56.32% examples, 181928 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-12 19:00:53,094 : INFO : EPOCH 3 - PROGRESS: at 56.91% examples, 181418 words/s, in_qsize 8, out_qsize 0\n",
      "2020-03-12 19:00:54,108 : INFO : EPOCH 3 - PROGRESS: at 57.43% examples, 181116 words/s, in_qsize 8, out_qsize 0\n",
      "2020-03-12 19:00:55,110 : INFO : EPOCH 3 - PROGRESS: at 58.04% examples, 180883 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-12 19:00:56,217 : INFO : EPOCH 3 - PROGRESS: at 58.67% examples, 180359 words/s, in_qsize 8, out_qsize 0\n",
      "2020-03-12 19:00:57,347 : INFO : EPOCH 3 - PROGRESS: at 59.34% examples, 180015 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-12 19:00:58,358 : INFO : EPOCH 3 - PROGRESS: at 60.03% examples, 179899 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-12 19:00:59,373 : INFO : EPOCH 3 - PROGRESS: at 60.60% examples, 179535 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-12 19:01:00,410 : INFO : EPOCH 3 - PROGRESS: at 61.23% examples, 179253 words/s, in_qsize 8, out_qsize 0\n",
      "2020-03-12 19:01:01,417 : INFO : EPOCH 3 - PROGRESS: at 61.86% examples, 178930 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-12 19:01:02,431 : INFO : EPOCH 3 - PROGRESS: at 62.58% examples, 178806 words/s, in_qsize 8, out_qsize 0\n",
      "2020-03-12 19:01:03,443 : INFO : EPOCH 3 - PROGRESS: at 63.29% examples, 178690 words/s, in_qsize 8, out_qsize 0\n",
      "2020-03-12 19:01:04,446 : INFO : EPOCH 3 - PROGRESS: at 64.22% examples, 178602 words/s, in_qsize 8, out_qsize 0\n",
      "2020-03-12 19:01:05,453 : INFO : EPOCH 3 - PROGRESS: at 65.10% examples, 178508 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-12 19:01:06,471 : INFO : EPOCH 3 - PROGRESS: at 65.97% examples, 178665 words/s, in_qsize 8, out_qsize 0\n",
      "2020-03-12 19:01:07,479 : INFO : EPOCH 3 - PROGRESS: at 66.79% examples, 178553 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-12 19:01:08,480 : INFO : EPOCH 3 - PROGRESS: at 67.54% examples, 178462 words/s, in_qsize 8, out_qsize 0\n",
      "2020-03-12 19:01:09,491 : INFO : EPOCH 3 - PROGRESS: at 68.39% examples, 178356 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-12 19:01:10,506 : INFO : EPOCH 3 - PROGRESS: at 69.14% examples, 178283 words/s, in_qsize 8, out_qsize 0\n",
      "2020-03-12 19:01:11,528 : INFO : EPOCH 3 - PROGRESS: at 70.02% examples, 178181 words/s, in_qsize 8, out_qsize 0\n",
      "2020-03-12 19:01:12,542 : INFO : EPOCH 3 - PROGRESS: at 71.01% examples, 178163 words/s, in_qsize 8, out_qsize 0\n",
      "2020-03-12 19:01:13,590 : INFO : EPOCH 3 - PROGRESS: at 71.96% examples, 178164 words/s, in_qsize 8, out_qsize 0\n",
      "2020-03-12 19:01:14,610 : INFO : EPOCH 3 - PROGRESS: at 72.89% examples, 178036 words/s, in_qsize 8, out_qsize 0\n",
      "2020-03-12 19:01:15,632 : INFO : EPOCH 3 - PROGRESS: at 73.89% examples, 178058 words/s, in_qsize 8, out_qsize 1\n",
      "2020-03-12 19:01:16,652 : INFO : EPOCH 3 - PROGRESS: at 74.74% examples, 178021 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-12 19:01:17,682 : INFO : EPOCH 3 - PROGRESS: at 75.76% examples, 177887 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-12 19:01:18,720 : INFO : EPOCH 3 - PROGRESS: at 77.02% examples, 177765 words/s, in_qsize 8, out_qsize 0\n",
      "2020-03-12 19:01:19,781 : INFO : EPOCH 3 - PROGRESS: at 77.98% examples, 177583 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-12 19:01:20,810 : INFO : EPOCH 3 - PROGRESS: at 78.88% examples, 177449 words/s, in_qsize 8, out_qsize 0\n",
      "2020-03-12 19:01:21,863 : INFO : EPOCH 3 - PROGRESS: at 79.67% examples, 177123 words/s, in_qsize 6, out_qsize 1\n",
      "2020-03-12 19:01:22,973 : INFO : EPOCH 3 - PROGRESS: at 80.68% examples, 176931 words/s, in_qsize 8, out_qsize 2\n",
      "2020-03-12 19:01:24,061 : INFO : EPOCH 3 - PROGRESS: at 81.59% examples, 176768 words/s, in_qsize 8, out_qsize 0\n",
      "2020-03-12 19:01:25,066 : INFO : EPOCH 3 - PROGRESS: at 82.38% examples, 176697 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-12 19:01:26,072 : INFO : EPOCH 3 - PROGRESS: at 83.20% examples, 176479 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-12 19:01:27,080 : INFO : EPOCH 3 - PROGRESS: at 84.16% examples, 176368 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-12 19:01:28,148 : INFO : EPOCH 3 - PROGRESS: at 85.08% examples, 176301 words/s, in_qsize 8, out_qsize 0\n",
      "2020-03-12 19:01:29,252 : INFO : EPOCH 3 - PROGRESS: at 85.96% examples, 176379 words/s, in_qsize 8, out_qsize 1\n",
      "2020-03-12 19:01:30,265 : INFO : EPOCH 3 - PROGRESS: at 86.71% examples, 176316 words/s, in_qsize 8, out_qsize 0\n",
      "2020-03-12 19:01:31,281 : INFO : EPOCH 3 - PROGRESS: at 87.38% examples, 176238 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-12 19:01:32,330 : INFO : EPOCH 3 - PROGRESS: at 88.28% examples, 175971 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-12 19:01:33,332 : INFO : EPOCH 3 - PROGRESS: at 89.12% examples, 175916 words/s, in_qsize 8, out_qsize 0\n",
      "2020-03-12 19:01:34,374 : INFO : EPOCH 3 - PROGRESS: at 89.81% examples, 175737 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-12 19:01:35,404 : INFO : EPOCH 3 - PROGRESS: at 90.64% examples, 175680 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-12 19:01:36,431 : INFO : EPOCH 3 - PROGRESS: at 91.35% examples, 175615 words/s, in_qsize 8, out_qsize 0\n",
      "2020-03-12 19:01:37,457 : INFO : EPOCH 3 - PROGRESS: at 92.01% examples, 175545 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-12 19:01:38,509 : INFO : EPOCH 3 - PROGRESS: at 92.72% examples, 175302 words/s, in_qsize 7, out_qsize 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-03-12 19:01:39,539 : INFO : EPOCH 3 - PROGRESS: at 93.46% examples, 175227 words/s, in_qsize 8, out_qsize 0\n",
      "2020-03-12 19:01:40,568 : INFO : EPOCH 3 - PROGRESS: at 94.22% examples, 175146 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-12 19:01:41,607 : INFO : EPOCH 3 - PROGRESS: at 95.07% examples, 175116 words/s, in_qsize 8, out_qsize 0\n",
      "2020-03-12 19:01:42,628 : INFO : EPOCH 3 - PROGRESS: at 95.84% examples, 175078 words/s, in_qsize 8, out_qsize 0\n",
      "2020-03-12 19:01:43,638 : INFO : EPOCH 3 - PROGRESS: at 96.59% examples, 175255 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-12 19:01:44,656 : INFO : EPOCH 3 - PROGRESS: at 97.41% examples, 175599 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-12 19:01:45,671 : INFO : EPOCH 3 - PROGRESS: at 98.27% examples, 176008 words/s, in_qsize 8, out_qsize 1\n",
      "2020-03-12 19:01:46,711 : INFO : EPOCH 3 - PROGRESS: at 99.10% examples, 176373 words/s, in_qsize 8, out_qsize 0\n",
      "2020-03-12 19:01:47,605 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-03-12 19:01:47,612 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-03-12 19:01:47,636 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-03-12 19:01:47,663 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-03-12 19:01:47,663 : INFO : EPOCH - 3 : training on 27374377 raw words (20808311 effective words) took 117.7s, 176751 effective words/s\n",
      "2020-03-12 19:01:48,675 : INFO : EPOCH 4 - PROGRESS: at 1.07% examples, 162195 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-12 19:01:49,747 : INFO : EPOCH 4 - PROGRESS: at 2.20% examples, 168036 words/s, in_qsize 8, out_qsize 0\n",
      "2020-03-12 19:01:50,823 : INFO : EPOCH 4 - PROGRESS: at 3.68% examples, 186314 words/s, in_qsize 8, out_qsize 1\n",
      "2020-03-12 19:01:51,893 : INFO : EPOCH 4 - PROGRESS: at 5.15% examples, 195283 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-12 19:01:52,980 : INFO : EPOCH 4 - PROGRESS: at 6.67% examples, 200452 words/s, in_qsize 8, out_qsize 0\n",
      "2020-03-12 19:01:54,053 : INFO : EPOCH 4 - PROGRESS: at 8.58% examples, 204414 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-12 19:01:55,117 : INFO : EPOCH 4 - PROGRESS: at 10.07% examples, 207309 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-12 19:01:56,134 : INFO : EPOCH 4 - PROGRESS: at 11.51% examples, 209960 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-12 19:01:57,162 : INFO : EPOCH 4 - PROGRESS: at 12.82% examples, 209157 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-12 19:01:58,182 : INFO : EPOCH 4 - PROGRESS: at 13.96% examples, 206582 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-12 19:01:59,184 : INFO : EPOCH 4 - PROGRESS: at 15.47% examples, 203222 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-12 19:02:00,199 : INFO : EPOCH 4 - PROGRESS: at 16.60% examples, 203501 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-12 19:02:01,210 : INFO : EPOCH 4 - PROGRESS: at 17.55% examples, 204665 words/s, in_qsize 8, out_qsize 0\n",
      "2020-03-12 19:02:02,231 : INFO : EPOCH 4 - PROGRESS: at 18.59% examples, 205688 words/s, in_qsize 8, out_qsize 0\n",
      "2020-03-12 19:02:03,241 : INFO : EPOCH 4 - PROGRESS: at 19.56% examples, 206588 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-12 19:02:04,243 : INFO : EPOCH 4 - PROGRESS: at 20.71% examples, 207546 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-12 19:02:05,253 : INFO : EPOCH 4 - PROGRESS: at 21.75% examples, 205787 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-12 19:02:06,311 : INFO : EPOCH 4 - PROGRESS: at 22.90% examples, 204762 words/s, in_qsize 6, out_qsize 1\n",
      "2020-03-12 19:02:07,332 : INFO : EPOCH 4 - PROGRESS: at 23.98% examples, 205966 words/s, in_qsize 8, out_qsize 0\n",
      "2020-03-12 19:02:08,346 : INFO : EPOCH 4 - PROGRESS: at 24.87% examples, 205322 words/s, in_qsize 8, out_qsize 0\n",
      "2020-03-12 19:02:09,394 : INFO : EPOCH 4 - PROGRESS: at 25.56% examples, 203726 words/s, in_qsize 8, out_qsize 0\n",
      "2020-03-12 19:02:10,407 : INFO : EPOCH 4 - PROGRESS: at 26.35% examples, 203588 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-12 19:02:11,415 : INFO : EPOCH 4 - PROGRESS: at 27.20% examples, 204424 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-12 19:02:12,425 : INFO : EPOCH 4 - PROGRESS: at 27.93% examples, 203714 words/s, in_qsize 8, out_qsize 0\n",
      "2020-03-12 19:02:13,437 : INFO : EPOCH 4 - PROGRESS: at 28.60% examples, 202482 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-12 19:02:14,519 : INFO : EPOCH 4 - PROGRESS: at 29.32% examples, 201342 words/s, in_qsize 8, out_qsize 0\n",
      "2020-03-12 19:02:15,558 : INFO : EPOCH 4 - PROGRESS: at 30.04% examples, 200595 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-12 19:02:16,599 : INFO : EPOCH 4 - PROGRESS: at 30.79% examples, 199618 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-12 19:02:17,631 : INFO : EPOCH 4 - PROGRESS: at 31.51% examples, 198564 words/s, in_qsize 8, out_qsize 1\n",
      "2020-03-12 19:02:18,653 : INFO : EPOCH 4 - PROGRESS: at 32.21% examples, 198097 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-12 19:02:19,725 : INFO : EPOCH 4 - PROGRESS: at 32.90% examples, 197104 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-12 19:02:20,763 : INFO : EPOCH 4 - PROGRESS: at 33.58% examples, 196608 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-12 19:02:21,783 : INFO : EPOCH 4 - PROGRESS: at 34.14% examples, 195541 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-12 19:02:22,871 : INFO : EPOCH 4 - PROGRESS: at 34.82% examples, 194200 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-12 19:02:23,901 : INFO : EPOCH 4 - PROGRESS: at 35.52% examples, 193264 words/s, in_qsize 8, out_qsize 0\n",
      "2020-03-12 19:02:24,922 : INFO : EPOCH 4 - PROGRESS: at 36.18% examples, 192433 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-12 19:02:25,969 : INFO : EPOCH 4 - PROGRESS: at 36.81% examples, 191499 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-12 19:02:27,060 : INFO : EPOCH 4 - PROGRESS: at 37.40% examples, 190603 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-12 19:02:28,120 : INFO : EPOCH 4 - PROGRESS: at 38.15% examples, 189901 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-12 19:02:29,172 : INFO : EPOCH 4 - PROGRESS: at 38.91% examples, 189070 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-12 19:02:30,179 : INFO : EPOCH 4 - PROGRESS: at 39.95% examples, 188461 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-12 19:02:31,188 : INFO : EPOCH 4 - PROGRESS: at 40.91% examples, 187878 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-12 19:02:32,233 : INFO : EPOCH 4 - PROGRESS: at 41.87% examples, 187171 words/s, in_qsize 8, out_qsize 0\n",
      "2020-03-12 19:02:33,261 : INFO : EPOCH 4 - PROGRESS: at 43.20% examples, 187687 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-12 19:02:34,303 : INFO : EPOCH 4 - PROGRESS: at 44.49% examples, 188474 words/s, in_qsize 8, out_qsize 0\n",
      "2020-03-12 19:02:35,331 : INFO : EPOCH 4 - PROGRESS: at 46.34% examples, 189200 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-12 19:02:36,332 : INFO : EPOCH 4 - PROGRESS: at 47.39% examples, 189921 words/s, in_qsize 8, out_qsize 0\n",
      "2020-03-12 19:02:37,339 : INFO : EPOCH 4 - PROGRESS: at 48.36% examples, 190394 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-12 19:02:38,345 : INFO : EPOCH 4 - PROGRESS: at 49.56% examples, 191033 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-12 19:02:39,353 : INFO : EPOCH 4 - PROGRESS: at 50.50% examples, 191205 words/s, in_qsize 8, out_qsize 0\n",
      "2020-03-12 19:02:40,372 : INFO : EPOCH 4 - PROGRESS: at 51.27% examples, 190932 words/s, in_qsize 8, out_qsize 0\n",
      "2020-03-12 19:02:41,437 : INFO : EPOCH 4 - PROGRESS: at 52.06% examples, 190491 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-12 19:02:42,479 : INFO : EPOCH 4 - PROGRESS: at 52.95% examples, 190164 words/s, in_qsize 8, out_qsize 0\n",
      "2020-03-12 19:02:43,502 : INFO : EPOCH 4 - PROGRESS: at 53.75% examples, 189955 words/s, in_qsize 8, out_qsize 0\n",
      "2020-03-12 19:02:44,535 : INFO : EPOCH 4 - PROGRESS: at 54.47% examples, 189680 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-12 19:02:45,632 : INFO : EPOCH 4 - PROGRESS: at 55.15% examples, 189223 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-12 19:02:46,648 : INFO : EPOCH 4 - PROGRESS: at 55.80% examples, 188900 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-12 19:02:47,656 : INFO : EPOCH 4 - PROGRESS: at 56.32% examples, 188352 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-12 19:02:48,705 : INFO : EPOCH 4 - PROGRESS: at 56.91% examples, 187725 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-12 19:02:49,714 : INFO : EPOCH 4 - PROGRESS: at 57.43% examples, 187324 words/s, in_qsize 8, out_qsize 0\n",
      "2020-03-12 19:02:50,765 : INFO : EPOCH 4 - PROGRESS: at 58.07% examples, 186957 words/s, in_qsize 8, out_qsize 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-03-12 19:02:51,769 : INFO : EPOCH 4 - PROGRESS: at 58.79% examples, 186962 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-12 19:02:52,775 : INFO : EPOCH 4 - PROGRESS: at 59.61% examples, 187336 words/s, in_qsize 8, out_qsize 0\n",
      "2020-03-12 19:02:53,845 : INFO : EPOCH 4 - PROGRESS: at 60.40% examples, 187479 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-12 19:02:54,894 : INFO : EPOCH 4 - PROGRESS: at 61.03% examples, 187137 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-12 19:02:55,895 : INFO : EPOCH 4 - PROGRESS: at 61.66% examples, 186810 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-12 19:02:56,919 : INFO : EPOCH 4 - PROGRESS: at 62.35% examples, 186321 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-12 19:02:57,931 : INFO : EPOCH 4 - PROGRESS: at 63.01% examples, 186093 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-12 19:02:58,965 : INFO : EPOCH 4 - PROGRESS: at 63.72% examples, 185595 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-12 19:03:00,092 : INFO : EPOCH 4 - PROGRESS: at 64.77% examples, 185209 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-12 19:03:01,185 : INFO : EPOCH 4 - PROGRESS: at 65.63% examples, 184894 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-12 19:03:02,190 : INFO : EPOCH 4 - PROGRESS: at 66.36% examples, 184588 words/s, in_qsize 8, out_qsize 0\n",
      "2020-03-12 19:03:03,225 : INFO : EPOCH 4 - PROGRESS: at 67.12% examples, 184327 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-12 19:03:04,228 : INFO : EPOCH 4 - PROGRESS: at 67.92% examples, 184160 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-12 19:03:05,323 : INFO : EPOCH 4 - PROGRESS: at 68.76% examples, 183889 words/s, in_qsize 8, out_qsize 0\n",
      "2020-03-12 19:03:06,328 : INFO : EPOCH 4 - PROGRESS: at 69.52% examples, 183681 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-12 19:03:07,328 : INFO : EPOCH 4 - PROGRESS: at 70.43% examples, 183346 words/s, in_qsize 8, out_qsize 0\n",
      "2020-03-12 19:03:08,346 : INFO : EPOCH 4 - PROGRESS: at 71.29% examples, 183161 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-12 19:03:09,374 : INFO : EPOCH 4 - PROGRESS: at 72.28% examples, 183141 words/s, in_qsize 8, out_qsize 0\n",
      "2020-03-12 19:03:10,439 : INFO : EPOCH 4 - PROGRESS: at 73.26% examples, 182930 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-12 19:03:11,454 : INFO : EPOCH 4 - PROGRESS: at 74.37% examples, 183361 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-12 19:03:12,533 : INFO : EPOCH 4 - PROGRESS: at 75.36% examples, 183136 words/s, in_qsize 8, out_qsize 0\n",
      "2020-03-12 19:03:13,547 : INFO : EPOCH 4 - PROGRESS: at 76.55% examples, 182993 words/s, in_qsize 8, out_qsize 0\n",
      "2020-03-12 19:03:14,562 : INFO : EPOCH 4 - PROGRESS: at 77.60% examples, 182847 words/s, in_qsize 8, out_qsize 0\n",
      "2020-03-12 19:03:15,609 : INFO : EPOCH 4 - PROGRESS: at 78.55% examples, 182619 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-12 19:03:16,700 : INFO : EPOCH 4 - PROGRESS: at 79.42% examples, 182383 words/s, in_qsize 6, out_qsize 1\n",
      "2020-03-12 19:03:17,724 : INFO : EPOCH 4 - PROGRESS: at 80.48% examples, 182377 words/s, in_qsize 8, out_qsize 0\n",
      "2020-03-12 19:03:18,742 : INFO : EPOCH 4 - PROGRESS: at 81.38% examples, 182208 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-12 19:03:19,786 : INFO : EPOCH 4 - PROGRESS: at 82.07% examples, 181831 words/s, in_qsize 8, out_qsize 0\n",
      "2020-03-12 19:03:20,796 : INFO : EPOCH 4 - PROGRESS: at 82.96% examples, 181700 words/s, in_qsize 8, out_qsize 0\n",
      "2020-03-12 19:03:21,814 : INFO : EPOCH 4 - PROGRESS: at 83.91% examples, 181424 words/s, in_qsize 8, out_qsize 0\n",
      "2020-03-12 19:03:22,861 : INFO : EPOCH 4 - PROGRESS: at 84.85% examples, 181346 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-12 19:03:23,911 : INFO : EPOCH 4 - PROGRESS: at 85.63% examples, 181313 words/s, in_qsize 8, out_qsize 0\n",
      "2020-03-12 19:03:24,966 : INFO : EPOCH 4 - PROGRESS: at 86.45% examples, 181197 words/s, in_qsize 7, out_qsize 1\n",
      "2020-03-12 19:03:25,998 : INFO : EPOCH 4 - PROGRESS: at 87.18% examples, 181190 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-12 19:03:27,090 : INFO : EPOCH 4 - PROGRESS: at 88.18% examples, 181009 words/s, in_qsize 8, out_qsize 1\n",
      "2020-03-12 19:03:28,136 : INFO : EPOCH 4 - PROGRESS: at 89.12% examples, 181050 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-12 19:03:29,294 : INFO : EPOCH 4 - PROGRESS: at 89.89% examples, 180757 words/s, in_qsize 8, out_qsize 1\n",
      "2020-03-12 19:03:30,409 : INFO : EPOCH 4 - PROGRESS: at 90.75% examples, 180579 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-12 19:03:31,527 : INFO : EPOCH 4 - PROGRESS: at 91.45% examples, 180369 words/s, in_qsize 8, out_qsize 1\n",
      "2020-03-12 19:03:32,560 : INFO : EPOCH 4 - PROGRESS: at 92.14% examples, 180238 words/s, in_qsize 8, out_qsize 0\n",
      "2020-03-12 19:03:33,586 : INFO : EPOCH 4 - PROGRESS: at 92.99% examples, 180205 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-12 19:03:34,597 : INFO : EPOCH 4 - PROGRESS: at 93.66% examples, 180104 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-12 19:03:35,597 : INFO : EPOCH 4 - PROGRESS: at 94.47% examples, 180024 words/s, in_qsize 8, out_qsize 0\n",
      "2020-03-12 19:03:36,610 : INFO : EPOCH 4 - PROGRESS: at 95.29% examples, 179923 words/s, in_qsize 8, out_qsize 0\n",
      "2020-03-12 19:03:37,635 : INFO : EPOCH 4 - PROGRESS: at 96.01% examples, 179779 words/s, in_qsize 8, out_qsize 0\n",
      "2020-03-12 19:03:38,663 : INFO : EPOCH 4 - PROGRESS: at 96.67% examples, 179744 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-12 19:03:39,719 : INFO : EPOCH 4 - PROGRESS: at 97.35% examples, 179660 words/s, in_qsize 8, out_qsize 0\n",
      "2020-03-12 19:03:40,727 : INFO : EPOCH 4 - PROGRESS: at 98.04% examples, 179660 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-12 19:03:41,747 : INFO : EPOCH 4 - PROGRESS: at 98.70% examples, 179629 words/s, in_qsize 8, out_qsize 0\n",
      "2020-03-12 19:03:42,857 : INFO : EPOCH 4 - PROGRESS: at 99.43% examples, 179472 words/s, in_qsize 6, out_qsize 1\n",
      "2020-03-12 19:03:43,505 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-03-12 19:03:43,531 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-03-12 19:03:43,544 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-03-12 19:03:43,559 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-03-12 19:03:43,560 : INFO : EPOCH - 4 : training on 27374377 raw words (20810691 effective words) took 115.9s, 179567 effective words/s\n",
      "2020-03-12 19:03:44,603 : INFO : EPOCH 5 - PROGRESS: at 1.11% examples, 164498 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-12 19:03:45,649 : INFO : EPOCH 5 - PROGRESS: at 2.34% examples, 178429 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-12 19:03:46,675 : INFO : EPOCH 5 - PROGRESS: at 3.76% examples, 193756 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-12 19:03:47,688 : INFO : EPOCH 5 - PROGRESS: at 5.15% examples, 200126 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-12 19:03:48,704 : INFO : EPOCH 5 - PROGRESS: at 6.56% examples, 204237 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-12 19:03:49,754 : INFO : EPOCH 5 - PROGRESS: at 8.37% examples, 206084 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-12 19:03:50,768 : INFO : EPOCH 5 - PROGRESS: at 9.79% examples, 208178 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-12 19:03:51,792 : INFO : EPOCH 5 - PROGRESS: at 11.08% examples, 207901 words/s, in_qsize 8, out_qsize 1\n",
      "2020-03-12 19:03:52,831 : INFO : EPOCH 5 - PROGRESS: at 12.56% examples, 209480 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-12 19:03:53,846 : INFO : EPOCH 5 - PROGRESS: at 13.86% examples, 209812 words/s, in_qsize 8, out_qsize 1\n",
      "2020-03-12 19:03:54,856 : INFO : EPOCH 5 - PROGRESS: at 15.82% examples, 211950 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-12 19:03:55,875 : INFO : EPOCH 5 - PROGRESS: at 16.90% examples, 212017 words/s, in_qsize 8, out_qsize 0\n",
      "2020-03-12 19:03:56,890 : INFO : EPOCH 5 - PROGRESS: at 17.86% examples, 212539 words/s, in_qsize 8, out_qsize 0\n",
      "2020-03-12 19:03:57,902 : INFO : EPOCH 5 - PROGRESS: at 18.59% examples, 208984 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-12 19:03:58,916 : INFO : EPOCH 5 - PROGRESS: at 19.20% examples, 205213 words/s, in_qsize 8, out_qsize 0\n",
      "2020-03-12 19:04:00,023 : INFO : EPOCH 5 - PROGRESS: at 20.15% examples, 202260 words/s, in_qsize 8, out_qsize 0\n",
      "2020-03-12 19:04:01,037 : INFO : EPOCH 5 - PROGRESS: at 21.00% examples, 200302 words/s, in_qsize 8, out_qsize 0\n",
      "2020-03-12 19:04:02,080 : INFO : EPOCH 5 - PROGRESS: at 21.95% examples, 197528 words/s, in_qsize 8, out_qsize 0\n",
      "2020-03-12 19:04:03,157 : INFO : EPOCH 5 - PROGRESS: at 22.96% examples, 195655 words/s, in_qsize 7, out_qsize 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-03-12 19:04:04,225 : INFO : EPOCH 5 - PROGRESS: at 23.77% examples, 193889 words/s, in_qsize 8, out_qsize 0\n",
      "2020-03-12 19:04:05,240 : INFO : EPOCH 5 - PROGRESS: at 24.56% examples, 192388 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-12 19:04:06,252 : INFO : EPOCH 5 - PROGRESS: at 25.22% examples, 190790 words/s, in_qsize 8, out_qsize 0\n",
      "2020-03-12 19:04:07,291 : INFO : EPOCH 5 - PROGRESS: at 25.87% examples, 189737 words/s, in_qsize 8, out_qsize 0\n",
      "2020-03-12 19:04:08,353 : INFO : EPOCH 5 - PROGRESS: at 26.56% examples, 189175 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-12 19:04:09,384 : INFO : EPOCH 5 - PROGRESS: at 27.27% examples, 188613 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-12 19:04:10,393 : INFO : EPOCH 5 - PROGRESS: at 27.96% examples, 188248 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-12 19:04:11,472 : INFO : EPOCH 5 - PROGRESS: at 28.66% examples, 187498 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-12 19:04:12,490 : INFO : EPOCH 5 - PROGRESS: at 29.32% examples, 186887 words/s, in_qsize 8, out_qsize 0\n",
      "2020-03-12 19:04:13,563 : INFO : EPOCH 5 - PROGRESS: at 29.98% examples, 185967 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-12 19:04:14,581 : INFO : EPOCH 5 - PROGRESS: at 30.73% examples, 185710 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-12 19:04:15,706 : INFO : EPOCH 5 - PROGRESS: at 31.48% examples, 185086 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-12 19:04:16,718 : INFO : EPOCH 5 - PROGRESS: at 32.18% examples, 184890 words/s, in_qsize 8, out_qsize 0\n",
      "2020-03-12 19:04:17,722 : INFO : EPOCH 5 - PROGRESS: at 32.99% examples, 185632 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-12 19:04:18,738 : INFO : EPOCH 5 - PROGRESS: at 33.65% examples, 185592 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-12 19:04:19,751 : INFO : EPOCH 5 - PROGRESS: at 34.26% examples, 185154 words/s, in_qsize 8, out_qsize 0\n",
      "2020-03-12 19:04:20,795 : INFO : EPOCH 5 - PROGRESS: at 34.97% examples, 184631 words/s, in_qsize 8, out_qsize 0\n",
      "2020-03-12 19:04:21,800 : INFO : EPOCH 5 - PROGRESS: at 35.62% examples, 183905 words/s, in_qsize 8, out_qsize 0\n",
      "2020-03-12 19:04:22,804 : INFO : EPOCH 5 - PROGRESS: at 36.27% examples, 183267 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-12 19:04:23,893 : INFO : EPOCH 5 - PROGRESS: at 36.93% examples, 182814 words/s, in_qsize 8, out_qsize 2\n",
      "2020-03-12 19:04:24,916 : INFO : EPOCH 5 - PROGRESS: at 37.62% examples, 182835 words/s, in_qsize 8, out_qsize 0\n",
      "2020-03-12 19:04:25,939 : INFO : EPOCH 5 - PROGRESS: at 38.34% examples, 182535 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-12 19:04:26,946 : INFO : EPOCH 5 - PROGRESS: at 39.28% examples, 182262 words/s, in_qsize 8, out_qsize 0\n",
      "2020-03-12 19:04:27,972 : INFO : EPOCH 5 - PROGRESS: at 40.31% examples, 181916 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-12 19:04:29,049 : INFO : EPOCH 5 - PROGRESS: at 41.39% examples, 181567 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-12 19:04:30,068 : INFO : EPOCH 5 - PROGRESS: at 42.42% examples, 181278 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-12 19:04:31,069 : INFO : EPOCH 5 - PROGRESS: at 43.40% examples, 180903 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-12 19:04:32,094 : INFO : EPOCH 5 - PROGRESS: at 44.30% examples, 180480 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-12 19:04:33,116 : INFO : EPOCH 5 - PROGRESS: at 45.36% examples, 179977 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-12 19:04:34,137 : INFO : EPOCH 5 - PROGRESS: at 46.74% examples, 179650 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-12 19:04:35,165 : INFO : EPOCH 5 - PROGRESS: at 47.51% examples, 179512 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-12 19:04:36,224 : INFO : EPOCH 5 - PROGRESS: at 48.30% examples, 179283 words/s, in_qsize 8, out_qsize 0\n",
      "2020-03-12 19:04:37,251 : INFO : EPOCH 5 - PROGRESS: at 49.22% examples, 179182 words/s, in_qsize 8, out_qsize 0\n",
      "2020-03-12 19:04:38,327 : INFO : EPOCH 5 - PROGRESS: at 50.13% examples, 178812 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-12 19:04:39,342 : INFO : EPOCH 5 - PROGRESS: at 50.91% examples, 178914 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-12 19:04:40,417 : INFO : EPOCH 5 - PROGRESS: at 51.74% examples, 178685 words/s, in_qsize 8, out_qsize 0\n",
      "2020-03-12 19:04:41,418 : INFO : EPOCH 5 - PROGRESS: at 52.47% examples, 178588 words/s, in_qsize 8, out_qsize 0\n",
      "2020-03-12 19:04:42,425 : INFO : EPOCH 5 - PROGRESS: at 53.27% examples, 178241 words/s, in_qsize 8, out_qsize 0\n",
      "2020-03-12 19:04:43,543 : INFO : EPOCH 5 - PROGRESS: at 53.99% examples, 177822 words/s, in_qsize 7, out_qsize 1\n",
      "2020-03-12 19:04:44,580 : INFO : EPOCH 5 - PROGRESS: at 54.71% examples, 177768 words/s, in_qsize 8, out_qsize 1\n",
      "2020-03-12 19:04:45,620 : INFO : EPOCH 5 - PROGRESS: at 55.55% examples, 178550 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-12 19:04:46,647 : INFO : EPOCH 5 - PROGRESS: at 56.34% examples, 179221 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-12 19:04:47,649 : INFO : EPOCH 5 - PROGRESS: at 57.13% examples, 179833 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-12 19:04:48,680 : INFO : EPOCH 5 - PROGRESS: at 57.88% examples, 180345 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-12 19:04:49,695 : INFO : EPOCH 5 - PROGRESS: at 58.73% examples, 180987 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-12 19:04:50,751 : INFO : EPOCH 5 - PROGRESS: at 59.61% examples, 181523 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-12 19:04:51,766 : INFO : EPOCH 5 - PROGRESS: at 60.43% examples, 182008 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-12 19:04:52,777 : INFO : EPOCH 5 - PROGRESS: at 61.12% examples, 182073 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-12 19:04:53,820 : INFO : EPOCH 5 - PROGRESS: at 61.82% examples, 181930 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-12 19:04:54,828 : INFO : EPOCH 5 - PROGRESS: at 62.58% examples, 181883 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-12 19:04:55,847 : INFO : EPOCH 5 - PROGRESS: at 63.41% examples, 182122 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-12 19:04:56,868 : INFO : EPOCH 5 - PROGRESS: at 64.39% examples, 182044 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-12 19:04:57,912 : INFO : EPOCH 5 - PROGRESS: at 65.30% examples, 181907 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-12 19:04:58,949 : INFO : EPOCH 5 - PROGRESS: at 66.05% examples, 181671 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-12 19:04:59,988 : INFO : EPOCH 5 - PROGRESS: at 66.95% examples, 181637 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-12 19:05:01,023 : INFO : EPOCH 5 - PROGRESS: at 67.74% examples, 181529 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-12 19:05:02,056 : INFO : EPOCH 5 - PROGRESS: at 68.55% examples, 181330 words/s, in_qsize 8, out_qsize 0\n",
      "2020-03-12 19:05:03,079 : INFO : EPOCH 5 - PROGRESS: at 69.31% examples, 181115 words/s, in_qsize 6, out_qsize 1\n",
      "2020-03-12 19:05:04,174 : INFO : EPOCH 5 - PROGRESS: at 70.20% examples, 180709 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-12 19:05:05,270 : INFO : EPOCH 5 - PROGRESS: at 71.16% examples, 180481 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-12 19:05:06,316 : INFO : EPOCH 5 - PROGRESS: at 72.13% examples, 180456 words/s, in_qsize 8, out_qsize 0\n",
      "2020-03-12 19:05:07,335 : INFO : EPOCH 5 - PROGRESS: at 73.05% examples, 180294 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-12 19:05:08,344 : INFO : EPOCH 5 - PROGRESS: at 73.93% examples, 180056 words/s, in_qsize 8, out_qsize 0\n",
      "2020-03-12 19:05:09,353 : INFO : EPOCH 5 - PROGRESS: at 74.74% examples, 179936 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-12 19:05:10,356 : INFO : EPOCH 5 - PROGRESS: at 75.77% examples, 179837 words/s, in_qsize 8, out_qsize 0\n",
      "2020-03-12 19:05:11,449 : INFO : EPOCH 5 - PROGRESS: at 77.07% examples, 179665 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-12 19:05:12,469 : INFO : EPOCH 5 - PROGRESS: at 78.03% examples, 179542 words/s, in_qsize 8, out_qsize 0\n",
      "2020-03-12 19:05:13,530 : INFO : EPOCH 5 - PROGRESS: at 78.91% examples, 179318 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-12 19:05:14,583 : INFO : EPOCH 5 - PROGRESS: at 79.84% examples, 179214 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-12 19:05:15,625 : INFO : EPOCH 5 - PROGRESS: at 80.86% examples, 179205 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-12 19:05:16,680 : INFO : EPOCH 5 - PROGRESS: at 81.75% examples, 179078 words/s, in_qsize 8, out_qsize 0\n",
      "2020-03-12 19:05:17,694 : INFO : EPOCH 5 - PROGRESS: at 82.63% examples, 179123 words/s, in_qsize 8, out_qsize 0\n",
      "2020-03-12 19:05:18,697 : INFO : EPOCH 5 - PROGRESS: at 83.82% examples, 179374 words/s, in_qsize 7, out_qsize 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-03-12 19:05:19,756 : INFO : EPOCH 5 - PROGRESS: at 84.78% examples, 179300 words/s, in_qsize 8, out_qsize 0\n",
      "2020-03-12 19:05:20,818 : INFO : EPOCH 5 - PROGRESS: at 85.50% examples, 179113 words/s, in_qsize 8, out_qsize 0\n",
      "2020-03-12 19:05:21,821 : INFO : EPOCH 5 - PROGRESS: at 86.24% examples, 178898 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-12 19:05:22,898 : INFO : EPOCH 5 - PROGRESS: at 86.91% examples, 178673 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-12 19:05:23,980 : INFO : EPOCH 5 - PROGRESS: at 87.82% examples, 178465 words/s, in_qsize 8, out_qsize 1\n",
      "2020-03-12 19:05:24,982 : INFO : EPOCH 5 - PROGRESS: at 88.70% examples, 178471 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-12 19:05:26,007 : INFO : EPOCH 5 - PROGRESS: at 89.43% examples, 178356 words/s, in_qsize 8, out_qsize 1\n",
      "2020-03-12 19:05:27,029 : INFO : EPOCH 5 - PROGRESS: at 90.30% examples, 178339 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-12 19:05:28,080 : INFO : EPOCH 5 - PROGRESS: at 91.11% examples, 178299 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-12 19:05:29,102 : INFO : EPOCH 5 - PROGRESS: at 91.76% examples, 178277 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-12 19:05:30,105 : INFO : EPOCH 5 - PROGRESS: at 92.52% examples, 178218 words/s, in_qsize 8, out_qsize 0\n",
      "2020-03-12 19:05:31,224 : INFO : EPOCH 5 - PROGRESS: at 93.32% examples, 178047 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-12 19:05:32,333 : INFO : EPOCH 5 - PROGRESS: at 94.07% examples, 177876 words/s, in_qsize 8, out_qsize 0\n",
      "2020-03-12 19:05:33,473 : INFO : EPOCH 5 - PROGRESS: at 94.92% examples, 177655 words/s, in_qsize 6, out_qsize 1\n",
      "2020-03-12 19:05:34,591 : INFO : EPOCH 5 - PROGRESS: at 95.74% examples, 177498 words/s, in_qsize 6, out_qsize 1\n",
      "2020-03-12 19:05:35,685 : INFO : EPOCH 5 - PROGRESS: at 96.53% examples, 177595 words/s, in_qsize 8, out_qsize 2\n",
      "2020-03-12 19:05:36,690 : INFO : EPOCH 5 - PROGRESS: at 97.20% examples, 177613 words/s, in_qsize 8, out_qsize 0\n",
      "2020-03-12 19:05:37,719 : INFO : EPOCH 5 - PROGRESS: at 97.87% examples, 177527 words/s, in_qsize 8, out_qsize 1\n",
      "2020-03-12 19:05:38,725 : INFO : EPOCH 5 - PROGRESS: at 98.54% examples, 177542 words/s, in_qsize 8, out_qsize 0\n",
      "2020-03-12 19:05:39,752 : INFO : EPOCH 5 - PROGRESS: at 99.19% examples, 177468 words/s, in_qsize 8, out_qsize 0\n",
      "2020-03-12 19:05:40,670 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-03-12 19:05:40,727 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-03-12 19:05:40,797 : INFO : EPOCH 5 - PROGRESS: at 99.97% examples, 177443 words/s, in_qsize 1, out_qsize 1\n",
      "2020-03-12 19:05:40,798 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-03-12 19:05:40,805 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-03-12 19:05:40,805 : INFO : EPOCH - 5 : training on 27374377 raw words (20809892 effective words) took 117.2s, 177494 effective words/s\n",
      "2020-03-12 19:05:40,806 : INFO : training on a 136871885 raw words (104049392 effective words) took 588.2s, 176888 effective words/s\n",
      "2020-03-12 19:05:40,808 : INFO : precomputing L2-norms of word weight vectors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most similar to: silicon\n",
      "[('germanium', 0.6562955379486084), ('arsenide', 0.5950453281402588), ('polysilicon', 0.5918377637863159), ('silicide', 0.5918266773223877), ('nitride', 0.5912208557128906), ('phosphide', 0.5898659229278564), ('oxynitride', 0.5877647399902344), ('indium', 0.5833597183227539), ('SiGe', 0.5799596905708313), ('hydrogenated', 0.5767875909805298)]\n",
      "\n",
      "Most similar to: flux\n",
      "[('fluxes', 0.5494881868362427), ('Poynting', 0.478647381067276), ('PON', 0.43299418687820435), ('diapycnal', 0.43154287338256836), ('downdraft', 0.42625388503074646), ('sinking', 0.424627423286438), ('irradiances', 0.41862404346466064), ('TCO2', 0.417788565158844), ('Joule', 0.4175970256328583), ('shortwave', 0.41750848293304443)]\n",
      "\n",
      "Most similar to: stratosphere\n",
      "[('troposphere', 0.7435741424560547), ('mesosphere', 0.7202921509742737), ('stratospheric', 0.6767431497573853), ('tropospheric', 0.6421911716461182), ('mesopause', 0.6389365792274475), ('mesospheric', 0.6287567615509033), ('MLT', 0.6159111261367798), ('midlatitude', 0.6140425205230713), ('thermosphere', 0.612541913986206), ('extratropics', 0.5994011163711548)]\n",
      "\n",
      "Most similar to: music\n",
      "[('literary', 0.6003448963165283), ('dance', 0.5973033308982849), ('genres', 0.5796207189559937), ('utterances', 0.5738712549209595), ('poetry', 0.5670974254608154), ('accented', 0.5615972280502319), ('QuickTime', 0.5583388209342957), ('bilinguals', 0.549634575843811), ('Hindi', 0.5482633709907532), ('phonemes', 0.5450809001922607)]\n",
      "\n",
      "Most similar to: pitch\n",
      "[('loudness', 0.615315318107605), ('luminance', 0.577523946762085), ('syllable', 0.5529230833053589), ('phrasing', 0.5342088937759399), ('chromaticity', 0.5324673652648926), ('vowels', 0.5267131924629211), ('accents', 0.5255218744277954), ('verses', 0.5114873647689819), ('inflections', 0.5053534507751465), ('coreference', 0.5043647289276123)]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "vectors_full = gensim.models.Word2Vec(full_tokenized_text, size=1000, window=3, min_count=10, sg=1, workers=4)\n",
    "\n",
    "print(\"Most similar to:\", 'silicon')\n",
    "print(vectors_full.wv.most_similar('silicon'))\n",
    "print()\n",
    "\n",
    "print(\"Most similar to:\", 'flux')\n",
    "print(vectors_full.wv.most_similar('flux'))\n",
    "print()\n",
    "\n",
    "print(\"Most similar to:\", 'stratosphere')\n",
    "print(vectors_full.wv.most_similar('stratosphere'))\n",
    "print()\n",
    "\n",
    "print(\"Most similar to:\", 'music')\n",
    "print(vectors_full.wv.most_similar('music'))\n",
    "print()\n",
    "\n",
    "print(\"Most similar to:\", 'pitch')\n",
    "print(vectors_full.wv.most_similar('pitch'))\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well well well, with much much more information, the similar words are better than before. Plus the score went down. Probably because the vocabulary is significantly larger."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, for that part with ELMo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load ELMo model (takes a little while)\n",
    "tf.compat.v1.disable_eager_execution()\n",
    "elmo = hub.Module(\"https://tfhub.dev/google/elmo/3\", trainable=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def elmo_vectors(sents):\n",
    "    embeddings = elmo(sents, signature=\"default\", as_dict=True)[\"elmo\"]\n",
    "    with tf.compat.v1.Session() as sess:\n",
    "        sess.run(tf.compat.v1.global_variables_initializer())\n",
    "        return sess.run(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence: The pillow was there to break the fall.\n",
      "Vector for 'break': [-0.11922167  0.00357817 -0.33186203 ... -0.15633361  0.6160723\n",
      " -0.154829  ]\n",
      "\n",
      "Sentence: He's going to break the ladder\n",
      "Vector for 'break': [-0.35467988 -0.31914636  0.3277204  ...  0.22955832  0.14098051\n",
      " -0.24496101]\n",
      "\n",
      "Sentence: Give me a break\n",
      "Vector for 'break': [-0.38369322 -0.30650288 -0.15357974 ...  0.20365655  0.23261338\n",
      "  0.19071695]\n",
      "\n",
      "Sentence: I'm taking a break after this\n",
      "Vector for 'break': [-0.20114338 -0.4249664   0.3780563  ... -0.22613028  0.5801039\n",
      " -0.0413116 ]\n",
      "\n",
      "Sentence: Her water is about to break\n",
      "Vector for 'break': [-0.13044037  0.17287034  0.536402   ...  0.20365655  0.23261338\n",
      "  0.19071695]\n",
      "\n",
      "Sentence: I will break this into pieces\n",
      "Vector for 'break': [-0.04437125 -0.4148057   0.18563403 ... -0.01195487  0.37412658\n",
      " -0.5906379 ]\n",
      "\n",
      "Sentence: Tea break is a common thing in England\n",
      "Vector for 'break': [ 0.33905056  0.06670371  0.17880782 ... -0.23717158  0.5685922\n",
      "  0.02736001]\n",
      "\n",
      "Sentence: All it needs is one break\n",
      "Vector for 'break': [-0.35614234  0.04308439  0.6850399  ...  0.20365655  0.23261338\n",
      "  0.19071695]\n",
      "\n",
      "Sentence: break a leg\n",
      "Vector for 'break': [ 0.25928286 -0.09716384  0.30375594 ...  0.24734862  0.2551699\n",
      "  0.2785468 ]\n",
      "\n",
      "Sentence: See me in my office during the break\n",
      "Vector for 'break': [-0.38974202 -0.2027393   0.4080636  ...  0.20365655  0.23261338\n",
      "  0.19071695]\n",
      "\n",
      "Word vector size: (1024,)\n"
     ]
    }
   ],
   "source": [
    "sents = [\n",
    "    'The pillow was there to break the fall.',\n",
    "    \"He's going to break the ladder\",\n",
    "    'Give me a break',\n",
    "    \"I'm taking a break after this\",\n",
    "    'Her water is about to break',\n",
    "    'I will break this into pieces',\n",
    "    'Tea break is a common thing in England',\n",
    "    'All it needs is one break',\n",
    "    'break a leg',\n",
    "    'See me in my office during the break'\n",
    "]\n",
    "\n",
    "target = 'break'\n",
    "\n",
    "elmo_vecs = elmo_vectors(sents)\n",
    "word_vecs = []\n",
    "for i, sent in enumerate(sents):\n",
    "    word_vecs.append(elmo_vecs[i][sent.split().index(target)])\n",
    "    print(\"Sentence:\", sent)\n",
    "    print(\"Vector for '%s':\" % target, word_vecs[-1])\n",
    "    print()\n",
    "\n",
    "print(\"Word vector size:\", word_vecs[0].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function just doesn't work with capital letters and end of the sentense, does it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarities between 'break' vector in sentences:\n",
      "Sent 5-0: 0.7517743\n",
      "Sent 5-1: 0.8202457\n",
      "Sent 5-2: 0.527225\n",
      "Sent 5-3: 0.5016476\n",
      "Sent 5-4: 0.6245154\n",
      "Sent 5-5: 1.0000001\n",
      "Sent 5-6: 0.4141451\n",
      "Sent 5-7: 0.50315714\n",
      "Sent 5-8: 0.77526045\n",
      "Sent 5-9: 0.5151755\n"
     ]
    }
   ],
   "source": [
    "vec_size = word_vecs[0].shape[0]\n",
    "print(\"Similarities between '%s' vector in sentences:\" % target)\n",
    "for i in range(0, len(sents)):\n",
    "    print(\"Sent 5-%d:\" % i, cosine_similarity(word_vecs[5].reshape((1,vec_size)), \n",
    "                                              word_vecs[i].reshape((1,vec_size)))[0][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pretty much okay. Except for the 5th sentence `(i=4)`, it seems to classify `to break` at the end of a sentence differently than when it's in the middle, although both are still verb. Not so much as fail but ambiguous results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "[top](#first-part)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
