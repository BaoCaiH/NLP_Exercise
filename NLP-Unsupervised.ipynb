{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"first-part\"></a>\n",
    "\n",
    "#!/usr/bin/env python<br># coding: utf-8\n",
    "\n",
    "Author: Bao Cai\n",
    "\n",
    "Course: Machine Learning for Descriptive Problems\n",
    "\n",
    "Topic: NLP-Unsupervised\n",
    "\n",
    "Start Date: 2020-03-11\n",
    "\n",
    "Last Save: 2020-03-11"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[1. Topic analysis](#topic-part)\n",
    "\n",
    "There are quantitative measures for evaluating various aspects of clustering results and topic models intrinsically, and they can also be evaluated extrinsically by how well the clusters/topics serve some supervised task as features. In this exervise, however, we well fovus on qualitative evaluation of the results in terms of their descriptiveness. As in the example code, you may limit yourself to the 1000 first documents of ther corpus when performing clustering, in order to simplify the task and speed up experimentation, but use the whole corpus to calculate tf-idf features.\n",
    "\n",
    "a. Experiment with different setups of the tf-idf feature extraction and clustering (k-means or hierarchical). In order to obtain meaningful results. When you arrive at a good configuration, describe it and motivate your chosen setup/parameters.\n",
    "\n",
    "b. Inspect the keywords of the clusters. List the 10 first clusters out of all (o.e. not cherry picked examples) and privde an as descriptive label as possible for each of them.\n",
    "\n",
    "c. Select one or two good clusters (that can be clearly interpreted) and one or two bad clusters (that might be difficult to interpret or distinguish). Motivate your choise (clusters may for instance, be overlapping, to broad/narrow or incoherent).\n",
    "\n",
    "d. Repeat the experiment in (a) with LDA topic modelling instead (on the whole corpus), and explain briefly how the results compare to your previously chosen clustering setup. A few concrete examples may be helpful. Do your best to make sure the list of topic keywords are informative through appropriate post-processing.\n",
    "\n",
    "[2. Word vectors](#word-vector)\n",
    "\n",
    "a. Choose about 5 words (arbitrary) to use as seed words in the following experiment. Train word2vec vectors on the corpus while trying out variations on the parameters. Evaluate the vector models by inspecting the most similar words for each of the seed words, and try to identify qualitative differences between different parameter choices. Which parameters seem to have the most interesting effect? At what values? Motivate. Finally, study the qualitative effect of increasing the training data, by similarly comparing vectors trained with the best setup on the texts from the awards_2020 directory against vectors trained on the whole set of abstracts (1990-2002).\n",
    "\n",
    "b. Repeat the experiment with ELMo from the lecture, with a different target word and diferent sentences. Choose a word that can have multiple senses, and construct 10 sentences that express 2-3 different senses of the word. Produce ELMo embeddings for the target word in each sentence and measure the similarity between the vectors. Evaluate in how many cases the measured similarities can be used to successfully distinguish between the different senses. Comment on the results, e.g. are you able to identify a particular way in which the model fails?\n",
    "\n",
    "[top](#first-part)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "import os\n",
    "import re\n",
    "import binascii\n",
    "import itertools\n",
    "import heapq\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from time import time\n",
    "from collections import Counter\n",
    "from scipy.spatial.distance import cosine\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage, fcluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions\n",
    "def get_fnames(path='./Data'):\n",
    "    \"\"\"Read all text files in a folder.\n",
    "    \"\"\"\n",
    "    fnames = []\n",
    "    for root, _, files in os.walk(path):\n",
    "        for fname in files:\n",
    "            if fname[-4:] == '.txt':\n",
    "                fnames.append(os.path.join(root, fname))\n",
    "    return fnames\n",
    "\n",
    "def read_file(fname):\n",
    "    with open(fname, 'rt', encoding='latin-1') as f:\n",
    "        # skip all lines until abstract\n",
    "        for line in f:\n",
    "            if \"Abstract    :\" in line:\n",
    "                break\n",
    "\n",
    "        # get abstract as a single string\n",
    "        abstract = ' '.join([line[:-1].strip() for line in f])\n",
    "        abstract = re.sub(' +', ' ', abstract)  # remove double spaces\n",
    "        return abstract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = [read_file(file) for file in get_fnames('./Data/awards_2002/')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9923, 20000)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set parameters and initialize\n",
    "tfidf_vectorizer = TfidfVectorizer(min_df=2, use_idf=True, sublinear_tf=True, max_df=1.0, max_features=20000)\n",
    "# Tip: the vectorizer also supports extracting n-gram features (common short sequences of words),\n",
    "# which may be more descriptive but also much less frequent\n",
    "\n",
    "# Calcualate term-document matrix with tf-idf scores\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(documents)\n",
    "\n",
    "# Check matrix shape\n",
    "tfidf_matrix.toarray().shape # N_docs x N_terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9637\tthe\n",
      "9619\tof\n",
      "9613\tand\n",
      "9511\tto\n",
      "9442\tin\n",
      "8743\tthis\n",
      "8625\tfor\n",
      "8269\tis\n",
      "8228\twill\n",
      "7632\tbe\n",
      "7419\ton\n",
      "7271\twith\n",
      "7167\tthat\n",
      "6656\tare\n",
      "6642\tresearch\n",
      "6561\tby\n",
      "6424\tas\n",
      "5968\tfrom\n",
      "5750\tan\n",
      "5402\tthese\n"
     ]
    }
   ],
   "source": [
    "terms_in_docs = tfidf_vectorizer.inverse_transform(tfidf_matrix)\n",
    "token_counter = Counter()\n",
    "for terms in terms_in_docs:\n",
    "    token_counter.update(terms)\n",
    "\n",
    "for term, count in token_counter.most_common(20):\n",
    "    print(\"%d\\t%s\" % (count, term))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('the', 1.0292424476114135), ('of', 1.0311118011519393), ('and', 1.0317356963577857), ('to', 1.0424019064410739), ('in', 1.0496823395486226), ('this', 1.126588314955317), ('for', 1.1401751676039031), ('is', 1.1823215567939547), ('will', 1.1872915652059788), ('be', 1.2624751130132212), ('on', 1.2907770086502655), ('with', 1.310924708954377), ('that', 1.3253293901569252), ('are', 1.399287133210988), ('research', 1.4013923971464504), ('by', 1.4136606312906448), ('as', 1.434759435048271), ('from', 1.5083766566430419), ('an', 1.5455823130979374), ('these', 1.608001710967626)]\n"
     ]
    }
   ],
   "source": [
    "print(sorted(zip(features, tfidf_vectorizer._tfidf.idf_),key=lambda x:x[1])[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Document 0, top terms by TF-IDF\n",
      "0.34\tflower\n",
      "0.24\tcolor\n",
      "0.22\tmutations\n",
      "0.21\tpollinators\n",
      "0.21\tdifferences\n",
      "\n",
      "Document 1, top terms by TF-IDF\n",
      "0.36\tpollinator\n",
      "0.25\tinbreeding\n",
      "0.22\tfragmentation\n",
      "0.21\tcorrelation\n",
      "0.20\tfragments\n",
      "\n",
      "Document 2, top terms by TF-IDF\n",
      "0.32\tpulsars\n",
      "0.23\tbinaries\n",
      "0.20\tgalaxy\n",
      "0.18\tsurvey\n",
      "0.18\tobservatory\n",
      "\n",
      "Document 3, top terms by TF-IDF\n",
      "0.26\tdogs\n",
      "0.23\tprairie\n",
      "0.20\tcaptivity\n",
      "0.20\tpredators\n",
      "0.18\treared\n",
      "\n",
      "Document 4, top terms by TF-IDF\n",
      "0.32\tcopulatory\n",
      "0.32\tcannibalism\n",
      "0.22\tmales\n",
      "0.21\treproductive\n",
      "0.19\tmate\n"
     ]
    }
   ],
   "source": [
    "## Inspect top terms per document\n",
    "\n",
    "features = tfidf_vectorizer.get_feature_names()\n",
    "for doc_i in range(5):\n",
    "    print(\"\\nDocument %d, top terms by TF-IDF\" % doc_i)\n",
    "    for term, score in sorted(list(zip(features,tfidf_matrix.toarray()[doc_i])), key=lambda x:-x[1])[:5]:\n",
    "        print(\"%.2f\\t%s\" % (score, term))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "print(tfidf_matrix.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document vector length: 20000\n",
      "Non-zero dimensions for document 0: 112\n",
      "Non-zero dimensions for document 1: 88\n",
      "Non-zero dimensions for document 2: 95\n",
      "Non-zero dimensions for document 3: 152\n",
      "Non-zero dimensions for document 4: 132\n"
     ]
    }
   ],
   "source": [
    "print(\"Document vector length:\", tfidf_matrix.shape[1])\n",
    "for i in range(5):\n",
    "    print(\"Non-zero dimensions for document %d: %d\" % (i, len([x for x in tfidf_matrix.toarray()[i] if x > 0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample word: allyl\n",
      "Occurs in 97 documents\n",
      "out of 9923 documents\n"
     ]
    }
   ],
   "source": [
    "print(\"Sample word:\", features[1000])\n",
    "print(\"Occurs in %d documents\" % len([x for x in tfidf_matrix.toarray()[:][1000] if x > 0]))\n",
    "print(\"out of %d documents\" % len(tfidf_matrix.toarray()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KMeans(algorithm='auto', copy_x=True, init='k-means++', max_iter=300,\n",
       "       n_clusters=30, n_init=10, n_jobs=None, precompute_distances='auto',\n",
       "       random_state=123, tol=0.0001, verbose=0)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# matrix_sample = tfidf_matrix[:1000]\n",
    "matrix_sample = tfidf_matrix\n",
    "# Do clustering\n",
    "km = KMeans(n_clusters=30, random_state=123, verbose=0)\n",
    "km.fit(matrix_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_clusters(matrix, clusters, n_keywords=10):\n",
    "    for cluster in range(min(clusters), max(clusters)+1):\n",
    "        cluster_docs = [i for i, c in enumerate(clusters) if c == cluster]\n",
    "        print(\"Cluster: %d (%d docs)\" % (cluster, len(cluster_docs)))\n",
    "        \n",
    "        # Keep scores for top n terms\n",
    "        new_matrix = np.zeros((len(cluster_docs), matrix.shape[1]))\n",
    "        for cluster_i, doc_vec in enumerate(matrix[cluster_docs].toarray()):\n",
    "            for idx, score in heapq.nlargest(n_keywords, enumerate(doc_vec), key=lambda x:x[1]):\n",
    "                new_matrix[cluster_i][idx] = score\n",
    "\n",
    "        # Aggregate scores for kept top terms\n",
    "        keywords = heapq.nlargest(n_keywords, zip(new_matrix.sum(axis=0), features))\n",
    "        print(', '.join([w for s,w in keywords]))\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([22,  8, 25, ..., 26,  0, 11], dtype=int32)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "km.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster: 0 (625 docs)\n",
      "solar, wind, magnetic, ionosphere, ocean, magnetosphere, oceanographic, iron, auroral, waves\n",
      "\n",
      "Cluster: 1 (343 docs)\n",
      "fluid, quantum, flows, turbulence, particles, transport, turbulent, fluids, colloidal, adsorption\n",
      "\n",
      "Cluster: 2 (491 docs)\n",
      "center, equipment, igert, manufacturing, ucrc, facility, engineering, station, industry, polymer\n",
      "\n",
      "Cluster: 3 (367 docs)\n",
      "workshop, geoscience, geon, eu, workshops, government, federal, committee, earth, cyberinfrastructure\n",
      "\n",
      "Cluster: 4 (254 docs)\n",
      "algebraic, theory, algebras, spaces, algebra, quantum, geometry, commutative, conjecture, geometric\n",
      "\n",
      "Cluster: 5 (330 docs)\n",
      "conference, symposium, meeting, gordon, speakers, 2002, 2003, young, travel, congress\n",
      "\n",
      "Cluster: 6 (893 docs)\n",
      "social, contract, political, firms, children, organizational, language, archaeological, policy, cultural\n",
      "\n",
      "Cluster: 7 (326 docs)\n",
      "wireless, networks, network, sensor, mobile, power, routing, qos, nodes, traffic\n",
      "\n",
      "Cluster: 8 (374 docs)\n",
      "forest, ecosystem, soil, species, tropical, nitrogen, co2, prey, ecosystems, plant\n",
      "\n",
      "Cluster: 9 (326 docs)\n",
      "mantle, fault, seismic, magma, subduction, deformation, lithosphere, arc, crust, plate\n",
      "\n",
      "Cluster: 10 (438 docs)\n",
      "software, grid, code, programming, user, language, memory, distributed, visualization, database\n",
      "\n",
      "Cluster: 11 (216 docs)\n",
      "equations, manifolds, hyperbolic, nonlinear, manifold, curvature, spaces, geometry, solutions, topology\n",
      "\n",
      "Cluster: 12 (225 docs)\n",
      "available, not, zygotic, zygomycota, zygomycetes, zygmund, zurich, zuni, zro2, zr\n",
      "\n",
      "Cluster: 13 (228 docs)\n",
      "stem, scholarship, scholarships, csems, college, csem, scholars, retention, mathematics, alliance\n",
      "\n",
      "Cluster: 14 (152 docs)\n",
      "reu, summer, site, experience, ten, chemistry, 2003, week, colleges, undergraduates\n",
      "\n",
      "Cluster: 15 (375 docs)\n",
      "sensor, fuel, market, drug, optical, power, polymer, coatings, imaging, cell\n",
      "\n",
      "Cluster: 16 (116 docs)\n",
      "nmr, spectrometer, instrument, molecules, chemists, diffractometer, ray, chemistry, spectroscopy, molecule\n",
      "\n",
      "Cluster: 17 (356 docs)\n",
      "ice, climate, ocean, arctic, variability, circulation, freshwater, atlantic, records, lake\n",
      "\n",
      "Cluster: 18 (197 docs)\n",
      "cell, signaling, neurons, cells, receptor, brain, plant, drosophila, enzyme, nerve\n",
      "\n",
      "Cluster: 19 (506 docs)\n",
      "polymer, composite, fiber, machining, alloys, pressure, optical, concrete, micro, polymers\n",
      "\n",
      "Cluster: 20 (202 docs)\n",
      "reactions, complexes, chemistry, reaction, compounds, organic, catalysts, metal, intermediates, oxidation\n",
      "\n",
      "Cluster: 21 (84 docs)\n",
      "fellowship, mathematical, sciences, postdoctoral, informatics, training, microbial, minority, fellowships, biology\n",
      "\n",
      "Cluster: 22 (355 docs)\n",
      "evolutionary, species, genetic, genes, genome, plants, speciation, traits, birds, populations\n",
      "\n",
      "Cluster: 23 (204 docs)\n",
      "teachers, school, fellows, teacher, mathematics, districts, middle, schools, district, engineering\n",
      "\n",
      "Cluster: 24 (280 docs)\n",
      "protein, proteins, genes, gene, arabidopsis, expression, transcription, dna, genome, binding\n",
      "\n",
      "Cluster: 25 (147 docs)\n",
      "galaxies, stars, galaxy, star, stellar, dark, universe, galactic, gravitational, clusters\n",
      "\n",
      "Cluster: 26 (454 docs)\n",
      "spin, magnetic, nanoscale, quantum, nano, films, nanotubes, nanostructures, nanoparticles, semiconductor\n",
      "\n",
      "Cluster: 27 (413 docs)\n",
      "learning, curriculum, nsdl, stem, library, digital, engineering, assessment, courses, modules\n",
      "\n",
      "Cluster: 28 (551 docs)\n",
      "algorithms, optimization, estimation, estimators, inference, regression, control, nonlinear, algorithm, nonparametric\n",
      "\n",
      "Cluster: 29 (95 docs)\n",
      "french, cnrs, france, us, german, indian, dst, visits, pasi, inria\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_clusters(matrix_sample, km.labels_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Z = linkage(matrix_sample.todense(), metric='cosine', method='complete')\n",
    "_ = dendrogram(Z, no_labels=True) # Plot dentrogram chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Get flat clusters from cluster hierarchy\n",
    "\n",
    "#clusters = fcluster(Z, 50, criterion='maxclust') # Create fix number of flat clusters\n",
    "clusters = fcluster(Z, 0.99, criterion='distance') # Create flat clusters by distance threshold\n",
    "\n",
    "print_clusters(matrix_sample, clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
